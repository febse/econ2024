
# Least Squares

The following data set contains information about thirty work days in an accounting company. Every day the company receives invoices from its clients and processes them (e.g. checks them, enters them into the accounting system, etc.). The data set contains the number of invoices processed on each day and the time needed to process them. Our goal is to predict the time needed to process a given number of invoices.

Variables description:

- `Day` (numeric): day
- `Invoices` (numeric): number of invoices
- `Time` (numeric): Time needed to process the invoices (hours)


```{r setup, include=FALSE}
# Load the tidyverse packages
library(tidyverse)

# Download and read the data
invoices <- read.delim('https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/invoices.txt')
```

Get an overview of the data set.

```{r}
invoices %>% skimr::skim()
```


More precisely, our task is to predict the time needed to process 50, 120, 201, 250, 400 invoices.


```{r}
ggplot(data = invoices, aes(x = Invoices, y = Time)) +
  geom_point() +
  geom_vline(xintercept = c(50, 120, 201, 250, 400), lty = 2, alpha = 0.5) +
  scale_x_continuous(breaks = c(50, 120, 201, 250, 400))

# +
# geom_vline(xintercept = 120, lty = 2) +
# geom_hline(yintercept = 2.110) +
# geom_abline(intercept = 0.1, slope = 0.015, colour = "steelblue2")
```

Let us set up a bit of notation. We have a sample of $n = 30$ observations. Let $i$  be the number of the days $i=1,2,\ldots, n$. The variable of interest is the time needed to process the invoices, denoted by $\text{Time}_i$. We have one predictor variable, the number of invoices, denoted by $\text{Invoices}_i$.

As our goal is to create predictions, we will denote the predicted time needed to process the invoices by $\widehat{\text{Time}}_i$.


Strategy 1: Use the average processing time for the predictions

$$
\widehat{\text{Time}}_i^{avg} = \overline{\text{Time}}
$$


Strategy 2: Use the following linear equation for the predictions

$$
\widehat{\text{Time}}_i^{eq1} = 0.1 + 0.02 \cdot \text{Invoices}_i
$$


:::{#exr-intercept-only-equation}
## Strategy 1

Compute the predicted time $\widehat{Time}_i$ for the first strategy and store it in a new column `Time_hat_eq1`. Then, compute the residuals (difference between actual and predicted values) and store them in a new column `residuals_eq1`.

```{r}
# Hint: use the mutate function

```

:::

:::{#exr-linear-equation}
## Strategy 2

Compute the predicted time $\widehat{Time}_i$ for the second strategy and store it in a new column `Time_hat_eq2`. Then, compute the residuals and store them in a new column `residuals_eq2`.

```{r}
# Hint: use the mutate function


```

:::

Before we proceed, let us visualize the residuals of the two strategies.

```{r}
#| label: fig-residuals-strategy-1
#| fig-cap: "Prediction and Residuals for Strategy 1"

invoices %>%
    mutate(
        Time_hat = mean(Time),
        residuals = Time - Time_hat
    ) %>%
ggplot(aes(x = Invoices, y = Time)) +
  geom_point() +
  geom_hline(yintercept = mean(invoices$Time), lty = 2) +
  ylim(c(0, 5)) +
  geom_segment(
    aes(
        xend = Invoices,
        yend = mean(invoices$Time)), 
        lty = 2,
        alpha = 0.5
    ) +
  geom_label(aes(label = round(residuals, 2)))
```

```{r}
#| label: fig-residuals-strategy-2
#| fig-cap: "Prediction and Residuals for Strategy 2"
#| 
invoices %>%
    mutate(
        Time_hat = 0.1 + 0.02 * Invoices,
        residuals = Time - Time_hat
    ) %>%
ggplot(aes(x = Invoices, y = Time)) +
  geom_point() +
  geom_abline(intercept = 0.1, slope = 0.02) +
  ylim(c(0, 5)) +
  geom_segment(
    aes(
        xend = Invoices,
        yend = Time_hat, 
        alpha = 0.5
    )) +
  geom_label(aes(label = round(residuals, 2)))
```


Which strategy is better? To answer this question we need to agree on a criterion for comparison. The criterion should be a summary of the differences between the actual and the predicted values and should be small when the differences are small. It should also be zero when the predictions are perfect (i.e. no differences between predicted and actual values).

A candidate criterion could be the average prediction error:

$$
\sum_{i = 1}^{n} (\text{Time}_i - \widehat{\text{Time}}_i)
$$

It suffers from a major drawback, however. Can you spot it?

A widely used criterion is the mean squared error (MSE). The MSE is defined as.

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (\text{Time}_i - \widehat{\text{Time}}_i)^2
$$

where $\widehat{\text{Time}}_i$ is the predicted time needed to process the invoices.

Let's calculate the MSE for the two strategies.

```{r}
# Strategy 1

```

```{r}
# Strategy 2

```



## Least Squares

The previous discussion leads us to a question. Can we find values for the coefficients in the linear equation that minimize the MSE? The answer is yes. The method is called least squares.

First, let us visualize the MSE as a function of the coefficients. For this purpose, we will simply calculate the MSE for a grid of values of the coefficients and plot the results (@fig-rss-3d)

```{r}
#| label: fig-rss-3d
#| fig-cap: "MSE as a function of the coefficients"

# NOTE: the code here is only for illustration purposes, you don't need to understand it

library(plotly)

# Create a grid of values for beta0_hat and beta1_hat

beta0_hat <- seq(0.4, 1, length.out = 50)
beta1_hat <- seq(0.001, 0.015, length.out = 50)

dt <- expand.grid(beta0_hat = beta0_hat, beta1_hat = beta1_hat) %>%
  mutate(
    # Compute the RSS for each combination of beta0_hat and beta1_hat
    RSS = map2_dbl(beta0_hat, beta1_hat, ~{
      Time_hat <- .x + .y * invoices$Invoices
      sum((invoices$Time - Time_hat)^2)
    })
  )

fig <- plot_ly(
  x=beta0_hat, 
  y=beta1_hat, 
  z = matrix(dt$RSS, nrow = length(beta0_hat), ncol = length(beta1_hat)),
  ) %>% 
    add_surface() %>% 
    layout(
        scene = list(
          xaxis = list(title = "beta1_hat"),
          yaxis = list(title = "beta1_hat"),
          zaxis = list(title = "RSS")
        )
    )

fig
```

Our goal is to find the values of $\beta_0$ and $\beta_1$ that minimize the MSE. The method of least squares provides a formula for the coefficients that minimize the MSE.

First, let us solve a simpler problem. Assume that the predictive equation is

$$
\widehat{\text{Time}}_i = \hat{beta}_0
$$


The MSE in this case is much simpler.

```{r}
#| label: fig-rss-beta0
#| fig-cap: "MSE as a function of beta0_hat in an intercept-only equation"

invoices_beta_0_dt <- expand_grid(
    beta0_hat = seq(0, 4, length.out = 100),
    invoices
)

rss_dt <- invoices_beta_0_dt %>%
  group_by(beta0_hat) %>%
  summarise(
    RSS = sum((Time - beta0_hat)^2)
  )

rss_dt %>%
    ggplot(aes(x = beta0_hat, y = RSS)) +
    geom_line()
```
