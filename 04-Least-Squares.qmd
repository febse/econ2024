
# Least Squares

The following data set contains information about thirty work days in an accounting company. Every day the company receives invoices from its clients and processes them (e.g. checks them, enters them into the accounting system, etc.). The data set contains the number of invoices processed on each day and the time needed to process them. Our goal is to predict the time needed to process a given number of invoices.

Variables description:

- `Day` (numeric): day
- `Invoices` (numeric): number of invoices
- `Time` (numeric): Time needed to process the invoices (hours)


```{r setup, include=FALSE}
# Load the tidyverse packages
library(tidyverse)

# Download and read the data
invoices <- read.delim('https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/invoices.txt')
```

Get an overview of the data set.

```{r}
invoices %>% skimr::skim()
```


More precisely, our task is to predict the time needed to process 50, 120, 201, 250, 400 invoices.


```{r}
ggplot(data = invoices, aes(x = Invoices, y = Time)) +
  geom_point() +
  geom_vline(xintercept = c(50, 120, 201, 250, 400), lty = 2, alpha = 0.5) +
  scale_x_continuous(breaks = c(50, 120, 201, 250, 400))

# +
# geom_vline(xintercept = 120, lty = 2) +
# geom_hline(yintercept = 2.110) +
# geom_abline(intercept = 0.1, slope = 0.015, colour = "steelblue2")
```

Let us set up a bit of notation. We have a sample of $n = 30$ observations. Let $i$  be the number of the days $i=1,2,\ldots, n$. The variable of interest is the time needed to process the invoices, denoted by $\text{Time}_i$. We have one predictor variable, the number of invoices, denoted by $\text{Invoices}_i$.

As our goal is to create predictions, we will denote the predicted time needed to process the invoices by $\widehat{\text{Time}}_i$.


Strategy 1: Use the average processing time for the predictions

$$
\widehat{\text{Time}}_i^{avg} = \overline{\text{Time}}
$$


Strategy 2: Use the following linear equation for the predictions

$$
\widehat{\text{Time}}_i^{eq1} = 0.6 + 0.01 \cdot \text{Invoices}_i
$$


:::{#exr-intercept-only-equation}
## Strategy 1

Compute the predicted time $\widehat{Time}_i$ for the first strategy and store it in a new column `Time_hat_eq1`. Then, compute the residuals (difference between actual and predicted values) and store them in a new column `residuals_eq1`.

```{r}
# Hint: use the mutate function

```

:::

:::{#exr-linear-equation}
## Strategy 2

Compute the predicted time $\widehat{Time}_i$ for the second strategy and store it in a new column `Time_hat_eq2`. Then, compute the residuals and store them in a new column `residuals_eq2`.

```{r}
# Hint: use the mutate function


```

:::

Before we proceed, let us visualize the residuals of the two strategies.

```{r}
#| label: fig-residuals-strategy-1
#| fig-cap: "Prediction and Residuals for Strategy 1"

invoices %>%
    mutate(
        Time_hat = mean(Time),
        residuals = Time - Time_hat
    ) %>%
ggplot(aes(x = Invoices, y = Time)) +
  geom_point() +
  geom_hline(yintercept = mean(invoices$Time), lty = 2) +
  ylim(c(0, 5)) +
  geom_segment(
    aes(
        xend = Invoices,
        yend = mean(invoices$Time)), 
        lty = 2,
        alpha = 0.5
    ) +
  geom_label(aes(label = round(residuals, 2)))
```

```{r}
#| label: fig-residuals-strategy-2
#| fig-cap: "Prediction and Residuals for Strategy 2"
#| 
invoices %>%
    mutate(
        Time_hat = 0.6 + 0.01 * Invoices,
        residuals = Time - Time_hat
    ) %>%
ggplot(aes(x = Invoices, y = Time)) +
  geom_point() +
  geom_abline(intercept = 0.6, slope = 0.01) +
  ylim(c(0, 5)) +
  geom_segment(
    aes(
        xend = Invoices,
        yend = Time_hat, 
        alpha = 0.5
    )) +
  geom_label(aes(label = round(residuals, 2)))
```

```{r}
lm(Time ~ Invoices, data = invoices) %>% summary()
```

```{r}
```


Which strategy is better? To answer this question we need to agree on a criterion for comparison. The criterion should be a summary of the differences between the actual and the predicted values and should be small when the differences are small. It should also be zero when the predictions are perfect (i.e. no differences between predicted and actual values).

A candidate criterion could be the average prediction error:

$$
\sum_{i = 1}^{n} (\text{Time}_i - \widehat{\text{Time}}_i)
$$

It suffers from a major drawback, however. Can you spot it?

A widely used criterion is the mean squared error (MSE). The MSE is defined as.

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (\text{Time}_i - \widehat{\text{Time}}_i)^2
$$

where $\widehat{\text{Time}}_i$ is the predicted time needed to process the invoices.

Let's calculate the MSE for the two strategies.

```{r}
# Strategy 1

```

```{r}
# Strategy 2

```

Can you guess better values for the coefficients in the linear equation? Try your own values and calculate the MSE.

```{r}
# Your own strategy

```

## Least Squares

The previous discussion leads us to a question. Can we find values for the coefficients in the linear equation that minimize the MSE? The answer is yes. The method is called least squares.

First, let us visualize the MSE as a function of the coefficients. For this purpose, we will simply calculate the MSE for a grid of values of the coefficients and plot the results (@fig-rss-3d)

```{r}
#| label: fig-rss-3d
#| fig-cap: "MSE as a function of the coefficients"

# NOTE: this code here is only for illustration purposes, you don't need to study it or understand it for the course

library(plotly)

# Create a grid of values for beta0_hat and beta1_hat

beta0_hat <- seq(0.4, 1, length.out = 50)
beta1_hat <- seq(0.001, 0.015, length.out = 50)

dt <- expand.grid(beta0_hat = beta0_hat, beta1_hat = beta1_hat) %>%
  mutate(
    # Compute the RSS for each combination of beta0_hat and beta1_hat
    RSS = map2_dbl(beta0_hat, beta1_hat, ~{
      Time_hat <- .x + .y * invoices$Invoices
      sum((invoices$Time - Time_hat)^2)
    })
  )

fig <- plot_ly(
  x=beta0_hat, 
  y=beta1_hat, 
  z = matrix(dt$RSS, nrow = length(beta0_hat), ncol = length(beta1_hat)),
  ) %>% 
    add_surface() %>% 
    layout(
        scene = list(
          xaxis = list(title = "beta1_hat"),
          yaxis = list(title = "beta0_hat"),
          zaxis = list(title = "RSS")
        )
    )

fig
```

Our goal is to find the values of $\beta_0$ and $\beta_1$ that minimize the RSS. The method of least squares provides a formula for the coefficients that minimize the RSS.

First, let us solve a simpler problem. Assume that the predictive equation is

$$
\widehat{\text{Time}}_i = \hat{\beta}_0
$$


The MSE in this case is much simpler.

```{r}
#| label: fig-rss-beta0
#| fig-cap: "MSE as a function of beta0_hat in an intercept-only equation"

invoices_beta_0_dt <- expand_grid(
    beta0_hat = seq(0, 4, length.out = 100),
    invoices
)

rss_dt <- invoices_beta_0_dt %>%
  group_by(beta0_hat) %>%
  summarise(
    RSS = sum((Time - beta0_hat)^2)
  )

rss_dt %>%
    ggplot(aes(x = beta0_hat, y = RSS)) +
    geom_line()
```

How do we find the value of $\hat{\beta}_0$ that minimizes the MSE? We take the derivative of the MSE with respect to $\hat{\beta}_0$ and set it to zero.

In order to find the minimum of the MSE, we take the derivative of the MSE with respect to $\hat{\beta}_0$ and set it to zero. Instead of $\text{Time}$ we will use the shorter notation $y_i$ and instead of $\hat{\text{Time}}_i$ we will use the shorter notation $\hat{y}_i$.

$$
\begin{align*}
RSS(\hat{\beta}_0) & = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
 & = \sum_{i=1}^n (y_i - \hat{\beta}_0)^2
\end{align*}
$$

Try to find the value of $\hat{\beta}_0$ that minimizes the MSE. **Hint**: Take the derivative of the RSS with respect to $\hat{\beta}_0$, set it to zero and solve for $\hat{\beta}_0$.

:::{.callout-note collapse="true"}
## Solution for the intercept-only equation

$$
\begin{align*}
\frac{\partial}{\partial \hat{\beta}_0} RSS(\hat{\beta}_0) & = 
\sum_{i=1}^n 2(y_i - \hat{\beta}_0) \cdot (-1) \\
& = -2 \sum_{i=1}^n (y_i - \hat{\beta}_0) \\
& = -2 \sum_{i=1}^n y_i + 2 \sum_{i=1}^n \hat{\beta}_0 \\
& = -2 \sum_{i=1}^n y_i + 2 n \hat{\beta}_0
\end{align*}
$$

Setting the derivative to zero, we get

$$
\begin{align*}
-2 \sum_{i=1}^n y_i + 2 n \hat{\beta}_0 & = 0 \\
\hat{\beta}_0 & = \frac{1}{n} \sum_{i=1}^n y_i \\
& = \overline{y}
\end{align*}
$$

The value of $\hat{\beta}_0$ that minimizes the MSE is thus just the average of the observed values of $y_i$.
:::

## The one variable case

Now let us consider the case where we have two variables, $\text{Invoices}_i$ and $\text{Time}_i$. We want to find the values of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize the RSS.

The prediction equation is

$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
$$

The RSS is (as above)

$$
\begin{align*}
\text{RSS}(\hat{\beta}_0, \hat{\beta}_1) & = \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
  & = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2
\end{align*}
$$

The first order conditions for the minimum are

$$
\begin{align*}
\frac{\partial}{\partial \hat{\beta}_0} \text{RSS}(\hat{\beta}_0, \hat{\beta}_1) & = -2 \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = 0 \\
\frac{\partial}{\partial \hat{\beta}_1} \text{RSS}(\hat{\beta}_0, \hat{\beta}_1) & = -2 \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) x_i = 0
\end{align*}
$$

The first equation gives

$$
\begin{align*}
\sum_{i=1}^n y_i - \hat{\beta}_0 n - \hat{\beta}_1 \sum_{i=1}^n x_i & = 0 \\
\hat{\beta}_0 n + \hat{\beta}_1 \sum_{i=1}^n x_i & = \sum_{i=1}^n y_i \\
\hat{\beta}_0 & = \overline{y} - \hat{\beta}_1 \overline{x}
\end{align*}
$$

The second equation gives

$$
\begin{align*}
\sum_{i=1}^n y_i x_i - \hat{\beta}_0 \sum_{i=1}^n x_i - \hat{\beta}_1 \sum_{i=1}^n x_i^2 & = 0 \\
\hat{\beta}_0 \sum_{i=1}^n x_i + \hat{\beta}_1 \sum_{i=1}^n x_i^2 & = \sum_{i=1}^n y_i x_i \\
\hat{\beta}_0 & = \overline{y} - \hat{\beta}_1 \overline{x}
\end{align*}
$$

Substituting the expression for $\hat{\beta}_0$ in the second equation, we get

$$
\begin{align*}
(\overline{y} - \hat{\beta}_1 \overline{x}) \sum_{i=1}^n x_i + \hat{\beta}_1 \sum_{i=1}^n x_i^2 & = \sum_{i=1}^n y_i x_i \\
\overline{y} \sum_{i=1}^n x_i - \hat{\beta}_1 \overline{x} \sum_{i=1}^n x_i + \hat{\beta}_1 \sum_{i=1}^n x_i^2 & = \sum_{i=1}^n y_i x_i \\
\hat{\beta}_1 \sum_{i=1}^n x_i^2 & = \sum_{i=1}^n y_i x_i - \overline{y} \sum_{i=1}^n x_i + \hat{\beta}_1 \overline{x} \sum_{i=1}^n x_i \\
\hat{\beta}_1 & = \frac{\sum_{i=1}^n y_i x_i - \overline{y} \sum_{i=1}^n x_i}{\sum_{i=1}^n x_i^2 - \overline{x} \sum_{i=1}^n x_i}
\end{align*}
$$

Simplyfing the expression, we get

$$
\begin{align*}
\hat{\beta}_1 & = \frac{\overline{x y} - \overline{x} \cdot \overline{y}}{\overline{x^2} - \overline{x}^2} \\
\hat{\beta}_0 & = \overline{y} - \hat{\beta}_1 \overline{x}
\end{align*}
$$

The last expression may seem a bit complicated, but it is actually quite simple. It is just the ratio between the empirical covariance between $x_i$ and $y_i$ divided by the variance of $x_i$.

The empirical covariance between $x_i$ and $y_i$ is defined as the sum of the products of the deviations of $x_i$ and $y_i$ from their respective means, divided by the number of observations.

:::{#def-covariance}
## Covariance

The covariance between two variables $x$ and $y$ with $n$ values is defined as

$$
S_{xy} = \frac{1}{n - 1} \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})
$$

:::

:::{#def-variance-decomposition}
## Variance decomposition

We have already defined the variance of a variable $x$ as

$$
S_x^2 = \frac{1}{n - 1} \sum_{i=1}^n (x_i - \overline{x})^2
$$

It can be shown that the variance of $x$ can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.

$$
S_x^2 = \frac{1}{n  - 1}(\overline{x_i^2} - \overline{x}^2)
$$
:::
:::{.proof}
The proof is straightforward. We have

$$
\begin{align*}
(n - 1) S_x^2 & =  \sum_{i=1}^n (x_i - \overline{x})^2 \\
& =  \sum_{i=1}^n (x_i^2 - 2x_i \overline{x} + \overline{x}^2) \\
& =  \sum_{i=1}^n x_i^2 - 2\overline{x} \sum_{i=1}^n x_i + \overline{x}^2 \sum_{i=1}^n 1 \\
& =  \sum_{i=1}^n x_i^2 - 2\overline{x} \sum_{i=1}^n x_i + \overline{x}^2 n \\
& =  \sum_{i=1}^n x_i^2 - 2\overline{x}^2 n + \overline{x}^2 n \\
& =  \sum_{i=1}^n x_i^2 - n \overline{x}^2 \\
& = n (\overline{x_i^2} - \overline{x}^2)
\end{align*}
$$
:::

:::{#def-covariance}
## Covariance and Covariance decomposition

The covariance between two variables $x$ and $y$ with $n$ values is defined as

$$
S_{xy} = \frac{1}{n - 1} \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})
$$

Much in the same way as the variance, the covariance can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.

$$
(n - 1) S_{xy} = n(\overline{x y} - \overline{x} \overline{y})
$$

The proof is similar to the proof for the variance decomposition.
:::
:::{.proof}
The proof is straightforward. We have

$$
\begin{align*}
(n - 1) S_{xy} & = \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y}) \\
& = \sum_{i=1}^n x_i y_i - \overline{x} \sum_{i=1}^n y_i - \overline{y} \sum_{i=1}^n x_i + n \overline{x} \overline{y} \\
& = n(\overline{x y} - \overline{x} \overline{y})
\end{align*}
$$
:::

To understand what the covariance measures, consider the following scatterplot:

```{r}
#| label: fig-covariance-positive
#| fig-cap: "Scatterplot of two variables with a positive linear assocition"

set.seed(123)
dt <- tibble(
    x = rnorm(100),
    y = 2 * x + rnorm(100)
)

dt %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    geom_vline(xintercept = mean(dt$x), colour = "firebrick4") +
    geom_hline(yintercept = mean(dt$y), colour = "steelblue4")
```

The reddish line is drawn at the average of the $x$ values and the bluish line is drawn at the average of the $y$ values. The covariance measures the average product of the deviations of the $x$ and $y$ values from their respective means. If the product is positive, it means that when $x$ is above its average, $y$ is also above its average. If the product is negative, it means that when $x$ is above its average, $y$ is below its average.

You can compute the empirical covariance between two variables using the `cov` function in R.

```{r}
cov(dt$x, dt$y)
```

Only the *sign* of the covariance is important. The magnitude of the covariance depends on the units of the variables. To make the covariance unit-free, we can divide it by the product of the standard deviations of the two variables. This gives us the correlation coefficient.

```{r}
cov(dt$x * 1000, dt$y)
cov(dt$x , dt$y * 50)
```

```{r}
#| label: fig-covariance-negative
#| fig-cap: "Scatterplot of two variables with a positive linear assocition"

set.seed(123)
dt1 <- tibble(
    x = rnorm(100),
    y = -3 * x + rnorm(100)
)

dt1 %>%
    ggplot(aes(x = x, y = y)) +
    geom_point() +
    geom_vline(xintercept = mean(dt1$x), colour = "firebrick4") +
    geom_hline(yintercept = mean(dt1$y), colour = "steelblue4")
```

```{r}
cov(dt1$x, dt1$y)
```

:::{#def-correlation}
## Correlation

The correlation between two variables $x$ and $y$ with $n$ values is defined as

$$
r_{xy} = \frac{S_{xy}}{S_x S_y}
$$

where $S_{xy}$ is the covariance between $x$ and $y$, and $S_x$ and $S_y$ are the standard deviations of $x$ and $y$ respectively.

:::

Because the covariance is divided by the product of the standard deviations, the correlation is unit-free. Furthermore, the correlation is always between -1 and 1. A correlation of 1 means that the two variables lie on a straight line with a positive slope. A correlation of -1 means that the two variables lie on a straight line with a negative slope. A correlation of 0 means that there is no *linear* association between the two variables.

```{r}
cor(dt$x, dt$y)
cor(dt1$x, dt1$y)
```

## Exercise

Given two variables $x$ and $y$, compute the regression coefficients $\hat{\beta}_0$ and $\hat{\beta}_1$ using the formulas above (XXX, insert reference).

```{r}
ols_two_variables <- function(x, y){
    # Compute the coefficients
    beta_1_hat <- 
    beta_0_hat <- 

    list(beta_0_hat = beta_0_hat, beta_1_hat = beta_1_hat)
}
```

Test your function using the following data set.

```{r}
set.seed(123)

x <- rnorm(100)
y <- 2 * x + rnorm(100)
```

Compare your results with the `lm` function in R.

```{r}
# Uncomment the following line to compare your results with the lm function

# ols_two_variables(x, y)
lm(y ~ x)
```

## Least Squares in Matrix Notation

The previous discussion can be summarized in matrix notation and you will commonly find it in this form in textbooks and articles.

For the one variable case, the prediction equation, written for all observations is

$$
\begin{align*}
\hat{y}_1 & = \hat{\beta}_0 + \hat{\beta}_1 x_1 \\
\hat{y}_2 & = \hat{\beta}_0 + \hat{\beta}_1 x_2 \\
\vdots & \\
\hat{y}_n & = \hat{\beta}_0 + \hat{\beta}_1 x_n \\
\end{align*}
$$

This can be written in matrix notation as

$$
\underbrace{\begin{bmatrix}
\hat{y}_1 \\
\hat{y}_2 \\
\vdots \\
\hat{y}_n
\end{bmatrix}
}_{\hat{y}_{n \times 1}}
=
\underbrace{\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{bmatrix}}_{X_{n \times 2}}
\underbrace{\begin{bmatrix}
\hat{\beta}_0 \\
\hat{\beta}_1
\end{bmatrix}}_{\hat{\beta}_{2 \times 1}}
$$

$$
\hat{y} = X \hat{\beta}
$$

You can think about the least squares as finding a solution to a system of linear equations where the number of equations is greater than the number of unknowns. Let us focus on a case with one variable and two observations (the first two from the `invoices` data set).

```{r}
invoices %>%
 head(n = 2)
```


$$
2.1 = \hat{\beta}_0 + \hat{\beta}_1 149 \\
1.8 = \hat{\beta}_0 + \hat{\beta}_1 60
$$

In this system of equations you have two unknowns, $\hat{\beta}_0$ and $\hat{\beta}_1$, and two equations. Therefore, you can actually solve for the unknowns (left as an exercise).

```{r}
M <- matrix(c(1, 149, 1, 60), ncol = 2, byrow = TRUE)
y <- c(2.1, 1.8)
solve(M, y)
```

You will find that the solution is
$$
\begin{align*}
\hat{\beta}_0 \approx 1.598 \\
\hat{\beta}_1 \approx 0.0034
\end{align*}
$$

Let's look at the first three observations in the `invoices` data set.

```{r}
invoices %>%
 head(n = 3)
```

The equations for the first three observations are

$$
\begin{align*}
2.1 & = \hat{\beta}_0 + \hat{\beta}_1 149 \\
1.8 & = \hat{\beta}_0 + \hat{\beta}_1 60 \\
2.2 & = \hat{\beta}_0 + \hat{\beta}_1 201
\end{align*}
$$

Substituting the solution for $\hat{\beta}_0$ and $\hat{\beta}_1$ that we
obtained from the first two equations into the third equation, we get

```{r}
1.597752809 + 0.003370787 * 201
```

which is not exactly equal to 2.2, therefore the third equation is not satisfied.

In the general case we cannot hope to solve the system of equations exactly. What we could hope for is to find an approximate solution which is close to the
actual left hand side values. The least squares method provides such a solution
by defining a sense of `closeness` between the approximate solution and the actual left hand side values (data) and then finding the approximate solution that minimizes that `closeness` measure.
