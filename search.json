[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear regression analysis",
    "section": "",
    "text": "General Information",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to linear regression analysis",
    "section": "Schedule",
    "text": "Schedule\n\nTu 10:15-11:45 in room 308 (will move to room 303 after 27.02.2024)\nTu 12:15-13:45 in room 308\nWe 14:15-15:45 in room 308",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Introduction to linear regression analysis",
    "section": "Grading",
    "text": "Grading\nTBA",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#github-repository",
    "href": "index.html#github-repository",
    "title": "Introduction to linear regression analysis",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nAll course materials for the exercise classes will be available in the GitHub repository:\nhttps://github.com/febse/econ2024",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#software-setup",
    "href": "index.html#software-setup",
    "title": "Introduction to linear regression analysis",
    "section": "Software Setup",
    "text": "Software Setup\nThe exercise classes require a minimal software setup:\n\nInstall Git for you operating system from https://git-scm.com/downloads/.\nOpen a GitHub account at https://github.com/signup. While you don’t need an account to download the course materials, you will need one to receive and submit your assignments (this will be explained in details during the exercise classes). You can apply for the GitHub student benefits at https://education.github.com/benefits. If approved you can receive free access to the GitHub Pro plan and to GitHub Copilot, an AI tool that helps you write code.\nOpen https://cran.r-project.org/, and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system, you will see a link “Install R for the first time .” Click on this link and then download the R installer. Run the installer. Leave the default settings unchanged unless you know what you are doing.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system, you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nShould you encounter difficulties installing R and R Studio, you can watch these video guides:\n\n\nWindows\nMac\nUbuntu 22.04\n\n\nHere are some video guides on how to install git:\n\n\nWindows\nMac\nLinux\n\n\nThe following steps depend on git being installed. Open R Studio and open a new project dialog: File -&gt; New Project. In the dialog, click on the third option: version control. From the next menu, select git.\n\n  \nIn the Repository URL field, paste the address of the course repository:\n\n\n\n\n\n\nRepository URL\n\n\n\nInsert the following address in the Repository URL field:\nhttps://github.com/febse/econ2024.git\nthe one shown in the screenshot is outdated.\n\n\n Click on the Create Project button and wait for git R studio to clone the repository and open the project.\n\n\n\n\n\n\nRenv\n\n\n\nTo install the packages necessary for the course, click on the command line in the R console and type:\nrenv::restore()\nThis will trigger the download and installation of all the dependencies. It can take some time, so be patient. You only need to do it once for the project,\n\n\n\n\n\nStep 5\n\n\n 6. The content of the GitHub repository will be updated continuously throughout the semester. In order to download the new files or updated versions of already existing files, you can use git pull. Open the git window in the upper right pane of R studio and click the pull button. This will download all changes from the GitHub repository to your local copy.\n\n\n\nStep 7\n\n\n\nNote that if you have modified the files tracked by git that have changed in the repository, git pull will fail with an error similar to this one:\n\n\n\n\nPull error\n\n\nTo avoid this, you can roll back the file to its original state. Right-click on the file in the git window and choose “revert.”\n\n\n\nRevert\n\n\n\nIn the exercise classes, we will use many functions from the tidyverse system and several other packages. Before accessing these packages’ functionality, you need to install them first. Find the R console in R studio and paste the following line on the command line. Press enter to run it and wait for the installation to complete. The renv package will take care of the installation of the packages in a separate environment, so you should’t worry about installing packages.\n\nIn case it does not work, you can install the packages manually by running the following command in the R console:\ninstall.packages(c(\"tidyverse\", \"broom\", \"patchwork\", \"GGally\", \"caret\", \"plotly\"))\nOptional: more on Quarto: https://quarto.org/docs/guide/\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "00-Recommended-Literature.html",
    "href": "00-Recommended-Literature.html",
    "title": "Recommended reading",
    "section": "",
    "text": "The R for data science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) book covers data management and exploration using the tools from the tidyverse packages.\nDalgaard (2008): Introductory statistics with R. 2nd edition. New York: Springer. This textbook offers an introductory level to probability and basic linear models with a focus on the software (R) and with only a minimal discussion of the mathematical fundamentals. The code shown is base R and mostly differs from what we are using in the classes (tidyverse).\nSheather (2009): A Modern Approach to Regression with R. New York, NY: Springer New York (Springer Texts in Statistics). Available at: https://doi.org/10.1007/978-0-387-09608-7.\nThe Textbook focuses on linear regression models and develops the mathematical fundamentals alongside with examples using R.\nGelman, Hill, and Vehtari (2021): Regression and other stories. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press (Analytical methods for social research). Available at: https://doi.org/10.1017/9781139161879.\nAnother introduction into basic probability theory and linear models. The references to Bayesian inference in the book are not relevant to our course.\nFaraway (2015): Linear models with R. Second edition. Boca Raton: CRC Press, Taylor & Francis Group.\nAnother introduction into linear models with examples in R.\n\n\n\n\n\nDalgaard, Peter. 2008. Introductory Statistics with R. 2nd edition. Statistics and Computing. New York: Springer.\n\n\nFaraway, Julian James. 2015. Linear Models with R. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press, Taylor & Francis Group.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with R. Edited by George Casella, Stephen Fienberg, and Ingram Olkin. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly.",
    "crumbs": [
      "Recommended reading"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html",
    "href": "01-Introduction-to-R.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Arithmetic Operations\n1 + 4\n\n[1] 5\n\n3 - 2\n\n[1] 1\n\n2 * 8\n\n[1] 16\n\n2 / 8\n\n[1] 0.25\n\n2^4\n\n[1] 16",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#assignment",
    "href": "01-Introduction-to-R.html#assignment",
    "title": "1  Introduction to R",
    "section": "1.2 Assignment",
    "text": "1.2 Assignment\nVery often we want to store a value in the memory of the computer so that we can reuse it later. In R we store values under names (variables) by using the assignment operator `&lt;-` Shortcut for the assignment operator: Alt - (minus)\n\ny &lt;- 34\ny - 40\n\n[1] -6\n\n\nRun this chunks and look at the global environment (right side of R Studio) to see it appear\nin the list of objects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#numeric-vectors",
    "href": "01-Introduction-to-R.html#numeric-vectors",
    "title": "1  Introduction to R",
    "section": "1.3 Numeric Vectors",
    "text": "1.3 Numeric Vectors\nIt is very common to group values that belong together in a single structure. By default numeric vectors in R are created as double precision floating point numbers. You can create a numeric vector using the c (concatenate) function.\n\nx &lt;- c(1, 4)\nx\n\n[1] 1 4\n\n\nTo see the type of a variable, you can use the typeof function.\n\ntypeof(x)\n\n[1] \"double\"\n\n\n\n## Length, average, sum of a numeric vector\nmean(x)\n\n[1] 2.5\n\nsum(x)\n\n[1] 5\n\nlength(x)\n\n[1] 2\n\n\n\n## Documentation\n# ?mean\n\nVectors can only hold data of the same type: either numeric, character, or logical. If you try to create a vector with different types of data, R will coerce the data to the same type. For example, if you try to create a vector with a number and a string, R will coerce the number to a string.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#character-vectors",
    "href": "01-Introduction-to-R.html#character-vectors",
    "title": "1  Introduction to R",
    "section": "1.4 Character Vectors",
    "text": "1.4 Character Vectors\nYou can assign character vectors to variables as well. String literals are enclosed in quotes. It does not matter if you use single or double quotes, but you have to be consistent.\n\nz &lt;- \"Hello, world!\"\n\nThe c function can be used to create character vectors as well.\n\nz1 &lt;- c(\"Hello\", \"world!\")\nz1\n\n[1] \"Hello\"  \"world!\"\n\n\nStrings can be concatenated using the paste function.\n\npaste(z1, collapse = \",\")\n\n[1] \"Hello,world!\"\n\n\n\npaste(z1, \"some string\", sep = \" \")\n\n[1] \"Hello some string\"  \"world! some string\"\n\n\nR is different from other programming languages in that it is vectorized. This means that most functions are designed to work with vectors. For example, the paste function can take a vector of strings as input and return a vector of strings as output.\nUnlike other languages, using length on a variable holding a string will not return the number of characters in the string, but the number of elements in the vector. To count the number of characters in a string, you can use the nchar function.\n\nlength(z)\n\n[1] 1\n\n\n\nnchar(z)\n\n[1] 13\n\n\nAs a lot of other function, nchar is vectorized, meaning that it can take a vector of strings as input and return a vector of integers as output.\n\nnchar(c(\"Hi\", \"world!\", \"Some longer sentence\"))\n\n[1]  2  6 20",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#recycling",
    "href": "01-Introduction-to-R.html#recycling",
    "title": "1  Introduction to R",
    "section": "1.5 Recycling",
    "text": "1.5 Recycling\nIf you try to perform an operation on two vectors of different lengths, R will recycle the shorter vector to match the length of the longer vector. This is called recycling or broadcasting.\nLet’s run an example to see how recycling works. We want to add a scalar to each element of a vector. Mathematically, this does not make sense, because you can only add/subtract element-wise two vectors of the same length. However, R will recycle the scalar to match the length of the vector. Recycling means that it creates a new vector by repeating the shorter vector until it has the same length as the longer vector.\n\nc(2, 5) + 1\n\n[1] 3 6\n\n\n\nc(2, 3, 5, 7) + c(10, 20)\n\n[1] 12 23 15 27\n\nc(2, 3, 5, 7) / c(10, 20)\n\n[1] 0.20 0.15 0.50 0.35\n\n\nTake care when using recycling, because it can lead to unexpected results. For example, if you try to add two vectors of different lengths, R will still recycle the shorter vector to match the length of the longer vector, but it will also issue a warning.\n\nc(1, 2, 10) + c(2, 5)\n\nWarning in c(1, 2, 10) + c(2, 5): longer object length is not a multiple of\nshorter object length\n\n\n[1]  3  7 12\n\n\nPay attention to the warning message. It is telling you that the shorter vector is being recycled to match the length of the longer vector but it cannot expand the shorter vector to match the longer vector exactly. Although this is not an error that will stop your program, most of the time, this is not what you want and is a result from some error before it. You should not rely on recycling vectors of incompatible lengths. Instead, you should be explicit about what you want to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "href": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "title": "1  Introduction to R",
    "section": "1.6 Logical Operators and Logical Values",
    "text": "1.6 Logical Operators and Logical Values\nThere are two logical values: TRUE and FALSE. These emerge from logical operations and indicate whether some condition is fulfilled (TRUE) or not FALSE. You will find similar constructs in all other languages, where this type of data is commonly known as boolean or binary (i.e., only two values).\nThe basic logical operators in R are\n\n## Less than\n2 &lt; 5\n\n[1] TRUE\n\n## Less than or equal\n2 &lt;= 5\n\n[1] TRUE\n\n## Greater than\n2 &gt; 5\n\n[1] FALSE\n\n## Greater or equal\n2 &gt;= 5\n\n[1] FALSE\n\n## Exactly equal\n2 == 5\n\n[1] FALSE\n\n\"Text 2\" == \"Text 2\"\n\n[1] TRUE\n\n\n\nz == \"Text 2\"\n\n[1] FALSE\n\n\n\n\n\n\n\n\nStrict Equality and Floating Point Numbers\n\n\n\nStrict equality generally makes sense for strings and integers, but not for floating point numbers! This is because real numbers cannot be stored exactly in memory and computers work with finite precision. This can lead to unexpected results when comparing floating point numbers. For example, you may get a result like this:\n\nsqrt(2)^2 == 2\n\n[1] FALSE\n\n\nMathematically, \\sqrt(2)^2 is exactl yequal to 2, but the comparison in R returns FALSE.\nWhen printing the number in the console you may not see the difference because the print function formats the number. To compare floating point numbers, you should use the all.equal function, which takes into accounts for the finite precision of floating point numbers.\n\nall.equal(sqrt(2)^2, 2)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#indexing",
    "href": "01-Introduction-to-R.html#indexing",
    "title": "1  Introduction to R",
    "section": "1.7 Indexing",
    "text": "1.7 Indexing\nYou can access elements of a vector using the square brackets. The index of the first element is 1, not 0 (this is different from many other programming languages). You can also use negative indices to exclude elements from the vector.\n\nexpenses &lt;- c(100, 200, 300, 400, 500)\nexpenses[1]\n\n[1] 100\n\n\n\nexpenses[2:4]\n\n[1] 200 300 400\n\n\n\nexpenses[-1]\n\n[1] 200 300 400 500\n\n\nYou can also use logical vectors to index a vector.\n\nexpenses[c(TRUE, FALSE, TRUE, FALSE, TRUE)]\n\n[1] 100 300 500\n\n\nBe careful when using logical vectors to index a vector. If the logical vector is shorter than the vector you are indexing, R will recycle the logical vector to match the length of the vector you are indexing.\n\n(1:10)[c(TRUE, FALSE)]\n\n[1] 1 3 5 7 9",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#factors",
    "href": "01-Introduction-to-R.html#factors",
    "title": "1  Introduction to R",
    "section": "1.8 Factors",
    "text": "1.8 Factors\nFactors are used to represent categorical data (e.g., sex: male/femal, employment status: employed, unemployed, retired, etc.). They are stored as integers and have labels associated with them. Factors are important in statistical modeling and are commonly used in plotting functions. Factors are not strings, they are integers with labels.\n\nsome_vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\")\nsome_factor &lt;- factor(some_vector)\nsome_factor\n\n[1] A B A C B A\nLevels: A B C\n\nlevels(some_factor)\n\n[1] \"A\" \"B\" \"C\"\n\n\nYou can coerce (convert) a factor to an integer vector using the as.integer function.\n\nas.integer(some_factor)\n\n[1] 1 2 1 3 2 1\n\n\nYou can coerce (convert) as factor to a character vector using the as.character function.\n\nas.character(some_factor)\n\n[1] \"A\" \"B\" \"A\" \"C\" \"B\" \"A\"\n\n\nFactors have also some safeguards. If you try to perform an operation that is not defined for factors, R will issue a warning. For example, you cannot meaningfully add a number to a factor. Note that this will only raise a warning, not an error.\n\nsome_factor + 1\n\nWarning in Ops.factor(some_factor, 1): '+' not meaningful for factors\n\n\n[1] NA NA NA NA NA NA\n\n\n\nsome_factor[1] &lt;- \"Some undefined level\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 1, value = \"Some undefined level\"): invalid\nfactor level, NA generated\n\nsome_factor\n\n[1] &lt;NA&gt; B    A    C    B    A   \nLevels: A B C\n\n\nRead more about factors in the R documentation, as well as the section on factors here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#functions",
    "href": "01-Introduction-to-R.html#functions",
    "title": "1  Introduction to R",
    "section": "1.9 Functions",
    "text": "1.9 Functions\nFunctions are a fundamental building block most languages, including R. They are used to carry out a specific task and encapsulate a sequence of steps. You can avoid repeating the same code over and over again by abstracting the code into a function.\nYou can think of a function as a recipe. It takes some ingredients (arguments) and returns a dish (output).\nLet’s look at some examples of functions in R, without going into too much detail.\nTo define a function, you use the function keyword and assing the function to a variable. The function is then called by using the variable name and passing the arguments in parentheses. The function has a body that is enclosed in curly braces.\nLet’s write a function that takes two numbers and returns TRUE or FALSE depending on whether the sum of the arguments is odd. Functions return the value of the last expression in the body of the function unless you explicitly use the return keyword.\n\nis_even_sum &lt;- function(x, y) {\n  (x + y) %% 2 == 0\n}\n\nis_even_sum(2, 3)\n\n[1] FALSE\n\nis_even_sum(2, 4)\n\n[1] TRUE\n\n\nThe %% operator is the modulo operator. It returns the remainder of the division of the first number by the second number.\n\n2 %% 2\n\n[1] 0\n\n3 %% 2\n\n[1] 1\n\n\nWriting functions is a large topic and we will not cover all the details in this course. You can find more information about functions in R in the R documentation.\nA thing to note is that R is (mostly) a functional programming language and functions generally do not modify their arguments. This means that if you pass a variable to a function and the function modifies that variable, the original value will not be changed. This is different from many other languages, where functions can modify their arguments. The reason for this is that arguments are copied when they are passed to a function. This is done to avoid side effects and make the code easier to reason about.\n\nx &lt;- c(1, 2, 3)\n\nf &lt;- function(y) {\n  y[1] &lt;- 100\n  y\n}\n\nf(x)\n\n[1] 100   2   3\n\nx\n\n[1] 1 2 3\n\n\nThe function above assigns a value to the frst element of the vector y and returns the modified vector. However, the original vector x is not modified, because y is a copy of x.\n\ns1 &lt;- c(4, 1, 2, 3)\ns1\n\n[1] 4 1 2 3\n\ns1[1] &lt;- 100\ns1\n\n[1] 100   1   2   3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#data-frames",
    "href": "01-Introduction-to-R.html#data-frames",
    "title": "1  Introduction to R",
    "section": "1.10 Data Frames",
    "text": "1.10 Data Frames\nThe data for our course will most often be a table of values with columns representing measurements of different characteristics (variables) and rows representing different observations. The base data structure to store this kind of data in R is the data frame. In this course we will use a structure called tibble that is provided by the dplyr package, part of the tidyverse collection of packages.\nYou can create a data frame using the tibble function. The tibble function takes named arguments, where the names are the names of the columns and the values are the vectors that will be the columns of the data frame.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndt &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  employed = c(TRUE, FALSE, TRUE)\n)\n\ndt\n\n# A tibble: 3 × 3\n  name      age employed\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;   \n1 Alice      25 TRUE    \n2 Bob        30 FALSE   \n3 Charlie    35 TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html",
    "href": "02-Descriptive-Statistics.html",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "2.1 Data Frames (Tibbles)\nIn most of our work we will use data tables containing variables (columns) that describe characteristics of observations (rows). Most of the time we will use tibble objects to hold the data. tibble objects are a modern rewrite of the data.frame (an older object type for storing data).\nTo use it we need to load the dplyr package, a part of the tidyverse collection of packages.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nIn a limited number of cases we need to construct tables by hand. You can find out more about tibble here.\ndt &lt;- tibble(\n  ## Shorthand syntax for creating a sequence of integers from one to five\n  id = 1:5,\n  y = c(2, 2.5, 3, 8, 12)\n)\ndt\n\n# A tibble: 5 × 2\n     id     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1   2  \n2     2   2.5\n3     3   3  \n4     4   8  \n5     5  12\nMost of our data will come from external sources such as text files in a csv format. For the purpose of this course you don’t need to worry about reading these files, you will always have a starter code chunk that imports the data.\nThe earnings data set contains data on 1816 customers of a shopping mall. The customers have answered a short interview and gave information about their sex, age, ethnicity, annual income, weight and height.\nWe will use this data set to demonstrate some common operations and basic data summaries.\nearnings &lt;- read_csv(\"https://raw.githubusercontent.com/febse/data/main/econ/earnings.csv\")\n\nRows: 1816 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): ethnicity\ndbl (14): height, weight, male, earn, earnk, education, mother_education, fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nFirst we will convert the height and weight measurements from their original scales (inch, pound) to cm and kg. We will create two new columns with informative names using the mutate function.\nearnings &lt;- mutate(\n  earnings,\n  height_cm = 2.54 * height,\n  weight_kg = 0.45 * weight\n)\nearnings1 &lt;- select(earnings, height_cm, weight_kg)\nThe same code can be rewritten in a more convenient way using pipes.\nearnings1 &lt;- earnings %&gt;%\n  mutate(\n    height_cm = 2.54 * height,\n    weight_kg = 0.45 * weight\n  ) %&gt;%\n  select(height_cm, weight_kg)\nNote that the object holding the original data is unaffected by mutate and select. The reason for this is that functions in R generally do not change their arguments. If you want to add the two new columns to the original data set earnings, you need to overwrite it with an assignment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "href": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "height (numeric): Height in inches (1 inch = 2.54 cm)\nweight (numeric): Weight in pounds (1 pound \\approx 0.45 kilograms)\nmale (numeric): 1: Male, 0: Female\nearn (numeric): Annual income in USD\nearnk (numeric): Annual income in 1,000 USD\nethnicity (character): Ethnicity\nage (numeric): Age",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#basic-data-summaries",
    "href": "02-Descriptive-Statistics.html#basic-data-summaries",
    "title": "2  Descriptive Statistics",
    "section": "2.2 Basic data summaries",
    "text": "2.2 Basic data summaries\nThe first step in any data analysis is to gain an initial understanding of the context of the data and the distributions of the variables of interest. In this course our main focus will be on two features of the variables: their location and their variability (how different are the observations between each other).\n\n2.2.1 Location\nThe most important measure of location for us will be the empirical mean of a variable (arithmetic average). Let i index the observation in our data set from the first (i = 1) to the last i = n. In our case n = 1816: the number of all interviewed customers. We can represent the values of some (numeric) characteristic (e.g., the persons’ weight) as a vector of values x = (x_1, \\ldots, x_n). In this notation x_1 is the weight of the first customer in the data set (x_1 = 210 pounds). The arithmetic average is defined as the sum of all values divided by the number of observations:\n\n\\bar{x} = \\frac{1}{n}(x_1 + x_2 + \\ldots + x_n) = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\nLet us now compute the arithmetic average of weight and height. One way to access the columns of the data set earnings is to write the name of the data set and then after a $ sign the name of the column.\n\nmean(earnings$height)\n\n[1] 66.56883\n\nmean(earnings$weight, na.rm = TRUE)\n\n[1] 156.3052\n\n\nAnother measure of location is the (empirical) median. You can compute it using the median function.\n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe result is a median height of 66 inches. This means that about half of the customers were taller than 66 inches.\n\n\n2.2.2 Variability (Spread)\nThe next important feature of the data is its variability. It answers the following question: how different are the customers between each other with respect to body height (for example). There are numerous ways to measure variability.\nOne intuitive measure would be the range of the data, defined as the difference by the maximal observed height and the minimal observed height\n\nmin(earnings$height)\n\n[1] 57\n\nmax(earnings$height)\n\n[1] 82\n\nrange(earnings$height)\n\n[1] 57 82\n\n\n\nmax(earnings$height) - min(earnings$height)\n\n[1] 25\n\n\nAnother measure is the inter-quartile range. The quartiles are defined similar to the median. To see this lets use the example of body height. The first quartile of height (25-th percentile and 0.25 quantile are different names for the same thing) is the height for which about one quarter of the customers are shorter than it. You can compute it with the function quantile.\n\nquantile(earnings$height, 0.25)\n\n25% \n 64 \n\n\nAbout 25 percent of our customers were shorter than 64 inches.\nThe second quartile is the same as the median (two quarters).\n\nquantile(earnings$height, 0.5)\n\n50% \n 66 \n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe third quartile is the height for which three quarter of the customers are shorter than it.\n\nquantile(earnings$height, 0.75)\n\n  75% \n69.25 \n\n\nIn our case about three quarter of the customers were shorter than 69.25 inches.\nThe inter-quartile range is simply the difference between the third quartile and the second quartile.\n\nquantile(earnings$height, 0.75) - quantile(earnings$height, 0.25)\n\n 75% \n5.25 \n\n\nAbout half of our customers had a hight between the first quartile (64 inches) and the third quartile (69.25 inches). The inter-quartile range shows you the difference between the height of the talles person and the shortest person for the central 50 percent of the customers.\nAs the range, the inter-quartile range is a measure of variability.\nThe most important measure of variability and the one that will be central to our analysis is the (empirical) variance.\n\nDefinition 2.1 (Empirical Variance) For a vector of values x = (x_1, \\ldots, x_n) it is defined as the average (apart from a small correction in the denominator) squared deviation of the values from their mean.\n\nS^2_x = \\frac{(x_1 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x})^2}{n - 1} = \\frac{1}{n - 1} \\sum_{i = 1}^{n}(x_i - \\bar{x})^2: \\quad \\text{variance}\\\\\nS_x = \\sqrt{S^2_x} \\quad \\text{standard deviation}\n\n\n\nExample 2.1 (Computing the empirical variance) Lets apply the formula from Definition 2.1 to a very simple example with just three values.\n\nx_1 = -1, x_2 = 0, x_3 = 4\n\nFirst, the empirical mean of these values is\n\n\\bar{x} = \\frac{-1 + 0 + 4}{3} = 1\n\nNow lets substitute these values in the definition of the empirical variance:\n\n\\begin{aligned}\nS^2_{x} & = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 }{n - 1} \\\\\n        & = \\frac{(-1 - 1)^2 + (0 - 1)^2 + (4 - 1)^2 }{3 - 1} \\\\\n        & = \\frac{(-2)^2 + (- 1)^2 + (3 )^2 }{2} \\\\\n        & = \\frac{4 + 1 + 9 }{2} \\\\\n        & = \\frac{14}{2} \\\\\n        & = 7\n\\end{aligned}\n\nUsing R to compute the same thing:\n\n# Create a vector called x\nx &lt;- c(-1, 0, 4)\n# Compute the average of the values in x and store it in a variable called x_avg (look it up in the global environment)\n\nx_avg &lt;- mean(x)\n\n# Manually compute the variance of x\n((-1 - x_avg)^2 + (0 - x_avg)^2 + (4 - x_avg)^2) / (length(x) - 1)\n\n[1] 7\n\n\nThere is also a special function var that can compute it from a vector\n\nvar(x)\n\n[1] 7\n\n\nThe (empirical) standard deviation is simply the square root of the (empirical) variance.\n\nS_x = \\sqrt{S^2_x} = \\sqrt{7} \\approx 2.64\n\nIn R you have two options: take the square root of the result of var using the sqrt function or use sd (standard deviation) to compute the standard deviation directly.\n\nsqrt(var(x))\n\n[1] 2.645751\n\nsd(x)\n\n[1] 2.645751\n\n\n\n\nExercise 2.1 (Empirical Moments (Mean and Variance))  \n\nCompute the sample mean and variance of the annual earnings of the customers in the earnings data set. First, compute it by accessing the columns of the data using the dollar syntax.\n\n\nThen use the summarize function from the dplyr (already loaded) package to compute the mean and the variance of the earn variable.\n\n\nRun the same calculations as before, but this time group the dataset by the ethnicity variable. This will give you the mean and the variance of the annual earnings for every ethnic group in the data set. Use the group_by function.\n\n\n\n\n2.2.3 Overview of the data\nThe skim function from the skimr package provides a quick overview of the data set. It shows the number of observations, the number of variables, the names of the variables, the type of the variables, the number of missing values, the mean, the standard deviation, the minimum and maximum values, and the quartiles for the numeric variables.\n\n## Basic summaries for the whole tibble\nearnings %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nethnicity\n0\n1\n5\n8\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1.00\n66.57\n3.83\n57.00\n64.00\n66.00\n69.25\n82.00\n▂▇▅▁▁\n\n\nweight\n27\n0.99\n156.31\n34.62\n80.00\n130.00\n150.00\n180.00\n342.00\n▅▇▃▁▁\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nearn\n0\n1.00\n21147.30\n22531.77\n0.00\n6000.00\n16000.00\n27000.00\n400000.00\n▇▁▁▁▁\n\n\nearnk\n0\n1.00\n21.15\n22.53\n0.00\n6.00\n16.00\n27.00\n400.00\n▇▁▁▁▁\n\n\neducation\n2\n1.00\n13.24\n2.56\n2.00\n12.00\n12.00\n15.00\n18.00\n▁▁▁▇▃\n\n\nmother_education\n244\n0.87\n13.61\n3.22\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nfather_education\n295\n0.84\n13.65\n3.25\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nwalk\n0\n1.00\n5.30\n2.60\n1.00\n3.00\n6.00\n8.00\n8.00\n▃▁▃▁▇\n\n\nexercise\n0\n1.00\n3.05\n2.32\n1.00\n1.00\n2.00\n5.00\n7.00\n▇▁▁▁▃\n\n\nsmokenow\n1\n1.00\n1.75\n0.44\n1.00\n1.00\n2.00\n2.00\n2.00\n▃▁▁▁▇\n\n\ntense\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nangry\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nage\n0\n1.00\n42.93\n17.16\n18.00\n29.00\n39.00\n56.00\n91.00\n▇▇▃▃▁\n\n\nheight_cm\n0\n1.00\n169.08\n9.73\n144.78\n162.56\n167.64\n175.89\n208.28\n▂▇▅▁▁\n\n\nweight_kg\n27\n0.99\n70.34\n15.58\n36.00\n58.50\n67.50\n81.00\n153.90\n▅▇▃▁▁\n\n\n\n\n\n\n\n2.2.4 Summarizing a single categorical variable\nWhile the mean, median, and variance are useful for numeric variables, they are not defined for categorical variables. Instead, we can use the table function to count the number of observations in each category.\n\ntable(earnings$ethnicity)\n\n\n   Black Hispanic    Other    White \n     180      104       38     1494",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#visualizations",
    "href": "02-Descriptive-Statistics.html#visualizations",
    "title": "2  Descriptive Statistics",
    "section": "2.3 Visualizations",
    "text": "2.3 Visualizations\nHistogram\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nA smooth density plot is an alternative way to visualize the distribution of a variable.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density()\n\n\n\n\n\n\n\n\nThe boxplot shows the median and the 25-th and 75-th percentiles (the box). The whiskers in the plot stretch to the minimum or the maximum observed value, unless there are extreme observations that are shown as single dots.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nGroup comparisons\n\nearnings %&gt;%\n  ggplot(aes(x = height, y = ethnicity)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe scatterplot will be our primary tool in studying associations between variables. It represents each observation as a point in a coordinate system defined by the variables that we would like to study.\n\nearnings1 %&gt;%\n  ggplot(aes(x = weight_kg, y = height_cm)) +\n  geom_point(position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Weight (kg)\",\n    y = \"Height (cm)\"\n  )\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create a scatterplot\nfig &lt;- plot_ly(earnings1, x = ~weight_kg, y = ~height_cm, mode = 'markers')\nfig\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nWarning: Ignoring 27 observations\n\n\n\n\n\n\n\nsummary(lm(height_cm ~ weight_kg, data = earnings1))\n\n\nCall:\nlm(formula = height_cm ~ weight_kg, data = earnings1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.639  -5.611  -0.084   5.645  36.248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 145.00916    0.89138  162.68   &lt;2e-16 ***\nweight_kg     0.34314    0.01237   27.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.15 on 1787 degrees of freedom\n  (27 observations deleted due to missingness)\nMultiple R-squared:  0.3009,    Adjusted R-squared:  0.3005 \nF-statistic: 769.1 on 1 and 1787 DF,  p-value: &lt; 2.2e-16\n\n\n\nExercise 2.2  \n\nCreate a new variable bmi (body mass index) in the earnings data set. The BMI is defined as the weight in kilograms divided by the square of the height in meters. The height in meters is the height in centimeters divided by 100.\n\n\nearnings &lt;- earnings %&gt;%\n  mutate(\n    bmi = weight_kg / (height_cm / 100)^2\n  )\n\n\nThe reference ranges for the BMI are as follows:\n\n\nUnder 18,5: Underweight\n18,5 - 24,9: Normal\n25 - 29,9: Overweight\n30 oder mehr: Obese\n\nCreate a two new variables is_underweight, is_normal in the dataset. Hint: Use the mutate function and the &lt;, &lt;=, &gt;, &gt;= operators. You can join multiple conditions using the & (logical and) operator. Don’t forget to uncommend the code first (Ctrl+Shift+C). How many customers are underweight and how many have a normal weight? Use the summarize function to compute the number of customers in each category.\n\nearnings &lt;- earnings %&gt;%\n  mutate(\n    is_underweight = bmi &lt; 18.5,\n    is_normal = bmi &gt;= 18.5 & bmi &lt;= 24.9\n  )\n\nearnings %&gt;%\n  summarize(\n    n_underweight = sum(is_underweight, na.rm = TRUE),\n    n_normal = sum(is_normal, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  n_underweight n_normal\n          &lt;int&gt;    &lt;int&gt;\n1            87     1032\n\n\n\n\nGroup the data by male and compute the number of customers in each group, as well as the number of underweight and normal weight customers in each group. Additionally, compute the share of underweight and normal weight customers in each group. Hint: Use the group_by function and the summarize function. Don’t forget to uncommend the code first (Ctrl+Shift+C). The n() function can be used to compute the number of observations in each group.\n\n\nearnings %&gt;%\n  group_by(male) %&gt;%\n  summarize(\n    # Count the number of customers in each group\n    n = n(),\n    n_underweight = sum(is_underweight, na.rm = TRUE),\n    n_normal = sum(is_normal, na.rm = TRUE),\n    share_underweight = mean(is_underweight, na.rm = TRUE),\n    share_normal = mean(is_normal, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 6\n   male     n n_underweight n_normal share_underweight share_normal\n  &lt;dbl&gt; &lt;int&gt;         &lt;int&gt;    &lt;int&gt;             &lt;dbl&gt;        &lt;dbl&gt;\n1     0  1141            71      688            0.0637        0.618\n2     1   675            16      344            0.0237        0.510",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html",
    "href": "03-Least-Squares.html",
    "title": "3  Least Squares",
    "section": "",
    "text": "3.1 Least Squares\nThe previous discussion leads us to a question. Can we find values for the coefficients in the linear equation that minimize the MSE? The answer is yes. The method is called least squares.\nFirst, let us visualize the MSE as a function of the coefficients. For this purpose, we will simply calculate the MSE for a grid of values of the coefficients and plot the results (Figure 3.4)\nCode\n# NOTE: this code here is only for illustration purposes, you don't need to study it or understand it for the course\n\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\n# Create a grid of values for beta0_hat and beta1_hat\n\nbeta0_hat &lt;- seq(0.4, 1, length.out = 50)\nbeta1_hat &lt;- seq(0.001, 0.015, length.out = 50)\n\ndt &lt;- expand.grid(beta0_hat = beta0_hat, beta1_hat = beta1_hat) %&gt;%\n  mutate(\n    # Compute the RSS for each combination of beta0_hat and beta1_hat\n    RSS = map2_dbl(beta0_hat, beta1_hat, ~{\n      Time_hat &lt;- .x + .y * invoices$Invoices\n      sum((invoices$Time - Time_hat)^2)\n    })\n  )\n\nfig &lt;- plot_ly(\n  x=beta0_hat, \n  y=beta1_hat, \n  z = matrix(dt$RSS, nrow = length(beta0_hat), ncol = length(beta1_hat)),\n  ) %&gt;% \n    add_surface() %&gt;% \n    layout(\n        scene = list(\n          xaxis = list(title = \"beta1_hat\"),\n          yaxis = list(title = \"beta0_hat\"),\n          zaxis = list(title = \"RSS\")\n        )\n    )\n\nfig\n\n\n\n\n\n\n\n\nFigure 3.4: RSS as a function of the coefficients\nOur goal is to find the values of \\beta_0 and \\beta_1 that minimize the RSS. The method of least squares provides a formula for the coefficients that minimize the RSS.\nFirst, let us solve a simpler problem. Assume that the predictive equation is\n\\widehat{\\text{Time}}_i = \\hat{\\beta}_0\nThe MSE in this case is much simpler.\ninvoices_beta_0_dt &lt;- expand_grid(\n    beta0_hat = seq(0, 4, length.out = 100),\n    invoices\n)\n\nrss_dt &lt;- invoices_beta_0_dt %&gt;%\n  group_by(beta0_hat) %&gt;%\n  summarise(\n    RSS = sum((Time - beta0_hat)^2)\n  )\n\nrss_dt %&gt;%\n    ggplot(aes(x = beta0_hat, y = RSS)) +\n    geom_line()\n\n\n\n\n\n\n\nFigure 3.5: MSE as a function of beta0_hat in an intercept-only equation\nHow do we find the value of \\hat{\\beta}_0 that minimizes the MSE? We take the derivative of the MSE with respect to \\hat{\\beta}_0 and set it to zero.\nIn order to find the minimum of the MSE, we take the derivative of the MSE with respect to \\hat{\\beta}_0 and set it to zero. Instead of \\text{Time} we will use the shorter notation y_i and instead of \\hat{\\text{Time}}_i we will use the shorter notation \\hat{y}_i.\n\\begin{align*}\nRSS(\\hat{\\beta}_0) & = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n& = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0)^2\n\\end{align*}\nTry to find the value of \\hat{\\beta}_0 that minimizes the MSE. Hint: Take the derivative of the RSS with respect to \\hat{\\beta}_0, set it to zero and solve for \\hat{\\beta}_0.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#least-squares",
    "href": "03-Least-Squares.html#least-squares",
    "title": "3  Least Squares",
    "section": "",
    "text": "Solution for the intercept-only equation\n\n\n\n\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} RSS(\\hat{\\beta}_0) & =\n\\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0) \\cdot (-1) \\\\\n& = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0) \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 \\sum_{i=1}^n \\hat{\\beta}_0 \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0\n\\end{align*}\n\nSetting the derivative to zero, we get\n\n\\begin{align*}\n-2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0 & = 0 \\\\\n\\hat{\\beta}_0 & = \\frac{1}{n} \\sum_{i=1}^n y_i \\\\\n& = \\overline{y}\n\\end{align*}\n\nThe value of \\hat{\\beta}_0 that minimizes the MSE is thus just the average of the observed values of y_i.\n\n\n\n\nExercise 3.3 Consider the following equation for the predictions\n\n\\hat{y} = \\hat{\\beta}_1 x\n\nFind the value of \\hat{\\beta}_1 that minimizes the RSS. Hint: Take the derivative of the RSS with respect to \\hat{\\beta}_1, set it to zero and solve for \\hat{\\beta}_1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#the-one-variable-case-with-an-intercept",
    "href": "03-Least-Squares.html#the-one-variable-case-with-an-intercept",
    "title": "3  Least Squares",
    "section": "3.2 The one variable case with an intercept",
    "text": "3.2 The one variable case with an intercept\nNow let us consider the case where we have two variables, \\text{Invoices}_i and \\text{Time}_i. We want to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the RSS.\nThe prediction equation is\n\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\nThe RSS is (as above)\n\n\\begin{align*}\n\\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n  & = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n\\end{align*}\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n\\frac{\\partial}{\\partial \\hat{\\beta}_1} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) x_i = 0\n\\end{align*}\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe first order conditions for the minimum are\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n\\frac{\\partial}{\\partial \\hat{\\beta}_1} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) x_i = 0\n\\end{align*}\n\nThe first equation gives\n\n\\begin{align*}\n\\sum_{i=1}^n y_i - \\hat{\\beta}_0 n - \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = 0 \\\\\n\\hat{\\beta}_0 n + \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = \\sum_{i=1}^n y_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nThe second equation gives\n\n\\begin{align*}\n\\sum_{i=1}^n y_i x_i - \\hat{\\beta}_0 \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = 0 \\\\\n\\hat{\\beta}_0 \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nSubstituting the expression for \\hat{\\beta}_0 in the second equation, we get\n\n\\begin{align*}\n(\\overline{y} - \\hat{\\beta}_1 \\overline{x}) \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\overline{y} \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i}{\\sum_{i=1}^n x_i^2 - \\overline{x} \\sum_{i=1}^n x_i}\n\\end{align*}\n\n\n\n\nSimplifying the expression, we get\n\n\\begin{align*}\n\\hat{\\beta}_1 & = \\frac{\\overline{x y} - \\overline{x} \\cdot \\overline{y}}{\\overline{x^2} - \\overline{x}^2} \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nThe last expression may seem a bit complicated, but it is actually quite simple. It is just the ratio between the empirical covariance between x_i and y_i divided by the variance of x_i.\nThe empirical covariance between x_i and y_i is defined as the sum of the products of the deviations of x_i and y_i from their respective means, divided by the number of observations.\n\nDefinition 3.1 (Covariance) The covariance between two variables x and y with n values is defined as\n\nS_{xy} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n\n\n\nDefinition 3.2 (Variance decomposition) We have already defined the variance of a variable x as\n\nS_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})^2\n\nIt can be shown that the variance of x can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.\n\nS_x^2 = \\frac{n}{n  - 1}(\\overline{x_i^2} - \\overline{x}^2)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor the proof we just write out the square in the sum and rearrange the terms.\n\n\\begin{align*}\n(n - 1) S_x^2 & =  \\sum_{i=1}^n (x_i - \\overline{x})^2 \\\\\n& =  \\sum_{i=1}^n (x_i^2 - 2x_i \\overline{x} + \\overline{x}^2) \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 \\sum_{i=1}^n 1 \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x}^2 n + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - n \\overline{x}^2 \\\\\n& = n (\\overline{x_i^2} - \\overline{x}^2)\n\\end{align*}\n\n\n\n\n\nDefinition 3.3 (Covariance) The covariance between two variables x and y with n values is defined as\n\nS_{xy} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n\nMuch in the same way as the variance, the covariance can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.\n\nS_{xy} = \\frac{n}{(n - 1)}(\\overline{x y} - \\overline{x} \\overline{y})\n\nThe proof is similar to the proof for the variance decomposition.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nFor the proof we just write out the product in the sum and rearrange the terms.\n\n\\begin{align*}\n(n - 1) S_{xy} & = \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) \\\\\n& = \\sum_{i=1}^n x_i y_i - \\overline{x} \\sum_{i=1}^n y_i - \\overline{y} \\sum_{i=1}^n x_i + n \\overline{x} \\overline{y} \\\\\n& = n(\\overline{x y} - \\overline{x} \\overline{y})\n\\end{align*}\n\n\n\n\nTo understand what the covariance, consider the following scatterplot:\n\nset.seed(123)\ndt_pos_cov &lt;- tibble(\n    x = rnorm(100),\n    y = 2 * x + rnorm(100)\n)\n\ndt_pos_cov %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_vline(xintercept = mean(dt_pos_cov$x), colour = \"firebrick4\") +\n    geom_hline(yintercept = mean(dt_pos_cov$y), colour = \"steelblue4\")\n\n\n\n\n\n\n\nFigure 3.6: Scatterplot of two variables with a positive linear assocition\n\n\n\n\n\nThe reddish line is drawn at the average of the x values and the bluish line is drawn at the average of the y values. The covariance measures the average product of the deviations of the x and y values from their respective means. If the product is positive, it means that when x is above its average, y is also above its average. If the product is negative, it means that when x is above its average, y is below its average.\nYou can compute the empirical covariance between two variables using the cov function in R.\n\ncov(dt_pos_cov$x, dt_pos_cov$y)\n\n[1] 1.622745\n\n\nOnly the sign of the covariance is important. The magnitude of the covariance depends on the units of the variables. To make the covariance unit-free, we can divide it by the product of the standard deviations of the two variables. This gives us the correlation coefficient.\n\ncov(dt_pos_cov$x * 1000, dt_pos_cov$y)\n\n[1] 1622.745\n\ncov(dt_pos_cov$x , dt_pos_cov$y * 50)\n\n[1] 81.13723\n\n\n\nset.seed(123)\ndt_neg_cov &lt;- tibble(\n    x = rnorm(100),\n    y = -3 * x + rnorm(100)\n)\n\ndt_neg_cov %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_vline(xintercept = mean(dt_neg_cov$x), colour = \"firebrick4\") +\n    geom_hline(yintercept = mean(dt_neg_cov$y), colour = \"steelblue4\")\n\n\n\n\n\n\n\nFigure 3.7: Scatterplot of two variables with a positive linear assocition\n\n\n\n\n\n\ncov(dt_neg_cov$x, dt_neg_cov$y)\n\n[1] -2.54342\n\n\n\nDefinition 3.4 (Correlation) The correlation between two variables x and y with n values is defined as\n\nr_{xy} = \\frac{S_{xy}}{S_x S_y}\n\nwhere S_{xy} is the covariance between x and y, and S_x and S_y are the standard deviations of x and y respectively.\n\nBecause the covariance is divided by the product of the standard deviations, the correlation is unit-free. Furthermore, the correlation is always between -1 and 1. A correlation of 1 means that the two variables lie on a straight line with a positive slope. A correlation of -1 means that the two variables lie on a straight line with a negative slope. A correlation of 0 means that there is no linear association between the two variables.\n\ncor(dt_pos_cov$x, dt_pos_cov$y)\n\n[1] 0.8786993\n\ncor(dt_neg_cov$x, dt_neg_cov$y)\n\n[1] -0.9448502",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#exercise",
    "href": "03-Least-Squares.html#exercise",
    "title": "3  Least Squares",
    "section": "3.3 Exercise",
    "text": "3.3 Exercise\nWrite a function that takes two vectors x and y as arguments and returns the OLS coefficients \\hat{\\beta}_0 and \\hat{\\beta}_1 using the formulas above.\n\nols_two_variables &lt;- function(x, y){\n    # Compute the coefficients\n    beta_1_hat &lt;- (mean(x * y) - mean(x) * mean(y)) / (mean(x^2) - mean(x)^2)\n    beta_0_hat &lt;- mean(y) - beta_1_hat * mean(x)\n\n    c(beta_0_hat = beta_0_hat, beta_1_hat = beta_1_hat)\n}\n\nTest your function using the following data set.\n\nset.seed(123)\n\nx_test &lt;- rnorm(100)\ny_test &lt;- 1 + 2 * x_test + rnorm(100)\n\nprint(ols_two_variables(x_test, y_test))\n\nbeta_0_hat beta_1_hat \n 0.8971969  1.9475284 \n\nprint(lm(y_test ~ x_test))\n\n\nCall:\nlm(formula = y_test ~ x_test)\n\nCoefficients:\n(Intercept)       x_test  \n     0.8972       1.9475  \n\n\nCompare your results with the lm function in R.\n\n# Uncomment the following line to compare your results with the lm function\n\n# ols_two_variables(x, y)\nlm(y_test ~ x_test)\n\n\nCall:\nlm(formula = y_test ~ x_test)\n\nCoefficients:\n(Intercept)       x_test  \n     0.8972       1.9475",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#ols-using-the-lm-function-in-r",
    "href": "03-Least-Squares.html#ols-using-the-lm-function-in-r",
    "title": "3  Least Squares",
    "section": "3.4 OLS using the lm function in R",
    "text": "3.4 OLS using the lm function in R\nOur primary tool for calculating the least squares coefficients will be the lm function in R. The lm function takes a formula as its first argument and a data set as its second argument. The formula is of the form y ~ x where y is the dependent variable and x is the independent variable. The data set is a data frame/tibble with the variables x and y.\nUse the lm function to compute the OLS coefficients for the invoices data set. and the model:\n\n\\widehat{\\text{Time}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{Invoices}\n\n\n# Uncomment the following two lines to fit the model\n\nfit_OLS &lt;- lm(Time ~ Invoices, data = invoices)\nfit_OLS\n\n\nCall:\nlm(formula = Time ~ Invoices, data = invoices)\n\nCoefficients:\n(Intercept)     Invoices  \n    0.64171      0.01129  \n\n\n\n# Compute the predicted values for the days (observations) in the sample\n\npredict(fit_OLS)\n\n        1         2         3         4         5         6         7         8 \n2.3241648 1.3192085 2.7645390 0.9014177 2.9113303 1.2966252 1.5111665 3.1484549 \n        9        10        11        12        13        14        15        16 \n2.6854975 0.9804592 1.8837907 1.5789163 1.3192085 0.9240010 2.5951643 2.5499977 \n       17        18        19        20        21        22        23        24 \n2.7871223 3.2726630 3.9049950 1.1498339 2.8209972 1.4321250 3.3629961 1.8047492 \n       25        26        27        28        29        30 \n2.4822479 1.9967072 2.9113303 2.1660818 1.5450414 0.9691676 \n\n\n\n# Compute the RSS for the OLS model\n\ninvoices &lt;- invoices %&gt;%\n    mutate(\n      Time_predicted_OLS = predict(fit_OLS),\n      residuals_OLS = Time - Time_predicted_OLS\n    )\n\n\n# Compute the RSS and MSE for the OLS model\n\ninvoices %&gt;%\n  summarize(\n    RSS_OLS = sum(residuals_OLS^2),\n    MSE_OLS = mean(residuals_OLS^2)\n  )\n\n   RSS_OLS   MSE_OLS\n1 3.045013 0.1015004",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#plot-the-ols-predictions",
    "href": "03-Least-Squares.html#plot-the-ols-predictions",
    "title": "3  Least Squares",
    "section": "3.5 Plot the OLS predictions",
    "text": "3.5 Plot the OLS predictions\n\n# Plot the predictions\n\ninvoices %&gt;%\n    ggplot(aes(x = Invoices, y = Time)) +\n    geom_point() +\n    geom_line(aes(y = Time_predicted_OLS), color = \"steelblue4\")\n\n\n\n\n\n\n\n\n\n# Plot the predictions using geom_smooth\n\ninvoices %&gt;%\n    ggplot(aes(x = Invoices, y = Time)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#interpretation-of-the-ols-coefficients",
    "href": "03-Least-Squares.html#interpretation-of-the-ols-coefficients",
    "title": "3  Least Squares",
    "section": "3.6 Interpretation of the OLS coefficients",
    "text": "3.6 Interpretation of the OLS coefficients\n\nScale of the coefficients\nInterpretation of the intercept\nInterpretation of the slope",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#least-squares-in-matrix-notation",
    "href": "03-Least-Squares.html#least-squares-in-matrix-notation",
    "title": "3  Least Squares",
    "section": "3.7 Least Squares in Matrix Notation",
    "text": "3.7 Least Squares in Matrix Notation\nThe previous discussion can be summarized in matrix notation and you will commonly find it in this form in textbooks and articles.\nFor the one variable case, the prediction equation, written for all observations is\n\n\\begin{align*}\n\\hat{y}_1 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 \\\\\n\\hat{y}_2 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_2 \\\\\n\\vdots & \\\\\n\\hat{y}_n & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_n \\\\\n\\end{align*}\n\nThis can be written in matrix notation as\n\n\\underbrace{\\begin{bmatrix}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\vdots \\\\\n\\hat{y}_n\n\\end{bmatrix}\n}_{\\hat{y}_{n \\times 1}}\n=\n\\underbrace{\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}}_{X_{n \\times 2}}\n\\underbrace{\\begin{bmatrix}\n\\hat{\\beta}_0 \\\\\n\\hat{\\beta}_1\n\\end{bmatrix}}_{\\hat{\\beta}_{2 \\times 1}}\n\nThe matrix X is called the design matrix and contains the predictor terms. The vector \\hat{\\beta} contains the coefficients that we want to estimate. The vector \\hat{y} contains the predicted values.\nYou can extract the design matrix from the fit_OLS object using the model.matrix function.\n\nfit_OLS &lt;- lm(Time ~ Invoices, data = invoices)\nmodel.matrix(fit_OLS)\n\n   (Intercept) Invoices\n1            1      149\n2            1       60\n3            1      188\n4            1       23\n5            1      201\n6            1       58\n7            1       77\n8            1      222\n9            1      181\n10           1       30\n11           1      110\n12           1       83\n13           1       60\n14           1       25\n15           1      173\n16           1      169\n17           1      190\n18           1      233\n19           1      289\n20           1       45\n21           1      193\n22           1       70\n23           1      241\n24           1      103\n25           1      163\n26           1      120\n27           1      201\n28           1      135\n29           1       80\n30           1       29\nattr(,\"assign\")\n[1] 0 1\n\n\nThe colum of ones in the design matrix is the intercept term. The other column is the predictor term (the number of invoices in our example).\n\n\\hat{y} = X \\hat{\\beta}\n\nYou can think about the least squares as finding a solution to a system of linear equations where the number of equations is greater than the number of unknowns. Let us focus on a case with one variable and two observations (the first two from the invoices data set).\n\ninvoices %&gt;%\n head(n = 2)\n\n  Day Invoices Time Time_hat_eq1 residuals_eq1 Time_hat_eq2 residuals_eq2\n1   1      149  2.1         2.11         -0.01         2.09          0.01\n2   2       60  1.8         2.11         -0.31         1.20          0.60\n  Time_predicted_OLS residuals_OLS\n1           2.324165    -0.2241648\n2           1.319209     0.4807915\n\n\n\n2.1 = \\hat{\\beta}_0 + \\hat{\\beta}_1 149 \\\\\n1.8 = \\hat{\\beta}_0 + \\hat{\\beta}_1 60\n\nIn this system of equations you have two unknowns, \\hat{\\beta}_0 and \\hat{\\beta}_1, and two equations. Therefore, you can actually solve for the unknowns (left as an exercise).\n\nM &lt;- matrix(c(1, 149, 1, 60), ncol = 2, byrow = TRUE)\ny &lt;- c(2.1, 1.8)\nsolve(M, y)\n\n[1] 1.597752809 0.003370787\n\n\nYou will find that the solution is \n\\begin{align*}\n\\hat{\\beta}_0 \\approx 1.598 \\\\\n\\hat{\\beta}_1 \\approx 0.0034\n\\end{align*}\n\nLet’s look at the first three observations in the invoices data set.\n\ninvoices %&gt;%\n head(n = 3)\n\n  Day Invoices Time Time_hat_eq1 residuals_eq1 Time_hat_eq2 residuals_eq2\n1   1      149  2.1         2.11         -0.01         2.09          0.01\n2   2       60  1.8         2.11         -0.31         1.20          0.60\n3   3      188  2.3         2.11          0.19         2.48         -0.18\n  Time_predicted_OLS residuals_OLS\n1           2.324165    -0.2241648\n2           1.319209     0.4807915\n3           2.764539    -0.4645390\n\n\nThe equations for the first three observations are\n\n\\begin{align*}\n2.1 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 149 \\\\\n1.8 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 60 \\\\\n2.2 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 201\n\\end{align*}\n\nSubstituting the solution for \\hat{\\beta}_0 and \\hat{\\beta}_1 that we obtained from the first two equations into the third equation, we get\n\n1.597752809 + 0.003370787 * 201\n\n[1] 2.275281\n\n\nwhich is not exactly equal to 2.2, therefore the third equation is not satisfied.\nIn the general case we cannot hope to solve the system of equations exactly. What we could hope for is to find an approximate solution which is close to the actual left hand side values. The least squares method provides such a solution by defining a sense of closeness between the approximate solution and the actual left hand side values (data) and then finding the approximate solution that minimizes that closeness measure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#the-geometry-of-least-squares",
    "href": "03-Least-Squares.html#the-geometry-of-least-squares",
    "title": "3  Least Squares",
    "section": "3.8 The Geometry of Least Squares",
    "text": "3.8 The Geometry of Least Squares\nIn the previous section we derived the formula using calculus and an objective function (the RSS) that we wanted to minimize. In this section we will derive the formula using a geometric approach.\nFirst, let us think about the data in terms of vectors in a \\mathbb{R}^n dimensional space (where n is the number of observations). For an easier visualization, let us consider the case n = 2 (only two observations) so that we can easily plot the data.\n\n\nCode\nif (!require(\"ggforce\")) {\n  install.packages(\"ggforce\")\n}\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading ggforce from CRAN ...             OK [1.8 Mb in 0.46s]\n- Downloading tweenr from CRAN ...              OK [449 Kb in 0.42s]\n- Downloading polyclip from CRAN ...            OK [115 Kb in 0.76s]\n- Downloading RcppEigen from CRAN ...           OK [1.8 Mb in 0.45s]\nSuccessfully downloaded 4 packages in 4.2 seconds.\n\nThe following package(s) will be installed:\n- ggforce   [0.4.2]\n- polyclip  [1.10-6]\n- RcppEigen [0.3.4.0.0]\n- tweenr    [2.0.3]\nThese packages will be installed into \"~/work/econ2024/econ2024/renv/library/R-4.3/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing tweenr ...                         OK [installed binary and cached in 0.6s]\n- Installing polyclip ...                       OK [installed binary and cached in 0.43s]\n- Installing RcppEigen ...                      OK [installed binary and cached in 0.72s]\n- Installing ggforce ...                        OK [installed binary and cached in 1.0s]\nSuccessfully installed 4 packages in 3 seconds.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggforce)\n\nX &lt;- c(2, 3) / 4\nY &lt;- c(1, 0.2)\n\nY_proj &lt;- X %*% Y / X %*% X * X\nY_min_Y_proj &lt;- Y_proj - Y\n\ndf &lt;- tibble(\n  x = c(0, 0, Y[1], 0, Y[1], Y[2], Y_proj[1], Y_proj[1]),\n  y = c(0, 0, Y[2], 0, X[2], Y[2], Y_proj[2], Y_proj[2]),\n  xend = c(X[1], Y[1], Y_proj[1], Y_proj[1], NA, NA, NA, NA),\n  yend = c(X[2], Y[2], Y_proj[2], Y_proj[2], NA, NA, NA, NA),\n  color = c('A', 'B', 'C', 'D', NA, NA, NA, NA)\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_segment(\n    aes(\n      xend = xend, yend = yend, \n      color = color\n      ),\n      arrow = arrow(length = unit(0.3,\"cm\")\n    )\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"x\",\n    x = 0.51, y = 0.77\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"y_hat = bx\",\n    x = 0.31, y = 0.6\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"y\",\n    x = 1.02, y = 0.19\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"r = y - bx\",\n    x = 0.76, y = 0.42\n  ) + \n  labs(\n    x = \"\",\n    y = \"\"\n  ) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 3.8: Vector projection\n\n\n\n\n\nWhat is the projection of the vector y onto the vector x? It is the vector that is closest to y and lies on the line spanned by x.\nTwo vectors are said to be orthogonal if their dot product is zero. The dot product of two vectors x and y is defined as\n\nx \\cdot y = \\sum_{i=1}^n x_i y_i\n\nAnother way to write the dot product is as a matrix product:\n\nx \\cdot y = x^T y\n\nwhere x^T is the transpose of x (meaning that x^T is a row vector).\nThe least squares method can be thought of as finding the vector y_{proj} that is closest to y and lies on the line spanned by x. The vector y - y_{proj} is the vector that is orthogonal to x and has the smallest length.\nThe vector y - y_{proj} is called the residual vector. The length of the residual vector is the square root of the sum of the squares of its elements. This is the same as the square root of the sum of the squares of the residuals.\nThe residual vector will be smallest in length when it is orthogonal to x. Therefore we can find a scalar \\hat{\\beta}_1 such that the residual vector is orthogonal to x.\nThe condition that the residual vector is orthogonal to x is that the dot product between the two vectors is equal to zero.\n\n\\begin{align*}\nx^T(y - \\hat{\\beta}_1 x) = 0 \\\\\nx^T y - \\hat{\\beta}_1 x^T x = 0 \\\\\n\\hat{\\beta}_1 x^T x = x^T y \\\\\n\\hat{\\beta}_1 = \\frac{x^T y}{x^T x}\n\\end{align*}\n\nYou can rewrite the dot products as sum to see the similarity to the formulas we derived using calculus.\n\n\\begin{align*}\nx^T y = \\sum_{i=1}^n x_i y_i \\\\\nx^T x = \\sum_{i=1}^n x_i^2\n\\end{align*}\n\n\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} = \\frac{n \\overline{x y}}{n \\overline{x^2}} = \\frac{\\overline{x y}}{\\overline{x^2}}\n\nIn the general case of multiple variables, the projection of the outcome vector y onto the space spanned by all linear combinations of the predictor variables X is\n\n\\begin{align*}\nX^T(y - X \\hat{\\beta}) = 0 \\\\\nX^T y - X^T X \\hat{\\beta} = 0 \\\\\nX^T X \\hat{\\beta} = X^T y \\\\\n\\end{align*}\n\nIn the one-variable case without an intercept term, we could devide the equation by x^T x to get the formula for \\hat{\\beta}_1. This worked because the scalar product of a vector is a scalar. However, here we need to deal with a whole matrix (X^TX). We can solve the equation for \\hat{\\beta} by pre-multiplying both sides by the inverse of X^T X.\n\n\\begin{align*}\n\\hat{\\beta} & = (X^T X)^{-1} X^T y\n\\end{align*}\n\nThe last operation requires that inverse of X^T X exists. This is the case if the columns of X are linearly independent. If some of the columns of X are linearly dependent, the matrix X^T X will be singular and its inverse will not exist.\n\n# Create t full column rank matrix\nx_fcr &lt;- matrix(\n  c(1, 149, 1, 60, 1, 201),\n  ncol = 2,\n  byrow = TRUE\n)\nx_fcr\n\n     [,1] [,2]\n[1,]    1  149\n[2,]    1   60\n[3,]    1  201\n\n\n\n# Compute the inverse of the matrix. The %*% operator is the matrix multiplication operator in R\n\nsolve(t(x_fcr) %*% x_fcr)\n\n            [,1]          [,2]\n[1,]  2.17013047 -1.343998e-02\n[2,] -0.01343998  9.834131e-05\n\n\n\n# To veritfy that the inverse is correct, multiply the matrix with its inverse\n\nt(x_fcr) %*% x_fcr %*% solve(t(x_fcr) %*% x_fcr)\n\n              [,1]          [,2]\n[1,]  1.000000e+00 -4.282599e-18\n[2,] -1.344411e-13  1.000000e+00\n\n\nWhat happens if we have a matrix that is not full column rank?\n\nx_rcr &lt;- matrix(\n  c(1, 149, 2, 1, 60, 2, 1, 149, 2),\n  ncol = 3,\n  byrow = TRUE\n)\nx_rcr\n\n     [,1] [,2] [,3]\n[1,]    1  149    2\n[2,]    1   60    2\n[3,]    1  149    2\n\n\n\n# solve(t(x_rcr) %*% x_rcr)\n\nIn the case of a matrix that is not full column rank, the inverse of the matrix does not exist, and solve will throw an error. We say that there is (perfect) multicollinearity in predictors.\nThe same will happen if some column is a linear combination (e.g. the sum) of some other columns.\n\nx_rcr1 &lt;- cbind(x_fcr, 0.2 * x_fcr[, 1] + 5 * x_fcr[, 2])\nx_rcr1\n\n     [,1] [,2]   [,3]\n[1,]    1  149  745.2\n[2,]    1   60  300.2\n[3,]    1  201 1005.2\n\n\n\n# solve(t(x_rcr1) %*% x_rcr1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html",
    "href": "04a-Probability-Review.html",
    "title": "4  Discrete Distributions (Review)",
    "section": "",
    "text": "4.1 Expectation\nThe expected value of a random variable is the average of all possible values that can occurr, weighted by their occurrence probabilities. It is a measure of the location of the distribution.\n\\begin{align}\n\\mu_x & = E(X) = \\sum_{x = 0}^{3} x p_X(x) = 0 \\times 0.250 + 1 \\times 0.095 + 2 \\times 0.272 + 3 \\times 0.383 = 1.788 \\\\\n\\end{align}\nmu_x &lt;- sum(px$x * px$p)\nmu_x\n\n[1] 1.788\n\nmean(smpl_x$x)\n\n[1] 2.2\npx %&gt;%\n  ggplot(aes(x = x, y = p)) + \n    geom_col() + \n    labs(\n      x = \"Outcome\",\n      y = \"Probability\"\n    ) + \n    geom_vline(xintercept = mu_x, color = \"firebrick\")\nIf you want to predict future values of a random variable, the expected value is your best guess in the sense that it minimizes the expected value of the quadratic loss function:\nE[(X - \\hat{x})^2]\nLet us construct an example. You need to predict the result of X and you think that the best prediction is \\bar{x} = 1. When the game runs it will produce four possible values: 0, 1, 2, and 3. The error that you will make is:\nL(x) = (x - 1)^2 =\n\\begin{cases}\n   (0 - 1)^2 = 1 & \\text{x = 0}\\\\\n   (1 - 1)^2 = 0 & \\text{x = 1}\\\\\n   (2 - 1)^2 = 1 & \\text{x = 2}\\\\\n   (3 - 1)^2 = 4 & \\text{x = 3}\n\\end{cases}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#expectation",
    "href": "04a-Probability-Review.html#expectation",
    "title": "4  Discrete Distributions (Review)",
    "section": "",
    "text": "Exercise 4.3 (Expected Value) Compute the expected value of Y.\n\n\nSolution. \n\n# Type your code here\n\n\n\n\n\n\n\nExercise 4.4 (Expected Quadratic Loss) Compute the expected quadratic loss for a prediction \\bar{x} = 1.5.\n\n\nSolution. \n\n## Type your code here\n\n# px %&gt;%\n#   summarise(\n#     loss = ?\n#   )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#variance",
    "href": "04a-Probability-Review.html#variance",
    "title": "4  Discrete Distributions (Review)",
    "section": "4.2 Variance",
    "text": "4.2 Variance\nThe variance of a random variable (distribution) measures how different the possible values that can occur are. Values that occur more often (have higher probability) under p_X receive a higher weight. Values that occur less frequently under p_X are given a lower weight in the sum.\n\n\\begin{align}\nVar(X) & = \\sum_{x = 0}^{3} (x - \\mu_x)^2 \\times p_X(x) \\\\\n       & = (0 - 1.788)^2 \\times 0.250 + (1 - 1.788)^2 \\times 0.095 + (2 - 1.788)^2\\times 0.272 + (3 - 1.788)^2 \\times 0.383 \\\\\n       & = (-1.788)^2 \\times 0.250 + (-0.788)^2 \\times 0.095 + (0.212)^2\\times 0.272 + (1.212)^2 \\times 0.383 \\\\\n       & = 3.196 \\times 0.250 + 0.620^2 \\times 0.095 + 0.044 \\times 0.272 + 1.468 \\times 0.383 \\\\\n       & \\approx 1.433\n\\end{align}\n\\tag{4.1}\n\n(px$x - mu_x)\n\n[1] -1.788 -0.788  0.212  1.212\n\n(px$x - mu_x)^2\n\n[1] 3.196944 0.620944 0.044944 1.468944\n\npx$p * (px$x - mu_x)^2\n\n[1] 0.79923600 0.05898968 0.01222477 0.56260555\n\nsum(px$p * (px$x - mu_x)^2)\n\n[1] 1.433056\n\n\nYou can see from Equation 4.1 that it is the expected value of the squared deviations from the expected value.\n\nVar(X) = E(X - E(X))^2\n\n\nDefinition 4.1 (Variance) The variance of a random variable (distribution) is a summary of the distribution and describes its spread: how different are the values that this distribution will generate.\n\nVar(X) = E[(X - E(X))^2] = E(X^2) - E(X)^2\n\n\n\nExercise 4.5 (Variance) Compute the Variance of Y.\n\n\nSolution. \n\n# Type your code here\n\n\n\nTheorem 4.1 (Properties of the Expectation) Let X be a random variable with expected value E(X), let Y be a random variable with expected value E(Y), and let a be a fixed constant (a \\in \\mathbb{R}). The following properties are true:\n\n\\begin{align}\nE(a) & = a \\\\\nE(aX) & = aE(X) \\\\\nE(X + Y) & = E(X) + E(Y)\n\\end{align}\n\nFurthermore, if X and Y are uncorrelated, then the expected value of the product of the two random variables equals the product of their expected values:\n\nE(XY) = E(X)E(Y)\n\n\n\n\n\n\n\n\nProof of the Properties of the Expectation\n\n\n\n\n\n\nE(a) = a\n\nThe expected value of a discrete variable X with possible outcomes x_1, x_2, \\ldots, x_n and probabilities p_1, p_2, \\ldots, p_n is given by\n\nE(X) = \\sum_{i = 1}^{n} x_i p_i\n\nMultiplying both sides by a gives\n\naE(X) = a\\sum_{i = 1}^{n} x_i p_i = \\sum_{i = 1}^{n} ax_i p_i\n\nThe right-hand side is the expected value of a random variable that takes the values ax_1, ax_2, \\ldots, ax_n with probabilities p_1, p_2, \\ldots, p_n. Therefore, E(aX) = aE(X).\n\nE(X + Y) = E(X) + E(Y)\n\nThis proof involves the joint distribution function of X and Y which we will introduce later. The proof is based on the linearity of the expected value operator.\n\n\n\n\nTheorem 4.2 (Properties of the Variance) Let X be a random variable with expected value E(X), let Y be a random variable with expected value E(Y), and let a be a fixed constant (a \\in \\mathbb{R}). The following properties are true:\n\nVar(X) = E(X^2) - E(X)^2\n\n\n\\begin{align}\nVar(a) & = 0 \\\\\nVar(aX) & = a^2 Var(X)\n\\end{align}\n\nFurthermore, if X and Y are uncorrelated, then the variance of their sum equals the sum of their variances:\n\nVar(X + Y) = Var(X) + Var(Y)\n\n\n\nExercise 4.6 (Expected value and variance) Use the distributions of X and Y from Table 4.1 and Table 4.2 to compute the expected value and the variance of\n\n2X + 3Y + 1.\n\nAssume that X and Y are independent.\n\n\nSolution. \nE(2X + 3Y + 1) = \\\\\nVar(2X + 3Y + 1) =",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#joint-distribution",
    "href": "04a-Probability-Review.html#joint-distribution",
    "title": "4  Discrete Distributions (Review)",
    "section": "4.3 Joint Distribution",
    "text": "4.3 Joint Distribution\nThe joint probability mass function tells you the probability of the simultaneous occurrence of x and y. For example, you can ask it the question: what is the probability of x = 2 and y = 3.\nFor two discrete variables, it is convenient to present the joint distribution as a table with cell entries holding the probabilities. The joint distribution is given in the tibble pxy in a long format.\n\npxy %&gt;%\n  knitr::kable()\n\n\n\n\nx\ny\np\n\n\n\n\n0\n2\n0.241\n\n\n0\n3\n0.009\n\n\n1\n2\n0.089\n\n\n1\n3\n0.006\n\n\n2\n2\n0.229\n\n\n2\n3\n0.043\n\n\n3\n2\n0.201\n\n\n3\n3\n0.182\n\n\n\n\n\nSometimes it is more convenient to see this distribution in a wide format:\n\npxy %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix=\"y=\"\n    ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 4.3: Joint distribution of X and Y.\n\n\n\n\n\n\nx\ny=2\ny=3\n\n\n\n\n0\n0.241\n0.009\n\n\n1\n0.089\n0.006\n\n\n2\n0.229\n0.043\n\n\n3\n0.201\n0.182\n\n\n\n\n\n\n\n\n\np_{XY}(x=2, y=3) = 0.043\n\nThe joint probability distribution function must sum (integrate) to one over all possible pairs of x and y.\n\n\\sum_{x = 0}^{3}\\sum_{y = 2}^{3} p_{XY}(x, y) = 1\n\n\nsum(pxy$p)\n\n[1] 1\n\n\nIn the example until now we have summarized the marginal distributions of X and Y but we have said nothing about their joint distribution. Usually the joint distribution is determined by the subject matter at hand, but for the sake of example we will look at two joint distributions so that we can get an idea how they work.\nFirst we will construct a special joint distribution under the assumption of independence. Intuitively, two random variables are independent, if the outcome of one of the variables does not influence the probability distribution of the other. Imagine that you hold two lottery tickets: one from a lottery in Germany and another from a lottery in Bulgaria. It would be safe to assume that the realized winnings from the German lottery will not affect the odds to win from the Bulgarian ticket.\nNow let us consider a case of dependent random variables. Let X be the level of a river (at some measurement point) at time t and Y be the level of the same river five minutes later. It would be safe to assume that if the level of the river was high at t this would affect the distribution of the level of the river at t plus five minutes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#marginal-distributions",
    "href": "04a-Probability-Review.html#marginal-distributions",
    "title": "4  Discrete Distributions (Review)",
    "section": "4.4 Marginal Distributions",
    "text": "4.4 Marginal Distributions\nThe marginal distribution of X is obtained by summing the joint distribution of X and Y over all possible values of Y.\n\np_X(x) = \\sum_{y=2}^{3}p_{XY}(x, y)\n\n\npxy %&gt;% \n  group_by(x) %&gt;% \n  summarise(p_x = sum(p))\n\n# A tibble: 4 × 2\n      x   p_x\n  &lt;dbl&gt; &lt;dbl&gt;\n1     0 0.25 \n2     1 0.095\n3     2 0.272\n4     3 0.383\n\n\n\nExercise 4.7 (Marginal distribution of Y) Compute the marginal distribution of Y from the joint distribution in Table 4.3. Use the pxy tibble and the group_by and summarise functions.\n\n# pxy %&gt;%\n#   group_by(...) %&gt;%\n#   summarise(p_y = sum(p))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#conditional-distributions",
    "href": "04a-Probability-Review.html#conditional-distributions",
    "title": "4  Discrete Distributions (Review)",
    "section": "4.5 Conditional Distributions",
    "text": "4.5 Conditional Distributions\n\npxy_w &lt;- pxy %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix = \"y=\"\n  ) %&gt;%\n  mutate(\n    p_x = `y=2` + `y=3`,\n    `y=2` = `y=2` / p_x,\n    `y=3` = `y=3` / p_x\n  )\n\npxy_w %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 4.4: Conditional distributions of Y given X\n\n\n\n\n\n\nx\ny=2\ny=3\np_x\n\n\n\n\n0\n0.964\n0.036\n0.250\n\n\n1\n0.937\n0.063\n0.095\n\n\n2\n0.842\n0.158\n0.272\n\n\n3\n0.525\n0.475\n0.383\n\n\n\n\n\n\n\n\nLooking at the conditional distributions of Y given X in Table 4.4, you should notice that these are not the same for each value of X. For example, Y=2 is much more likely when X = 0 compared to X = 3.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#joint-distribution-under-independence",
    "href": "04a-Probability-Review.html#joint-distribution-under-independence",
    "title": "4  Discrete Distributions (Review)",
    "section": "4.6 Joint Distribution under Independence",
    "text": "4.6 Joint Distribution under Independence\nLets construct the joint distribution p_{XY}(x, y) that assigns a probability to the points (x, y), assuming that X and Y are independent.\nFor independent random variables the joint probability of occurrence is simply the product of the marginal distributions.\n\np_{XY}(x, y) = p_X(x)p_Y(y)\n\n\npxy_ind &lt;- expand_grid(\n  px %&gt;% rename(p_x = p), \n  py %&gt;% rename(p_y = p)\n)\npxy_ind &lt;- pxy_ind %&gt;%\n  mutate(\n    p = p_x * p_y\n  )\n\n\npxy_ind_w &lt;- pxy_ind %&gt;%\n  pivot_wider(\n    id_cols = x,\n    names_from = y,\n    values_from = p,\n    names_prefix = \"y=\"\n  )\n\npxy_ind_w %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 4.5: Joint distribution of X and Y under independence.\n\n\n\n\n\n\nx\ny=2\ny=3\n\n\n\n\n0\n0.190\n0.060\n\n\n1\n0.072\n0.023\n\n\n2\n0.207\n0.065\n\n\n3\n0.291\n0.092\n\n\n\n\n\n\n\n\nLet’s look at the conditional distributions of Y given X. These answer the questions of the type: if X turns out to be 0, what are the probabilities for Y = 2 and Y = 3.\nTo get the conditional distributions of Y for each possible value of X we divide the cells of the joint distribution table by the marginal probabilities of each x.\n\np_{Y|X}(x, y) = \\frac{p_{XY}(x, y)}{p_X(x)}\n\n\npxy_ind_w %&gt;%\n  mutate(\n    p_x = `y=2` + `y=3`,\n    `y=2` = `y=2` / p_x,\n    `y=3` = `y=3` / p_x\n  ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 4.6: Conditional distributions of Y. Independence case.\n\n\n\n\n\n\nx\ny=2\ny=3\np_x\n\n\n\n\n0\n0.76\n0.24\n0.250\n\n\n1\n0.76\n0.24\n0.095\n\n\n2\n0.76\n0.24\n0.272\n\n\n3\n0.76\n0.24\n0.383\n\n\n\n\n\n\n\n\nIt is convenient to visualize the conditional distributions of Y given X.\n\npxy %&gt;%\n  ggplot(aes(y = factor(x), x = p, color=factor(y))) +\n  geom_col(position=\"dodge\") +\n  labs(\n    x = \"p(x | y)\",\n    y = \"x\",\n  ) + \n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat you should see in Table 4.6 is that the conditional distributions of Y are the same for every possible value of X. This is of course a consequence of the way we constructed this joint distribution in the first place: namely, we assumed that X and Y are independent.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#conditional-expectation",
    "href": "04a-Probability-Review.html#conditional-expectation",
    "title": "4  Discrete Distributions (Review)",
    "section": "4.7 Conditional Expectation",
    "text": "4.7 Conditional Expectation\nWe have seen how we derived the conditional distributions of Y given X in the previous section. Now we can ask the question: what is the expected value of Y given that X has already turned out to be 0 (for example). We can take the conditional distribution of Y given X = 0 and compute the expected value of this distribution.\nFor the joint distribution under independence:\n\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.76 + 3 \\times 0.24 = 2.24\n\n\n2 * 0.76 + 3 * 0.24\n\n[1] 2.24\n\n\nFor the joint distribution in Table 4.4 the conditional expectation of Y given X = 0 is\n\nE(Y | X=0) = \\sum_{y = 2}^{3} y p_{Y|X=0}(y) = 2 \\times 0.964 + 3 \\times 0.036 = 2.036\n\n\n2 * 0.964 + 3 * 0.036\n\n[1] 2.036\n\n\nLet us write the conditional expectation of Y for each possible value of X for the dependent joint distribution case.\n\nE(Y | X = x) = \\begin{cases}\n  2.036 & \\text{for } x = 0 \\\\\n  2.060 & \\text{for } x = 1 \\\\\n  2.158 & \\text{for } x = 2 \\\\\n  2.475 & \\text{for } x = 3\n\\end{cases}\n\n\npxy %&gt;%\n  group_by(x) %&gt;%\n  mutate(\n    p_y_x = p / sum(p)\n  ) %&gt;%\n  summarize(\n    E_Y_given_X = sum(y * p_y_x) \n  ) %&gt;%\n  knitr::kable(digits = 3)\n\n\n\nTable 4.7: Conditional expectation of Y for each possible value of X.\n\n\n\n\n\n\nx\nE_Y_given_X\n\n\n\n\n0\n2.036\n\n\n1\n2.063\n\n\n2\n2.158\n\n\n3\n2.475\n\n\n\n\n\n\n\n\nAn important thing to see here is that the conditional expectation is different for each value of X. As the value of X is uncertain (it is a random variable), the conditional expectation of Y given X is also a random variable. Its distribution is given by the possible values and the probabilities of occurrence of X (the marginal distribution of X).\n\nExercise 4.8 Calculate the expected value of Y given X for every possible value of X in the case joint distribution under independence.\n\n\nExample 4.2 (Sampling from the Joint Distribution)  \n\nsample_joint &lt;- pxy %&gt;%\n  slice_sample(n = 1000, weight_by = p, replace = TRUE)\n\nhead(sample_joint)\n\n# A tibble: 6 × 3\n      x     y     p\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     2 0.241\n2     3     3 0.182\n3     0     2 0.241\n4     3     3 0.182\n5     0     2 0.241\n6     2     2 0.229\n\n\n\nsample_joint %&gt;%\n  group_by(x, y) %&gt;%\n  summarise(\n    p = first(p),\n    n = n(),\n    f = n / nrow(sample_joint)\n  )\n\n`summarise()` has grouped output by 'x'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 8 × 5\n# Groups:   x [4]\n      x     y     p     n     f\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1     0     2 0.241   232 0.232\n2     0     3 0.009     7 0.007\n3     1     2 0.089    80 0.08 \n4     1     3 0.006     3 0.003\n5     2     2 0.229   241 0.241\n6     2     3 0.043    44 0.044\n7     3     2 0.201   204 0.204\n8     3     3 0.182   189 0.189",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#covariance",
    "href": "04a-Probability-Review.html#covariance",
    "title": "4  Discrete Distributions (Review)",
    "section": "4.8 Covariance",
    "text": "4.8 Covariance\nThe covariance measures the (linear) dependency between two random variables.\n\nDefinition 4.2 (Covariance) The covariance of two random variables X and Y is given by\n\nCov(X, Y) = E[(X - E(X))(Y - E(Y))]\n Alternatively, it can be computed using the decomposition formula:\n\nCov(X, Y) = E(XY) - E(X)E(Y)\n\n\nIn the analysis of time series we will often encounter situations where the expected value of one of the random variables is zero. As can be seen from the decomposition formula, in that case the covariance reduces to\n\nCov(X, Y) = E(XY).\n\nClosely related to the covariance is the correlation between X and Y.\n\nDefinition 4.3 (Correlation) \n\\rho(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}}\n It is easy to show that the correlation is bounded between -1 and 1.\n\n-1 \\leq \\rho(X, Y) \\leq 1\n\n\n\nExercise 4.9 (Correlation) Let X be a random variable with, and Y = a + bX. Show that the correlation between X and Y equals one or minus one depending on the sign of b. For simplicity, assume that E(X) = 0.\n\n\nTheorem 4.3 (Properties of the Covariance) Let X and Y be random variables and let a, b \\in \\mathbb{R} be fixed constants.\n\nVar(aX + bY) = a^2 Var(X) + b^2Var(Y) + 2abCov(X, Y)\n\n\n\nExercise 4.10 (Covariance) Compute the covariance of X and Y under the joint distributions given in Table 4.5 and Table 4.3. Use the pxy and pxy_ind tables for these calculations.\n\n\nSolution. \n\n# Type your code here",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Discrete Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html",
    "href": "04b-Continuous-Distributions.html",
    "title": "5  Continuous Distributions (Review)",
    "section": "",
    "text": "5.1 The Uniform Distribution on [-1, 1]\nX \\sim \\text{Uniform}(-1, 1)\nf(x) = \\begin{cases}\n\\frac{1}{2} & -1 \\leq x \\leq 1\\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\nThe density of the distribution is constant on the interval [-1, 1] and zero elsewhere. The mean of the distribution is zero and the variance is 1/3. The distribution function is linear on the interval [-1, 1].\nunif_dens_plt &lt;- ggplot() +\n  xlim(c(-2, 2)) +\n  stat_function(fun = dunif, args = list(min = -1, max = 1), n = 1000) +\n  labs(\n    x = \"x\",\n    y = \"Density\"\n  )\n\nunif_dens_plt\n\n\n\n\n\n\n\nFigure 5.1: Density of the uniform distribution on [-1, 1]\n\\begin{align*}\nE(X) = \\int_{-1}^{1} x f(x) dx = 0 \\\\\nVar(X) = \\int_{-1}^{1} \\left(x - E(X)\\right)^2 f(x) dx = \\frac{1}{3} \\\\\nSD(X) = \\sqrt{Var(X)} = 1/\\sqrt{3} \\\\\n\\end{align*}\nF(x) = \\int_{-\\infty}^{x} f(x) dx = \\begin{cases}\n0 & x &lt; -1 \\\\\n\\frac{x + 1}{2} & -1 \\leq x \\leq 1\\\\\n1 & x &gt; 1\n\\end{cases}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#the-uniform-distribution-on--1-1",
    "href": "04b-Continuous-Distributions.html#the-uniform-distribution-on--1-1",
    "title": "5  Continuous Distributions (Review)",
    "section": "",
    "text": "5.1.1 Sampling from the Uniform Distribution\n\n## Generate 10 random numbers from the uniform distribution on [-1, 1]\nx_unif &lt;- runif(10, min = -1, max = 1)\nx_unif\n\n [1]  0.06129698  0.20390429 -0.96051032 -0.20719079 -0.90593740  0.63888326\n [7] -0.94571472 -0.60967732 -0.68101519  0.23816951\n\n\n\n## Count the number of values less than zero\n\nsum(x_unif &lt; 0)\n\n[1] 6\n\n## Count the number of values greater than 0.5\n\n\n# Compute the probability of the event X &lt; 0 using the punif function\npunif(0, min = -1, max = 1)\n\n[1] 0.5\n\n\n\n# Compute the probability of the event X &gt; 0.5 using the punif function\n1 - punif(0.5, min = -1, max = 1)\n\n[1] 0.25\n\n\nCompare the result above with the probability of the event X &lt; 0.\n\n## Compute the average of the values (mean function)\n\n## Compute the variance of the values (var function)\n\nCompare the results above with the expected value and variance of the uniform distribution on [-1, 1].\nRerun the simulation with 10,000 values and compare the share of outcomes less than -1 with the probability of the event X &lt; -1 and the average and variance of the values with the expected value and variance of the uniform distribution on [-1, 1].",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#the-normal-distribution",
    "href": "04b-Continuous-Distributions.html#the-normal-distribution",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.2 The Normal Distribution",
    "text": "5.2 The Normal Distribution\nThe family of normal distributions is defined by two parameters: the mean \\mu and the standard deviation \\sigma. The density of the normal distribution is given by the formula:\n\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\nBecause we will use this distribution very often, we will introduce a special notation for the normal distribution:\n\nX \\sim N(\\mu, \\sigma^2)\n\nFor this course you don’t need to remember the density function of the normal distribution.\nThere are few properties of the normal distribution that you shold remember, though. The first two properties relate the expected value and the variance of the normal distribution to its parameters \\mu and \\sigma:\n\n\\begin{align*}\nE(X) & = \\mu \\\\\nVar(X) & = \\sigma^2\n\\end{align*}\n\n\nmeans &lt;- c(0, 0, 0, 2, 2, 2)\nsds &lt;- c(0.2, 0.5, 1, 0.2, 0.5, 1)\n\ndf &lt;- expand_grid(\n    mean = c(0, 2),\n    sd = c(0.2, 0.5, 1),\n    x = seq(-3, 5, length.out = 200)\n) %&gt;%\nmutate(\n    y = dnorm(x, mean = mean, sd = sd),\n    mean = paste0(\"mu = \", mean),\n    sd = paste0(\"sigma = \", sd)\n)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = mean, lty = sd)) +\n  geom_line() +\n  labs(x = \"x\", y = \"Density\", color = \"Parameters\") +\n  ggtitle(\"Normal distributions with different means and variances\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#how-does-the-normal-distribution-arise",
    "href": "04b-Continuous-Distributions.html#how-does-the-normal-distribution-arise",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.3 How Does the Normal Distribution Arise?",
    "text": "5.3 How Does the Normal Distribution Arise?\n\nplayers_n &lt;- 300\ngames_n &lt;- 16\n\nunif_games &lt;- expand_grid(\n  game = 1:games_n,\n  player = 1:players_n\n) %&gt;%\n  mutate(\n    ## When used in mutate, n() returns the number of rows in a group of obs\n    ## When the data is not grouped as here, it retuns the number of obs in the whole table\n    result = runif(n(), min = -1, max = 1)\n  ) %&gt;%\n  bind_rows(\n    ## Add a initial values so that all players start with 0\n    tibble(\n      player = 1:players_n,\n      game = as.integer(0),\n      result = 0,\n    )\n  )\n\nunif_games &lt;- unif_games %&gt;%\n  ## Sort the data by player id and game id\n  arrange(player, game) %&gt;%\n  ## Groups the data by player, because we want the running totals to be calculated for each\n  ## player separately\n  group_by(player) %&gt;%\n  mutate(\n    running_total = cumsum(result)\n  )\n\n## Illustration only\nunif_games %&gt;%\n  ggplot(aes(x = game, y = running_total, group = player)) +\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(color = player &lt; 2, alpha = player &lt; 2)) +\n  scale_color_manual(values = c(\"skyblue4\", \"firebrick4\")) +\n  scale_alpha_manual(values = c(1 / 5, 1)) +\n  scale_x_continuous(\"Game number\", breaks = c(0, 4, 8, 12, 16)) +\n  theme(legend.position = \"none\") +\n  labs(y = \"Running Total\")\n\n\n\n\n\n\n\n\n\nunif_games %&gt;%\n  filter(game == 4) %&gt;%\n  ggplot(aes(x = running_total)) +\n  geom_density() +\n  labs(title = \"Running total distribution at the 4-th game\") +\n  labs(\n    x = \"Running total\"\n  )\n\n\n\n\n\n\n\n\n\nunif_games %&gt;%\n  filter(game == 16) %&gt;%\n  ggplot(aes(x = running_total)) +\n  geom_density() +\n  labs(title = \"Running total distribution at the 16-th game\") +\n  labs(\n    x = \"Running total\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#probabilities-and-quantiles-of-the-normal-distribution",
    "href": "04b-Continuous-Distributions.html#probabilities-and-quantiles-of-the-normal-distribution",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.4 Probabilities and Quantiles of the Normal Distribution",
    "text": "5.4 Probabilities and Quantiles of the Normal Distribution\nAs with the other continuous distributions, we can compute probabilities and quantiles of the normal distribution using the functions pnorm and qnorm, respectively.\n\n\n\n\n\n\n\n\nFigure 5.2: Density of the standard normal distribution\n\n\n\n\n\nCompute the probability of the event X &lt; 0.5 for the standard normal distribution.\n\npnorm(0.5, mean=0, sd=1)\n\n[1] 0.6914625\n\n\nCompute the probability of the event X &gt; 1.96 for the standard normal distribution.\nCompute the probability of the event -1.3, 1 for the standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#sampling-from-the-normal-distribution",
    "href": "04b-Continuous-Distributions.html#sampling-from-the-normal-distribution",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.5 Sampling from the Normal Distribution",
    "text": "5.5 Sampling from the Normal Distribution\nTake a sample of 10 values from the standard normal distribution and store them in a variable x_norm.\n\nx_norm &lt;- rnorm(10, mean = 0, sd = 1)\n\n\n# Count the number of values less than zero\n\nsum(x_norm &lt; 0)\n\n[1] 3\n\n\nCompare the result with the theoretical probability of the event X &lt; 0.\n\n# Compute the average of the values\n\n# Compute the standard deviation of the values\n\nCompare your results with the expected value and variance of the standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "05-One-Way-ANOVA.html",
    "href": "05-One-Way-ANOVA.html",
    "title": "6  One Way ANOVA",
    "section": "",
    "text": "6.1 The Data\n## Warning: to compile the notes you need the \"bookdown\" and the \"broom\" packages. Install them by\n## running install.packages, see the commented lines below\n\nif (!require(\"tidyverse\")) {\n  install.packages(\"tidyverse\")\n}\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nif (!require(\"broom\")) {\n  install.packages(\"broom\")\n}\n\nLoading required package: broom\n\nif (!require(\"patchwork\")) {\n  install.packages(\"patchwork\")\n}\n\nLoading required package: patchwork\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'patchwork'\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading patchwork from CRAN ...           OK [3.1 Mb in 0.48s]\nSuccessfully downloaded 1 package in 0.81 seconds.\n\nThe following package(s) will be installed:\n- patchwork [1.2.0]\nThese packages will be installed into \"~/work/econ2024/econ2024/renv/library/R-4.3/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing patchwork ...                      OK [installed binary and cached in 1.0s]\nSuccessfully installed 1 package in 1.1 seconds.\n\nif (!require(\"latex2exp\")) {\n  install.packages(\"latex2exp\")\n}\n\nLoading required package: latex2exp\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'latex2exp'\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading latex2exp from CRAN ...           OK [1.5 Mb in 0.81s]\nSuccessfully downloaded 1 package in 0.95 seconds.\n\nThe following package(s) will be installed:\n- latex2exp [0.9.6]\nThese packages will be installed into \"~/work/econ2024/econ2024/renv/library/R-4.3/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing latex2exp ...                      OK [installed binary and cached in 0.67s]\nSuccessfully installed 1 package in 0.7 seconds.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(patchwork)\n\nkids &lt;- read_csv(\n  \"https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv\") %&gt;%\n  select(kid_score, mom_hs)\n\nRows: 434 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): kid_score, mom_hs, mom_iq, mom_work, mom_age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nkids %&gt;%\n  head() %&gt;%\n  knitr::kable()\n\n\n\nTable 6.1: First few rows of the data set kids.\n\n\n\n\n\n\nkid_score\nmom_hs\n\n\n\n\n65\n1\n\n\n98\n1\n\n\n85\n1\n\n\n83\n1\n\n\n115\n1\n\n\n98\n0\nVariables description:\nFor the sake of simplicity we will assume that the children in the sample (in the data set kids) were selected at random from all children living in the US at the time of the survey.\nResearch question: Do children whose mother did not finish a high school (mom_hs = 0) tend to achieve lower IQ scores compared to the children of mothers with a high school degree (mom_hs = 1).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One Way ANOVA</span>"
    ]
  },
  {
    "objectID": "05-One-Way-ANOVA.html#the-data",
    "href": "05-One-Way-ANOVA.html#the-data",
    "title": "6  One Way ANOVA",
    "section": "",
    "text": "kid_score: (numeric) Kid’s IQ score.\nmom_hs (numeric): This variable has only two possible values (a binary variable): 1 if the mother of the child has finished high school, and 0 otherwise.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One Way ANOVA</span>"
    ]
  },
  {
    "objectID": "05-One-Way-ANOVA.html#the-linear-regression-model-with-a-single-binary-predictor",
    "href": "05-One-Way-ANOVA.html#the-linear-regression-model-with-a-single-binary-predictor",
    "title": "6  One Way ANOVA",
    "section": "6.2 The Linear Regression Model with a single binary predictor",
    "text": "6.2 The Linear Regression Model with a single binary predictor\n\n## Where to get the data\nkids %&gt;%\n  ggplot(\n    aes(\n      ## Map the status of the mother to the y-axis.\n      ## The factor function converts mom_hs to a factor variable\n      ## so that ggplot would not treat mom_hs as a continuous variable.\n      y = factor(mom_hs),\n      ## Map the kid_score column to the x-axis.\n      x = kid_score\n  )\n  ) +\n  geom_point(\n    ## Add some noise to each observation so that we can \n    ## see the collection of dots. Without this noise\n    ## all dots would lie on two straight lines\n    position = \"jitter\"\n  ) +\n  ## Draws the two boxplots to help us see the centers and the spreads\n  ## of the distributions of the scores in the two groups\n  geom_boxplot(\n    ## Makes the boxplots transparent so that we can see the dots\n    ## behind them\n    alpha = 0.5\n  ) +\n  ## Sets human-readable labels for the two axes\n  labs(\n    x = \"IQ score\",\n    y = \"Status of the kid's mother\"\n  )\n\n\n\n\n\n\n\nFigure 6.1: IQ scores of the children by the status of their mother (high school degree/no high school degree). The plot adds a small amount of noise to the observations to avoid overplotting.\n\n\n\n\n\n\nExercise 6.1 Use the following code stub to compute the average IQ scores for the two groups of children in the kids data set.\n\n# Compute the average IQ scores for the two groups\n# Uncomment the following code and fill in the ellipsis\n\n# kids %&gt;%\n#   group_by(...) %&gt;%\n#   summarize(\n#     ... = mean(...)\n#   )\n\n\nLet us see how we can model the average IQ scores within a simple linear model.\n\n\\begin{align*}\n& i = 1,\\ldots, n = 434 \\text{ observations}\\\\\n& y_i: \\text{IQ score} \\\\\n& \\hat{y}_i: \\text{Predicted IQ score} \\\\\n& x_i \\in \\{0, 1\\}: \\text{status of the mother}\n\\end{align*}\n\nThe linear model is then\n\ny_i = \\beta_0 + \\beta_1 x_i + e_i, e_i \\sim N(0, \\sigma^2)\n\nThe predicted IQ score for the i-th child is\n\n\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\nAlternatively (and more generally) we can write the model as:\n\n\\begin{align*}\n& y_i \\sim N(\\mu_i, \\sigma^2), \\quad i = 1,\\ldots,n \\\\\n& \\mu_i = \\beta_0 + \\beta_1 x_i, \\quad x_i \\in \\{0, 1\\}\n\\end{align*}\n\\tag{6.1}\nLet us see what the model coefficients mean. This becomes obvious when we consider the expected value of y (the IQ score) for the two possible values of x. For x = 1 (children with a mother with a HS degree) the expected IQ score is:\n\n\\mu_1 = \\beta_0 + \\beta_1 \\cdot 1\n\\tag{6.2}\nFor x = 0 (children with a mother without a HS degree)\n\n\\mu_0 = \\beta_0 + \\beta_1 \\cdot 0\n\\tag{6.3}\nIf you take the difference between equations 6.2 and 6.3 you will get:\n\n\\begin{align*}\n\\beta_0 & = \\mu_0 \\\\\n\\beta_1 & = \\mu_1 - \\mu_0\n\\end{align*}\n\nTherefore, the constant in the model (\\beta_0) equals the population average IQ score of children in the x = 0 group. The slope coefficient \\beta_1 equals the difference between the population average IQ scores. Figure 6.2 visualizes the population of children (as the model in Equation 6.1 sees it). There are two groups of children in that population. The first has an average IQ score of \\mu_0, the second group has an average IQ score of \\mu_1. The children in both groups are normally distributed and the two distributions have the same standard deviation (spread).\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\n\n\n\nFigure 6.2: Distribution of IQ scores in the population of children according to Equation 6.1.\n\n\n\n\n\nWithin that model our research question boils down to examining \\beta_1 (the difference between the population averages). If \\beta_1 is positive, then children born to a mother with a HS degree would tend to perform better at the IQ test.\nThe first obstacle to answering the research question is that we don’t know the value of \\beta_1. Therefore, we rely on the sample to learn something about its value.\nIn the previous section we discussed the OLS estimator and now we will use it to find the best (in terms of lowest RSS) estimates for \\beta_0 and \\beta_1. As in the previous section will will find these using the lm function. We will also store the result of lm in an object so that we can use it later without running lm every time.\n\n## Run lm and save the output in an object called \"fit\"\nfit &lt;- lm(kid_score ~ 1 + mom_hs, data = kids)\n## Print the object storing the lm output\nfit\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nCoefficients:\n(Intercept)       mom_hs  \n      77.55        11.77  \n\n\nLook at the output of lm and write down the estimated regression equation:\n\n\\begin{align*}\n\\hat{\\mu} = 77.55 + 11.77 x\n\\end{align*}\n\nThis equation summarizes the sample. The children that we have selected and actually observed. For these children there was a difference of average IQ scores of 11.77. However, the sample only includes 434 children. A more interesting question is whether there is a difference between the average IQ scores between the two groups (mother with HS degree, mother without HS degree) in the population from which the sample was selected.\nAn interesting research hypothesis that we can test is whether the population coefficient \\beta_1 is equal to zero, for example against a two-sided alternative \\beta_1 \\neq 0.\n\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\nA test of this hypothesis against the alternative is a t-test with a test statistic\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 0}{se(\\hat{\\beta}_1)}\n\nIf \\beta_1 is really equal to zero, then this statistic follows a t-distribution with n - p degrees of freedom. The critical values of this test at \\alpha = 0.05 probability of wrong rejection of the null hypothesis.\nFor the kids data set you can test this hypothesis using the output of summary.\n\nsummary(fit)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nThe second row in the coefficients table corresponds to our \\hat{\\beta}_1 in our model notation (the coefficient of mom_hs). You can find the estimated standard error of \\hat{\\beta}_1 in the second column (Std. Error).\n\n\\text{t-statistic}^{obs} = \\frac{11.77 - 0}{2.322} = 5.0689\n\nYou can find this value in the column t value in the summary above. This t-statistic is shown in the regression output by almost all statistical software, because the hypothesis\n\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\nis particularly interesting. If true it implies that there are no population-level differences between the average IQ scores of the two groups of children.\nIn order to make a decision whether to reject this hypothesis or not, we compare the observed t-statistic to two critical values (because the alternative is two-sided).\nThe critical values at a five percent error probability (wrong rejection of a true null hypothesis) are derived from the t-distribution. You can find a detailed explanation in Chapter 7.\nThe quantiles of the t-distribution are\n\nalpha &lt;- 0.05\nqt(alpha / 2, df = nrow(kids) - 2)\n\n[1] -1.965471\n\nqt(1 - alpha / 2, df = nrow(kids) - 2)\n\n[1] 1.965471\n\n\nThe observed t-statistic equals 5.0689 and is therefore greater than the upper critical value (1.96). Therefore we reject the null hypothesis at a significance level 1 - \\alpha = 0.95 (this is another way to state the probability of wrong rejection of a true null hypothesis).\nApart from the point estimate for the difference of average IQ scores (11.77 points) we usually also want to communicate the uncertainty of this estimate. One description of that uncertainty is the (estimated) standard error of the estimator for the regression coefficient.\nOne way to construct a confidence interval for the coefficients is to invert the two-sided t-test (see Section 7.7 for more details).\nWe will use the critical values of the t-test at the 95 percent significance level to obtain an confidence interval (CI) with a 95 percent coverage probability:\n\nP\\left(\\hat{\\beta}_1 - 1.96 se(\\hat{\\beta}_1)  &lt; \\beta_1 &lt; \\hat{\\beta}_1 + 1.96 se(\\hat{\\beta}_1) \\right) = 0.95\n\\tag{6.4}\nNotice that the boundaries of the interval are random variables (because \\hat{\\beta}_1 and se(\\hat{\\beta}_1)) are random variables (they depend on the data). For the sample of children in kids we obtain estimates for these boundaries: [11.77 - 1.96 \\cdot 2.322; 11.77 + 1.96 \\cdot 2.322] \\approx [7.2; 16.3]. Based on these estimated boundaries we would say that range of plausible values for the difference between the population IQ score averages between 7.2 and 16.3.\nNotice that in interpreting the boundaries of the CI we don’t say that the probability of \\beta_1 being between 7.2 and 16.3 is 0.95. Generally this statement does not make any sense, because we have assumed that \\beta_1 is a fixed number (not random). The probability statement in Equation 6.4 does make sense, because the boundaries are random variables (depend on the data), but the statement P(7.2 &lt; \\beta_1 &lt; 16.3) = 0.95 does not!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One Way ANOVA</span>"
    ]
  },
  {
    "objectID": "05-One-Way-ANOVA.html#mathematical-details-optional",
    "href": "05-One-Way-ANOVA.html#mathematical-details-optional",
    "title": "6  One Way ANOVA",
    "section": "6.3 Mathematical Details (Optional)",
    "text": "6.3 Mathematical Details (Optional)\n\n6.3.1 The OLS Estimator in the Binary Predictor Case\nIn the case when x_i \\in \\{0, 1\\} the average of x, i.e \\overline{x} is simply the proportion of ones, lets say n_1 / n. Notice that the average of the squared x is the same as the average of x, because x only contains zeroes and ones (therefore x_i^2 = x_i). The average of the product of x and y is equal to the sum of y where the x = 1, because the other values in y are multiplied by zero.\n\n\\begin{align*}\n\\overline{x^2} = & \\frac{1}{n}\\sum_{i = 1}^{n} x_i^2 =  \\\\\n               = & \\frac{1}{n} \\sum_{i = 1}^n x_i \\\\\n               = & \\bar{x}\n\\end{align*}\n\n\n\\begin{align*}\n\\overline{xy} = & \\frac{1}{n} \\sum_{i = 1}^{n} x_i y_i = \\\\\n              = & \\frac{1}{n}\\left(\\sum_{i: x_i = 0} y_i \\cdot 0 + \\sum_{ i: x_i = 1} y_i \\cdot 1 \\right) \\\\\n              = & \\frac{1}{n}\\sum_{i: x_i = 1} y_i \\\\\n              = & \\frac{n_1}{n_1}\\frac{1}{n} \\sum_{i: x_i = 1} y_i \\\\\n              = & \\frac{n_1}{n} \\bar{y}_{1}\n\\end{align*}\n\nIn the last expression we use \\bar{y}_1 to denote the sample average of the x = 1 group.\nTherefore the expression for \\hat{\\beta}_1 simplifies to:\n\n\\hat{\\beta}_1 = \\frac{ \\frac{n_1}{n} \\overline{y}_{1} - \\overline{x}\\overline{y}}{\\overline{x} - \\overline{x}^2} \\\\\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\overline{y}}{1 - \\frac{n_1}{n}} \\\\\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\overline{y}}{\\frac{n - n_1}{n}} \\\\\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\overline{y}}{\\frac{n_0}{n}} \\\\\n\nFurthermore, the average of y is simply the weighted average of the two group means. Let there be n_0 observations with x_i = 0 and n_1 observations with x_i = 1.\n\n\\overline{y} = \\frac{1}{n}\\left(\\sum_{x_i = 0} y_i + \\sum_{x_i = 0} y_i\\right) \\\\\n\\overline{y} = \\frac{1}{n}\\left(n_0 \\overline{y}_0 + n_1 \\overline{y}_1 \\right)\n\nSubstituting gives us:\n\n\\hat{\\beta}_1 = \\frac{\\overline{y}_{1} - \\frac{1}{n}\\left(n_0 \\overline{y}_0 + n_1 \\overline{y}_1 \\right)}{\\frac{n_0}{n}} \\\\\n\nWhen you simplify the above expression (this is left as an exercise) you will arrive at:\n\n\\hat{\\beta}_1 = \\bar{y}_1 - \\bar{y}_0\n\nFrom the equation for \\hat{\\beta}_0 we get:\n\n\\hat{\\beta}_0 = \\bar{y} - (\\bar{y}_1 - \\bar{y}_0) \\bar{x}\n\nTaking into account that \\bar{x} = n_1 / n this equation simplifies to\n\n\\hat{\\beta}_0 = \\overline{y}_0\n\n\n\n6.3.2 What about the standard errors?\nIn the case of the simple ANOVA model the standard errors of both coefficients are pretty easy to derive, because the estimators are simply a group average (\\hat{\\beta}_0) and the difference between two group averages (\\hat{\\beta}_1).\nWe can compute the variance of \\hat{beta}_0. Note that when studying the statistical properties of an estimator like the OLS estimator for \\beta_0 we treat the data y_i as random variables. In the following derivation we assume that the y_i are not correlated.\n\n\\begin{align*}\nVar(\\hat{\\beta}_0) & = Var\\left(\\frac{1}{n_0}\\sum_{i: x_i = 0} y_i \\right) \\\\\n                   & = \\frac{1}{n_0^2} \\sum_{i: x_i = 0} Var(y_i) \\\\\n                   & = \\frac{1}{n_0^2} \\sum_{i: x_i = 0} \\sigma^2 \\\\\n                   & = \\frac{n_0 \\sigma^2}{n_0^2} \\\\\n                   & = \\frac{\\sigma^2}{n_0}\n\\end{align*}\n\\tag{6.5}\nFor the variance of \\hat{beta}_1 we obtain. In the derivation we use the assumption of y_i being uncorrelated and the fact that the variance of a difference of uncorrelated random variables equals the sum of their variances.\n\n\\begin{align*}\nVar(\\hat{\\beta}_1) & = Var\\left(\\frac{1}{n_1}\\sum_{i: x_i = 1} y_i - \\frac{1}{n_0}\\sum_{i: x_i = 0} y_i\\right) \\\\\n                   & = Var\\left(\\frac{1}{n_1}\\sum_{i: x_i = 1} y_i \\right) + Var\\left(\\frac{1}{n_0}\\sum_{i: x_i = 0} y_i\\right) \\\\\n                   & = \\frac{\\sigma^2}{n_1} + \\frac{\\sigma^2}{n_0} \\\\\n                   & = \\sigma^2 \\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)\n\\end{align*}\n\\tag{6.6}\nThe variances of the two coefficients involve the unknown parameter \\sigma^2. From the expressions for the variances you will notice that the variances increase in \\sigma^2. This should make sense intuitively. \\sigma^2 describes how much noise the data contains (random deviations from the regression line). Estimating the coefficients from noisy data will result in uncertain estimates (high variances).\nIn order to arrive to estimates the coefficient variances we need a way to estimate \\sigma^2 from the data. Here we will take a theorem from mathematical statistics that says that\n\n\\hat{\\sigma}^2 = \\frac{1}{n - p} \\text{RSS}\n\\tag{6.7}\nis an unbiased estimator for \\sigma. In the above expression n is the number of observations and p is the number of coefficients in the model. RSS is the residual sum of squares ?sec-ols-intro.\n\nE\\hat{\\sigma}^2 = \\sigma\n\nPlugging in this estimator for \\sigma^2 into Equation 6.5 and Equation 6.6 yields the following estimators for the variances of the model coefficients.\n\n\\begin{align*}\nVar(\\hat{\\beta}_0) = & \\frac{\\hat{\\sigma}^2}{n_0} \\\\\nVar(\\hat{\\beta}_1) = & \\hat{\\sigma}^2 \\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)\n\\end{align*}\n Let us compare the two formulas with the standard errors from the lm output (printed here again for convinience).\n\nsummary(fit)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nThe number of children in the two groups are n_0 = 93 and n_1 = 341. The number of observations is n = n_0 + n_1 = 434. The number of coefficients in the model is p = 2. We can compute the RSS by extracting the residuals from the model fit object (for example by using the residuals function).\n\nres &lt;- residuals(fit)\n\nsum(res ^ 2)\n\n[1] 170261.2\n\n\nThe estimate for \\sigma^2 is therefore:\n\n\\hat{\\sigma}^2 = \\frac{170261.2}{434 - 2} = 394.1231 \\\\\n\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2} = 19.85\n\nYou can find last value under residual standard error in the summary of the lm output above.\nNow let us compute the estimated variances and standard deviations of the coefficients:\n\n\\begin{align*}\nVar(\\hat{\\beta}_0) & = \\frac{\\hat{\\sigma}^2}{n_0} = \\frac{394.1}{93} = 4.23 \\\\\nse(\\hat{\\beta}_0) & = \\sqrt{Var(\\hat{\\beta}_0)} = \\sqrt{4.23} = 2.059 \\\\\nVar(\\hat{\\beta}_1) & = \\hat{\\sigma}^2 \\left(\\frac{1}{n_0} + \\frac{1}{n_1}\\right)  = 394.1 \\left(\\frac{1}{93} + \\frac{1}{341}\\right) = 5.39\\\\\nse(\\hat{\\beta}_1) & = \\sqrt{Var(\\hat{\\beta}_1)} = \\sqrt{5.39} = 2.32\n\\end{align*}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>One Way ANOVA</span>"
    ]
  },
  {
    "objectID": "06-Simulation.html",
    "href": "06-Simulation.html",
    "title": "7  A Simulation Study",
    "section": "",
    "text": "7.1 The population\nIn order to study the statistical properties of the OLS estimators \\hat{\\beta}_0 and \\hat{\\beta}_1 we will generate a large number of samples from the following model.\n\\begin{align*}\n& \\text{kid\\_score}_i \\sim N(\\mu_i, \\sigma^2 = 20^2), \\quad i = 1,\\ldots, N = 1e6 \\\\\n& \\mu_i = 80 + 15 \\text{mom\\_hs}_i, \\quad \\text{mom\\_hs} \\in \\{0, 1\\}\n\\end{align*}\n\\tag{7.1}\nThis model takes the sample of children from Chapter 6 for an inspiration. Basically, we will study a population that looks exactly like the sample of children in the previous example. However, the insights gained in the next sections are more general and are not tied to that specific sample.\n## Population parameters\n\nbeta0 &lt;- 80 # Average IQ of children with mom_hs = 0\nbeta1 &lt;- 15 # Difference in average IQ between children with mom_hs = 1 and mom_hs = 0\nsigma &lt;- 20 # Standard deviation of the IQ scores\nprop_ones &lt;- 0.8 # Proportion of children with mom_hs = 1\n\n## Population\nset.seed(123)\n\nN &lt;- 1e6 # Number of children in the population\n\npop &lt;- tibble(\n  mom_hs = rbinom(N, 1, prop_ones),\n  mu = beta0 + beta1 * mom_hs,\n  kid_score = rnorm(N, mu, sigma)\n) %&gt;%\n  select(-mu)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A Simulation Study</span>"
    ]
  },
  {
    "objectID": "06-Simulation.html#selecting-the-sample",
    "href": "06-Simulation.html#selecting-the-sample",
    "title": "7  A Simulation Study",
    "section": "7.2 Selecting the sample",
    "text": "7.2 Selecting the sample\nWe will select R = 1000 samples from the population. Each sample will have n = 434 children. We will store the samples in a data frame called sim.\n\n## Fix the random numbers generator so that you can reproduce your results\nset.seed(123)\n\nn &lt;- 434 # Sample size\nR &lt;- 1000 # Number of samples\n\nsim &lt;- pop %&gt;%\n  slice_sample(n = n * R, replace = TRUE) %&gt;%\n  mutate(\n    # We will add a column to identify the sample number\n    sample_id = rep(1:R, each = n)\n  )\n\nThe resulting data frame sim has 434 \\times 1000 = 434000 rows and contains the samples that we will use to study the statistical properties of the OLS estimators.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A Simulation Study</span>"
    ]
  },
  {
    "objectID": "06-Simulation.html#ols-estimates-in-each-sample",
    "href": "06-Simulation.html#ols-estimates-in-each-sample",
    "title": "7  A Simulation Study",
    "section": "7.3 OLS estimates in each sample",
    "text": "7.3 OLS estimates in each sample\nNow that we have the samples in the data set sim_samples we can compute the OLS estimates for \\beta_0 and \\beta_1 in each sample.\n\nsim_coeffs &lt;- sim %&gt;%\n  group_by(sample_id) %&gt;%\n  ## The tidy function reformats the output of lm so that it can fit in a data frame\n  do(tidy(lm(kid_score ~ 1 + mom_hs, data = .))) %&gt;%\n  select(sample_id, term, estimate, std.error, statistic)\n\n\n## Creates a separate table with the coefficients for mom_hs\nsim_slopes &lt;- sim_coeffs %&gt;%\n  filter(term == \"mom_hs\")\n\nThe last code chunk may seem a little bit complex, but is simply groups the sim_samples data by the sample number and then runs the lm function with the data in each sample. You can verify the results in sim_coeffs by running the lm function manually with the data from the first sample (of course you can choose another sample). The coefficient estimates in sim_coeffs are stored in a column called estimate. As there are two coefficients in our model, the column term tells you whether a row holds the estimate for the intercept \\hat{\\beta}_0 (term == \"(Intercept)\") or the slope \\hat{\\beta}_1 (term == \"mom_hs\").\nYou can use the filter function to select only the observations in the first sample\n\nsample_1 &lt;- sim %&gt;% filter(sample_id == 1)\n\nNow apply lm on that sample and compare the coefficient estimates with the first two values in sim_coeffs.\n\nlm(kid_score ~ 1 + mom_hs, data = sample_1)\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = sample_1)\n\nCoefficients:\n(Intercept)       mom_hs  \n      78.11        16.53",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A Simulation Study</span>"
    ]
  },
  {
    "objectID": "06-Simulation.html#distribution-of-the-ols-estimates",
    "href": "06-Simulation.html#distribution-of-the-ols-estimates",
    "title": "7  A Simulation Study",
    "section": "7.4 Distribution of the OLS estimates",
    "text": "7.4 Distribution of the OLS estimates\nFirst we plot the distribution of the slope estimates for each sample. In geom_point we add a small random value to each estimate so that we can see all the points (this is what position = \"jitter\" does). Otherwise all estimates would lie on the x-axis and we would not be able to see the individual points.\n\n# The red line is drawn at the population value of $\\beta_1$: 11.77. The  position of the dots on the y-axis does not convey any meaningful information and it only serves to  disentangle  the points so that they don't overplot.\n\nsim_slopes %&gt;%\n  ggplot(aes(x = estimate)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  ## Draws a density estimate\n  geom_density(color = \"steelblue\") +\n  ## Draws a vertical line a 11.77 (the value of beta_1 used in the simulation)\n  geom_vline(xintercept = 15, color = \"firebrick\") + \n  ## Sets the labels of the x and y axis and the title of the plot\n  labs(\n    x = \"Estimate\",\n    title = \"Distribution of the slope estimates in 1000 samples)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\nFigure 7.1: Sampling distribution of \\hat{\\beta}_1. Each dot represents the slope estimate in one sample.\n\n\n\n\n\nThe plot reveals three key insights:\n\nThe estimates for the slope (\\beta_1) vary from sample to sample\nIn most of the samples the estimate was close to the the real value of \\beta_1 (11.77)\nThere is a small number of samples that resulted in extreme values of the estimate, e.g. in sample 97 \\hat{\\beta}_1 = 4.7 and in sample 44 the estimated coefficient was 18.6.\n\nWe can estimate the center (expected value) of this distribution by computing the mean estimate (i.e. the average estimate over all generated samples).\n\nmean(sim_slopes$estimate)\n\n[1] 14.94834\n\n\nWe see that this value is very close to the real value of 11.77. This is a consequence of a property of the OLS estimator which we call unbiasedness ?thm-ols-expected-value. The average estimate computed above estimates the expected value of the distribution of \\hat{\\beta}_1: E\\hat{\\beta}_1.\nThe standard deviation of this distribution is called the standard error of \\hat{\\beta}_1. It describes the spread of the sampling distribution of the estimates.\n\nsd(sim_slopes$estimate)\n\n[1] 2.37398",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A Simulation Study</span>"
    ]
  },
  {
    "objectID": "06-Simulation.html#hypothesis-testing",
    "href": "06-Simulation.html#hypothesis-testing",
    "title": "7  A Simulation Study",
    "section": "7.5 Hypothesis testing",
    "text": "7.5 Hypothesis testing\nIn Chapter 6 we tested the hypothesis that \\beta_1 = 0 vs \\beta_1 \\neq 0 and talked about a t-statistic, a t-distribution and about critical values derived from that distribution. In the present section our goal is to demystify all these words.\nWe begin with a simple test of hypotheses about the population value of one of the regression coefficients. Let us start with \\beta_1 and let us suppose that we want to test the theory that the difference between the average IQ score of the two groups in the population equals exactly 11.77. Notice that this is the value of \\beta_1 that we used in the simulation, so this theory is correct. We also want to test this theory against the alternative that the difference between the average IQ scores is less than 11.77.\n\n7.5.1 Testing a true null hypothesis\n\nH_0: \\beta_1 \\geq 15\\\\\nH_1: \\beta_1 &lt; 15\n\\tag{7.2}\nThe mathematical statistics informs us how to summarize the data so that we can make a decision whether to reject the null hypothesis. The t-statistic is the summary that we need for this test. In general it is the difference between the estimate for \\beta_1 and the value under the null hypothesis divided by the standard error of the estimate.\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - \\beta_1^{H_o}}{se(\\hat{\\beta}_1)}\n\nIn our case the value of \\beta_1 under the null hypothesis is 15 so the test statistic becomes:\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 15}{se(\\hat{\\beta}_1)}.\n\nNotice that this statistic can be large (far away from zero) in two cases:\n\nThe null hypothesis is not true. As we have seen in Figure 7.1, the estimates \\hat{\\beta}_1 vary around the true value of \\beta_1 and this behavior does not depend on the hypothesis that we are testing.\nLarge values of the t-statistic can happen by chance alone, even if the null hypothesis is true. Such large values occur more often (in more samples) when the distribution of \\hat{\\beta}_1 has a high standard deviation.\n\nLet us compute the t-statistic for the hypothesis pair in 7.2. In the data set slopes the column estimate holds the estimated \\beta_1, the (estimate) standard error is in the column std.error. Figure 7.2 shows the distribution of the t-statistics in our simulated samples.\n\nsim_slopes &lt;- sim_slopes %&gt;%\n  mutate(\n    t_statistic = (estimate - 15) / std.error\n)\n\n\nsim_slopes %&gt;%\n  ggplot(aes(x = t_statistic)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  labs(\n    x = \"Value of the t-statistic\",\n    title = \"Distribution of t-statistic under a true null hypothesis beta_1 = 11.77 (2000 samples)\",\n    y = \"\"\n  ) +\n  geom_density(color = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  geom_vline(xintercept = c(-2), color = \"steelblue\", lty = 2) +\n  geom_vline(xintercept = c(-3), color = \"firebrick\", lty = 2) +\n  xlim(c(-4, 8)) +\n  scale_x_continuous(breaks = c(-3, -2, 0))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\n\n\n\nFigure 7.2: Distribution of the t-statistic when the null hypothesis is correct.\n\n\n\n\n\nWe should see two things in Figure 7.2.\n\nFirst, the t-statistic is different in each sample, even though all samples come from one and the same population (model).\nThe center of the distribution is close to zero. We should have expected this, because we know that the null hypothesis in 7.2 is true.\nIn some samples the t-statistic has large negative values. In one sample it is even less than -3.\n\nFor the hypothesis pair in 7.2, large negative values of the t-statistic are consistent with the alternative. Small values close to zero are consistent with the null hypothesis. However, we can see in Figure 7.2 that large negative values of the t-statistic can happen by pure change chance even under a true null hypothesis. Because such extreme values can occur by chance, any decision rule that rejects the null hypothesis for t-statistics lower than some threshold will inevitably produce wrong decision (rejection of the null hypothesis even if it is true).\nLet us study the two decision rules depicted as vertical dashed lines in Figure 7.2. The first rule (blue) rejects the null hypothesis in samples with a t-statistic less than -2. The second decision rule rejects the null hypothesis for t-statistics less than -3.\nIn the simulation we can simply count the number of samples where we will make a wrong decision to reject the null hypothesis (remember, in this example H_0 is true).\n\nsim_slopes &lt;- sim_slopes %&gt;%\n  mutate(\n    wrong_decision_blue = t_statistic &lt; -2,\n    wrong_decision_red = t_statistic &lt; -3\n  )\n\n## Share of TRUE values (blue)\nsum(sim_slopes$wrong_decision_blue)\n\n[1] 24\n\nmean(sim_slopes$wrong_decision_blue)\n\n[1] 0.024\n\n## Share of TRUE values (red)\nsum(sim_slopes$wrong_decision_red)\n\n[1] 1\n\nmean(sim_slopes$wrong_decision_red)\n\n[1] 0.001\n\n\nThe “less than -2” rule makes a wrong decision in 7 out of 200 samples (3.5 percent). The “less than -3” rule makes a wrong decision in 2 out of 200 samples (one percent).\n\n\n7.5.2 Testing a wrong null hypothesis\nNow that we’ve seen the distribution of the t-statistic under a true null hypothesis, let’s now examine its distribution under a wrong null hypothesis.\nThe following hypothesis is wrong in our simulation (because the real \\beta_1 = 11.77).\n\nH_0: \\beta_1 \\leq 0\\\\\nH_1: \\beta_1 &gt; 0\n\\tag{7.3}\nWe use the same statistic for testing this hypothesis. The only difference now is that the alternative is in the other direction. Therefore we would reject for large positive values of the t-statistic.\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\n\\tag{7.4}\nWe will compute the t-statistic in 7.4 for each sample and will visualize its distribution in Figure 7.3.\n\nsim_slopes &lt;- sim_slopes %&gt;%\n  mutate(\n    t_statistic0 = (estimate - 0) / std.error\n  )\n\n\nsim_slopes %&gt;%\n  ggplot(aes(x = t_statistic0)) +\n  geom_point(\n    aes(y = 0),\n    position = \"jitter\",\n    size = 1 / 2,\n    alpha = 0.5\n  ) +\n  geom_boxplot(alpha = 0.5) +\n  labs(\n    x = \"t-statistic\",\n    y = \"Density\"\n  ) +\n  geom_density(color = \"steelblue\") +\n  geom_vline(xintercept = 0, color = \"red\") +\n  geom_vline(xintercept = c(2), color = \"steelblue\", lty = 2) +\n  geom_vline(xintercept = c(3), color = \"firebrick\", lty = 2) +\n  xlim(c(-4, 8)) +\n  scale_x_continuous(breaks = c(0, 2, 3))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\n\n\n\nFigure 7.3: Distribution of the t-statistic under the (wrong) null hypothesis \beta_1 = 0.\n\n\n\n\n\nNotice that the center of the distribution of the t-statistic is no longer close to zero. This happens because the null hypothesis is wrong in this example.\nLet us consider two decision rules for rejecting H_0. The first one rejects for t-statistics larger than 2, the other one rejects for t-statistics larger than 3.\nWe can count the number of samples where these two rules lead to wrong decisions. A wrong decision in this case is the non-rejection of the null hypothesis.\n\nsim_slopes &lt;- sim_slopes %&gt;%\n  mutate(\n    ## A wrong decision here is to not-reject the null\n    wrong_decision_blue_1 = t_statistic0 &lt; 2,\n    wrong_decision_red_1 = t_statistic0 &lt; 3\n  )\n\n## Number of TRUE values\nsum(sim_slopes$wrong_decision_blue_1)\n\n[1] 0\n\nsum(sim_slopes$wrong_decision_red_1)\n\n[1] 1\n\n## Share of TRUE values\nmean(sim_slopes$wrong_decision_blue_1)\n\n[1] 0\n\nmean(sim_slopes$wrong_decision_red_1)\n\n[1] 0.001\n\n\nThe rejection rule “greater than 2” rejected the null hypothesis in all samples. The rejection rul “greater than 3” failed to reject the null hypothesis in 8 out of 200 samples (four percent).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A Simulation Study</span>"
    ]
  },
  {
    "objectID": "06-Simulation.html#the-t-distribution",
    "href": "06-Simulation.html#the-t-distribution",
    "title": "7  A Simulation Study",
    "section": "7.6 The t-distribution",
    "text": "7.6 The t-distribution\nThe mathematical statistics provides us with a theorem that states that under certain conditions (we will talk more about these assumptions) the t-statistic follows a (central) t-distribution with degrees of freedom equal to n - p where n is the number of observations in the sample and p is the number of model coefficients (betas) in the model.\nThe t-distributions relevant for us have zero mean and the variance determined by the the parameter of the distribution (the degrees of freedom). T-distributions with low degrees of freedom place less probability at the center (zero) and more in the tails of the distribution (extreme values far away from the center) Figure 7.4.\n\ndt &lt;- expand_grid(\n  ## Creates a sequence of 100 numbers between -3 and 3\n  x = seq(-4, 4, length.out = 200),\n  df = c(1, 5, 50, 500)\n) %&gt;%\n  mutate(\n    ## Computes the standard normal density at each of the 100 points in x\n    t_dens = dt(x, df = df),\n    df = factor(df)\n  )\n\nggplot() +\n  ## Draws the normal density line\n  geom_line(data = dt, aes(x = x, y = t_dens, colour = df)) + \n  labs(\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\nFigure 7.4: Densities of t-distributions with different degrees of freedom (df).\n\n\n\n\n\nKnowing the distribution of the test statistic under the null hypothesis (if it is true) allows us to derive critical values (boundaries for the rejection rules) for the tests. These critical values depend on the type of the alternative in the test.\nFor a hypothesis pair of the type considered in Section 7.5.1 we reject for negative values ot the t-statistic.\n\nH_0: \\beta_1 \\geq \\beta_1^{H_0}\\\\\nH_1: \\beta_1 &lt; \\beta_1^{H_0}\n\\tag{7.5}\nWe want to choose the critical value that meets a predetermined probability of wrong rejection of a true null hypothesis.\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\nFigure 7.5: Critical value for a one-sided alternative (left).\n\n\n\n\n\nIf we choose the error probability to be \\alpha, then the critical value for the one-sided alternative in 7.5 is the \\alpha quantile of the t-distribution. We write t_{\\alpha}(n - p) to denote this quantile.\nIn the simulation we had sample sizes n = 434 and two coefficients in the model (\\beta_0, \\beta_1), therefore p = 2. Let \\alpha = 0.05. We can compute the quantile using the qt function.\n\nqt(0.05, df = 434 - 2)\n\n[1] -1.648388\n\n\nThe 0.05 quantile of a t-distribution with 432 degrees of freedom is t_{0.05}(432) = -1.64. Therefore the probability to observe samples with a t-statistic less than -1.64 (under a true) null hypothesis is 0.05.\nLets consider another alternative.\n\nH_0: \\beta_1 \\leq \\beta_{H_0}\\\\\nH_1: \\beta_1 &gt; \\beta_{H_0}\n\nThe t-statistic is the same as in the previous test. This time we reject for t-statistics that are large (and positive), because these are the values expected under the alternative.\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\nFigure 7.6: Critical value for a one-sided alternative (right).\n\n\n\n\n\nThis time the critical value of the 1 - \\alpha quantile of the t-distribution.\nFor df = 432 and \\alpha = 0.05 this quantile is t_{0.95}(432) = 1.64.\n\nqt(1 - 0.05, df = 434 - 2)\n\n[1] 1.648388\n\n\nDue to the symmetry of the t-distribution it is equal to the 0.05 quantile in absolute value.\nFinally, let us consider a two sided alternative.\n\nH_0: \\beta_1 = \\beta_1^{H_0} \\\\\nH_1: \\beta_1 \\neq \\beta_1^{H_0}\n\nIn this test we would reject H_0 both for very large positive values and for large (in absolute value) negative values of the t-statistic. Again, we can obtain the critical values from the t-distribution. Because we reject for both positive and negative values, we choose the critical value so that the total probability of t-statistics greater than the upper and less than the lower critical value is \\alpha.\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\nFigure 7.7: Critical values for a two-sided alternative.\n\n\n\n\n\n\nqt(0.05 / 2, df = 434 - 2)\n\n[1] -1.965471\n\nqt(1 - 0.05 / 2, df = 434 - 2)\n\n[1] 1.965471\n\n\nFor df = 432 and \\alpha = 0.05 the critical values are t_{0.05 / 2}(432) = -1.96 and t_{1 - 0.05 / 2}(432) = 1.96. Due to the symmetry of the t-distribution these critical values are equal in absolute value.\n\n7.6.1 The p-value\nAnother way to decide whether to reject a null hypothesis or not is to compute the p-value of the test. The p-value is conditional the probability (under the null hypothesis) to see samples with a t-statistic that is even more extreme than the one observed in a sample. How the p-value is calculated depends on the alternative in the test.\nLet us take the example with the children from Chapter 6. For convenience we print here again the output from summary.\n\nsummary(lm(kid_score ~ 1 + mom_hs, data = kids))\n\n\nCall:\nlm(formula = kid_score ~ 1 + mom_hs, data = kids)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nNow we consider three null-hypotehsis and alternative pairs.\n\nH_0: \\beta_1 \\leq 0 \\\\\nH_0: \\beta_1 &gt; 0\n The sample value of the t-statistic is\n\n\\frac{11.77 - 0}{2.3} \\approx 5.1\n\nFor this alternative extreme (consistent with the alternative) values of the t-statistic are large positive values. Therefore the p-value of this test is\n\nP(t &gt; \\text{t-statistic}^{obs} | H_0)\n\nThe notation \\text{t-statistic}^{obs} refers to the value of the statistic in the sample (5.1). If H_0 is true, i.e. \\beta_1 = 0 we know that the t-statistic follows a t-distribution with df = 432. We can compute the probability of samples with t-statistics less than 5.1 using the pt function.\n\n1 - pt(5.1, df = 434 - 2)\n\n[1] 2.546875e-07\n\n\nThis probability is equal to about 2.5 out of 10 million. This means that samples with a t-statistic greater than the one observed (5.1) are extremely rare. Either we have been extremely lucky to select a very rare sample or the null hypothesis is wrong.\nLet us now consider the other one-sided alternative\n\nH_0: \\beta_1 \\geq 0 \\\\\nH_1: \\beta_1 &lt; 0\n For this alternative extreme values of the t-statistic are large (in absolute value) negative values. Therefore the p-value of the test is\n\nP(t &lt; \\text{t-statistic}^{obs} | H_0)\n We can compute this probability using pt.\n\npt(5.1, df = 434 -2)\n\n[1] 0.9999997\n\n\nThis p-value is close to one, implying that under the null hypothesis almost all samples would have a t-statistic less than 5.1.\nFinally, let us consider the two-sided alternative\n\nH_0: \\beta_1 = 0 \\\\\nH_1: \\beta_1 \\neq 0\n\nNow both large positive and large (in absolute value) negative values of the t-statistic are extreme (giving evidence against the null hypothesis). The p-value in this case is the sum of two probabilities. For t = 5.1 more extreme values against the null hypothesis are the ones greater than 5.1 and the ones less than -5.1.\n\n\\text{p-value} = P(t &lt; -5.1 | H_0) + P(t &gt; 5.1 | H_0)\n We can compute it with a the pt function. Notice that this is the same value (up to rounding errors) as the one shown in the Pr(&gt;|t|) column in the output of summary.\n\npt(-5.1, df = 434 - 2) + (1 - pt(5.1, df = 434 - 2))\n\n[1] 5.09375e-07\n\n\nHow would the p-value look like if the t-statistic was negative, e.g. -5.1? Again, extreme values are the ones less that -5.1 and greater than the absolute value, i.e. 5.1. We can write this more succinctly (and this explains the notation for the column in the summary output):\n\n\\text{p-value} = P(|t| &gt; |\\text{t-statistic}^{obs}| |H_0)\n\nFinally, let us see the distribution of p-values in the simulation study. Here we will compute it for the true null hypothesis H_0: \\beta_1 = 11.77, H_1: \\beta_1 \\neq 11.77. Figure Figure 7.8 shows the estimated density of the p-values (in each sample). You can see that the density is roughly constant over the interval [0, 1]. Indeed, it can be shown that the p-value if uniformly distributed over this interval if the null hypothesis is true.\n\nsim_slopes &lt;- sim_slopes %&gt;%\n  mutate(\n    p_value_h0_true = 2 * pt(-abs(t_statistic), df = 434 - 2)\n  )\n\n\nsim_slopes %&gt;%\n  ggplot(aes(x = p_value_h0_true)) + \n  geom_density() + \n  labs(\n    x = \"p-value\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\nFigure 7.8: Distribution of p-values under the (true) null hypothesis\n\n\n\n\n\nWhen using the p-value to make a rejection decision we compare it to a predetermined probability of a wrong rejection of a true null hypothesis. This is the same approach that we followed when we derived the critical values of the tests. A widely used convention is to reject a null hypothesis if the p-value is less than 0.05.\nLet us apply this convention in the simulation and see in how many samples we make a wrong decision.\n\nsim_slopes &lt;- sim_slopes %&gt;%\n  mutate(\n    reject_h0_based_on_p_value = p_value_h0_true &lt; 0.05\n  )\n\ntable(sim_slopes$reject_h0_based_on_p_value)\n\n\nFALSE  TRUE \n  956    44",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A Simulation Study</span>"
    ]
  },
  {
    "objectID": "06-Simulation.html#sec-sim-ci",
    "href": "06-Simulation.html#sec-sim-ci",
    "title": "7  A Simulation Study",
    "section": "7.7 Confidence intervals",
    "text": "7.7 Confidence intervals\nFrom the distribution of the t-statistic we can see that\n\n\\text{t-statistic} = \\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1)} \\sim t(n - p)\n\nTherefore the probability to observed a value of the t-statistic in the interval [t_{\\alpha / 2}(n - p), t_{1 - \\alpha / 2}] is 1 - \\alpha.\n\nP\\left(t_{\\alpha / 2}(n - p) &lt; \\frac{\\hat{\\beta}_1 - \\beta_1}{se(\\hat{\\beta}_1)} &lt; t_{1 - \\alpha / 2}(n - p)\\right) = \\\\\nP\\left(\\hat{\\beta_1} + se(\\hat{\\beta}_1) t_{\\alpha/2}(n - p) &lt; \\beta_1 &lt; \\hat{\\beta_1} + se(\\hat{\\beta}_1) t_{1 - \\alpha/2}(n - p) \\right) = 1 - \\alpha\n\nCompute the upper and the lower bounds of the confidence intervals for each sample in the simulation with a coverage probability of 1 - \\alpha = 0.9. In how many samples did the confidence interval contain the real coefficient \\beta_1?\n\nsim_slopes &lt;- sim_slopes %&gt;%\n  mutate(\n    CI_lower = estimate + std.error * qt(0.1 / 2, df = 434 - 2),\n    CI_upper = estimate + std.error * qt(1 - 0.1 / 2, df = 434 - 2),\n    ## Construct a new variables that is TRUE/FALSE if the\n    ## true value of beta_1 (11.77) is inside the confidence interval\n    beta1_in_CI = CI_lower &lt; 11.77 & 11.77 &lt; CI_upper\n  )\n\n\nsum(sim_slopes$beta1_in_CI)\n\n[1] 625\n\nmean(sim_slopes$beta1_in_CI)\n\n[1] 0.625\n\n\nThe CI contained the true value of \\beta_1 in 178 out of 200 samples (89 percent). This is quite close to the coverage probability 1 - \\alpha = 0.9 The CI that we constructed above are visualized in Figure 7.9. For the sake of clarity the plot is limited to the first 50 samples. Confidence intervals that do not contain the true value of \\beta_1 are show in red. Notice that the boundaries differ from sample to sample (as to the coefficient estimates).\n\nsim_slopes %&gt;%\n  ungroup() %&gt;%\n  slice_head(n = 50) %&gt;%\n  ggplot(\n    aes(\n      x = estimate, \n      y = factor(R),\n      xmin = CI_lower,\n      xmax = CI_upper,\n      color = beta1_in_CI\n    )\n  ) + \n  geom_point() + \n  geom_errorbarh() + \n  labs(\n    x = \"\",\n    y = \"Sample\",\n    color = \"Beta1 in CI\"\n  ) + \n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n  ) + \n  geom_vline(\n    xintercept = 15\n  )\n\n\n\n\n\n\n\nFigure 7.9: Confidence intervals for the first 50 samples. The vertical line is drawn at 11.77 (the real value of \\beta_1).\n\n\n\n\n\n\ndt &lt;- tibble(\n  x = 1:10,\n  y = rnorm(10, mean = 2 + 0.3 * x, sd = 0.5)\n)\n\nfit &lt;- lm(y ~ 0 + x, data = dt)\nsum(residuals(fit))\n\n[1] 4.630873",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>A Simulation Study</span>"
    ]
  },
  {
    "objectID": "lecture_problems.html",
    "href": "lecture_problems.html",
    "title": "8  Задача от лекциите",
    "section": "",
    "text": "8.1 Алтернативен начин за пресмятане на \\hat{\\beta}_1 и \\hat{\\beta}_2\nМоже да се покаже, че оценките за \\beta_1 и \\beta_2 са:\n\\begin{align*}\n\\hat{\\beta}_2 & = \\frac{\\overline{x y} - \\bar{x} \\bar{y}}{\\overline{x^2} - \\bar{x}^2} \\\\\n\\hat{\\beta}_1 & = \\bar{y} - \\hat{\\beta}_2 \\bar{x}\n\\end{align*}\n За да приложите тази формула, пресметнете за всяко наблюдение x_i y_i, x_i^2 и \\bar{x}^2:\ndt &lt;- dt %&gt;% \n  mutate(xy = x * y,\n         x2 = x^2)\ndt %&gt;% knitr::kable()\n\n\n\n\nx\ny\nxy\nx2\n\n\n\n\n15\n36\n540\n225\n\n\n6\n25\n150\n36\n\n\n2\n13\n26\n4\n\n\n19\n49\n931\n361\n\n\n8\n17\n136\n64\nСега можем да пресметнем \\overline{x y}, \\overline{x^2} и \\bar{x}^2:\ndt %&gt;%\n  summarise(\n    x_mean = mean(x),\n    y_mean = mean(y),\n    xy_mean = mean(xy),\n    x2_mean = mean(x2),\n    x_mean2 = mean(x)^2\n  )\n\n# A tibble: 1 × 5\n  x_mean y_mean xy_mean x2_mean x_mean2\n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     10     28    357.     138     100\nВ следващата задача се търси R^2. Значението на R^2 се вижда от разлагането на общата вариация на y. Може да се покаже, че важи следната формула:\n\\underbrace{\\sum_{i = 1}^{n} (y_i - \\bar{y})^2}_{TSS} = \\underbrace{\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2}_{RSS} + \\underbrace{\\sum_{i = 1}^{n} (\\hat{y}_i - \\bar{y})^2}_{ESS}\nR^2 = 1 - \\frac{RSS}{TSS}\nTSS = (36 - 28)^2 + (25 - 28)^2 + (13 - 28)^2 + (49 - 28)^2 + (17 - 28)^2 = 860 \\\\\nЗа RSS трябва да изчислим остатъците y_i - \\hat{y}_i. За първото наблюдение остатъкът е 36 - (7.842 + 2.016 \\cdot 15) = -2.082. За второто наблюдение остатъкът е 36 - (7.842 + 2.016 \\cdot 6) = 5.06. Другите остатъци можете да видите в следващата таблица в колонката res.\ndt &lt;- dt %&gt;%\n  mutate(\n    y_hat = 7.842 + 2.016 * x,\n    res = y - y_hat\n  )\ndt\n\n# A tibble: 5 × 6\n      x     y    xy    x2 y_hat   res\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    15    36   540   225  38.1 -2.08\n2     6    25   150    36  19.9  5.06\n3     2    13    26     4  11.9  1.13\n4    19    49   931   361  46.1  2.85\n5     8    17   136    64  24.0 -6.97\nRSS = (-2.08)^2 + 5.06^2 + 1.13^2 + 2.85^2 + (-6.97)^2 = 87.95\nОт тук за R^2 получаваме:\nR^2 = 1 - \\frac{87.95}{860} = 0.897\nrho &lt;- 2.016 / sum((dt$y - mean(dt$y)) ^ 2)\nСледващата задача търси доверителен интервал за дисперсията \\sigma^2 на e_i. От страница 28 в презентация 5 намираме, че\n(n - k) \\frac{\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi^2_{n - k}\nЧи-квадрат разпределението има един параметър, наречен степени на свобода (degrees of freedom). Степените на свобода в линейните регресионни модели са равни на броя наблюдения (n = 5) минус броя на коефициентите в модела (тук k = 2, тъй като има два коефициента: \\beta_1, \\beta_2). me\nЗа доверителния интервал (confidence interval, CI) в задачата е дадена вероятност за покритие от 95 процента. 1 - \\alpha = 0.95 \\implies \\alpha = 0.05. Знаем, че случайна променлива, която следва чи квадрат разпределение с n - k степени на свобода ще се резлизира с верояност 95 процента между \\alpha / 2-рия квантил (\\chi_{n - k}(\\alpha/2)) и (1 - \\alpha / 2) квантил: \\chi_{n - k}(1 - \\alpha / 2). При \\alpha = 0.05 търсим квантилите при 0.05 / 2 = 0.025 и 1 - 0.05 / 2 = 0.975 Тези квантили можем да пресметнем например в R:\nqchisq(0.025, df = 5 - 2)\n\n[1] 0.2157953\n\nqchisq(0.975, df = 5 - 2)\n\n[1] 9.348404\nКогато трябва да използваме таблицата на страница 12 от презентация 4: първо намираме реда, съответстващ на степените на свобода: 3. Tаблицата в презентацията се различава от имплементацията в R и представя квантилите в обратен ред. На 0.975-тия квантил от обясненията тук в таблицата съответства 0.025-тия квантил на на стр. 13 (ред 3): 9.34. На 0.025-тия квантил от обясненията тук в таблицата съответства 0.975-тия квантил (стр. 12, ред 3: 0.215).\nОт дефиницията на квантилите получаваме, че вероятността (n - k) \\frac{\\hat{\\sigma}^2}{\\sigma^2} да се реализира в интервала [0.215; 9.34] е 95 процента.\nP\\left( 0.215 &lt; (n - k) \\frac{\\hat{\\sigma}^2}{\\sigma^2} &lt; 9.34\\right) = 0.95\\\\\nОпростяваме и преобразуваме двете неравенства, така че да получим интервал за \\sigma^2. При умножаването със \\sigma^2 използваме факта, че дисперсията е строго положителна, така че можем да умножим и двете страни на неравенство с нея без да променяме посоката на неравенството. Използваме също така, че в задачата е дадено \\hat{\\sigma} = 5.41.\n0.215 &lt; 3 \\frac{\\hat{\\sigma}^2}{\\sigma^2} \\implies \\\\\n\\frac{0.215}{3}\\sigma^2 &lt; \\hat{\\sigma}^2 \\implies \\\\\n\\sigma^2 &lt; \\frac{3 \\hat{\\sigma}^2}{0.215} = \\frac{3 \\cdot 5.41 ^ 2}{0.215} \\approx 408.4\nот второто неравенство получаваме\n3 \\frac{\\hat{\\sigma}^2}{\\sigma^2} &lt; 9.34 \\implies \\\\\n3 \\frac{\\hat{\\sigma}^2}{9.34} &lt; \\sigma^2 \\implies \\\\\n3 \\frac{5.41^2}{9.34} &lt; \\sigma^2 \\implies \\\\\n9.4 &lt; \\sigma^2\nС това границите на 95 процентовия доверителен интервал за \\sigma^2 са [9.4; 408.4].\nВ последната задача се пита за тест на хипотезата относно един коефициент в модела. В условието не пише нищо за алтернатива, така че ще допуснем, че става въпрос за двустранна алтернатива.\nH_0: \\beta_1 = 3 \\\\\nH_1: \\beta_1 \\neq 3\nЗа този тест използваме следната статистика\nt = \\frac{\\hat{\\beta}_1 - 3}{se(\\hat{\\beta}_1)}\nЗаместваме (в условието имаме se(\\hat{\\beta}_1) = 4.61):\nt = \\frac{7.84 - 3}{4.61} \\approx 1.05\nТази стойност сравняваме с квантилите на t разпределение с n - k = 5 - 2 = 3 степени на свобода (стр. 8 от презентация 4). От нивото на значимост (significance level) в задачата намираме кои квантили трябва да търсим: \\alpha / 2 и (1 - \\alpha / 2). При един процент ниво на значимост (\\alpha = 0.01) трябва да намерим квантилите при 0.01 / 2 = 0.005 и при 1 - 0.01 / 2 = 0.995. Степените на свобода са същите (това не е случайно съвпадение) както и при чи-квадрат разпределението от примера с доверителния интервал: 3. Тъй като t разпределението е симетрично е достатъчно да намерим един от квантилите, другият е със същата абсолютна стойност и с обратен знак.\nТърсения квантил намираме на ред 3 (df = 3) в таблицата на стр. 8 от презентация 4: 5.84\nqt(0.005, df = 5 - 2)\n\n[1] -5.840909\nСледователно критичните стойности на теста са [-5.84; 5.84]. Сравняваме стойността на t статистиката с критичните стойности и виждаме, че тя е по-малка от тях по абсолютна стойност, следователно не можем да отхвърлим нулевата хипотеза при ниво на значимост 0.01.\nВ случай, че се получи стойност на t-статистиката по-голяма от горната критична стойност или по-ниска от долната критична стойност, тогава отхвърляме нулевата хипотеза при ниво на значимост 0.01.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Задача от лекциите</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dalgaard, Peter. 2008. Introductory Statistics with\nR. 2nd edition. Statistics and Computing. New York:\nSpringer.\n\n\nFaraway, Julian James. 2015. Linear Models with R.\nSecond edition. Chapman & Hall/CRC Texts\nin Statistical Science Series. Boca Raton: CRC Press, Taylor &\nFrancis Group.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge\nNew York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge\nUniversity Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to\nRegression with R. Edited by George\nCasella, Stephen Fienberg, and Ingram Olkin. Springer Texts\nin Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo:\nO’Reilly.",
    "crumbs": [
      "References"
    ]
  }
]