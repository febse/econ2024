[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear regression analysis",
    "section": "",
    "text": "General Information",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to linear regression analysis",
    "section": "Schedule",
    "text": "Schedule\n\nTu 10:15-11:45 in room 308 (will move to room 303 after 27.02.2024)\nTu 12:15-13:45 in room 308\nWe 14:15-15:45 in room 308",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Introduction to linear regression analysis",
    "section": "Grading",
    "text": "Grading\nTBA",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#github-repository",
    "href": "index.html#github-repository",
    "title": "Introduction to linear regression analysis",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nAll course materials for the exercise classes will be available in the GitHub repository:\nhttps://github.com/febse/econ2024",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#software-setup",
    "href": "index.html#software-setup",
    "title": "Introduction to linear regression analysis",
    "section": "Software Setup",
    "text": "Software Setup\nThe exercise classes require a minimal software setup:\n\nInstall Git for you operating system from https://git-scm.com/downloads/.\nOpen a GitHub account at https://github.com/signup. While you don’t need an account to download the course materials, you will need one to receive and submit your assignments (this will be explained in details during the exercise classes). You can apply for the GitHub student benefits at https://education.github.com/benefits. If approved you can receive free access to the GitHub Pro plan and to GitHub Copilot, an AI tool that helps you write code.\nOpen https://cran.r-project.org/, and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system, you will see a link “Install R for the first time .” Click on this link and then download the R installer. Run the installer. Leave the default settings unchanged unless you know what you are doing.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system, you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nShould you encounter difficulties installing R and R Studio, you can watch these video guides:\n\n\nWindows\nMac\nUbuntu 22.04\n\n\nHere are some video guides on how to install git:\n\n\nWindows\nMac\nLinux\n\n\nThe following steps depend on git being installed. Open R Studio and open a new project dialog: File -&gt; New Project. In the dialog, click on the third option: version control. From the next menu, select git.\n\n  \nIn the Repository URL field, paste the address of the course repository:\n\n\n\n\n\n\nRepository URL\n\n\n\nInsert the following address in the Repository URL field:\nhttps://github.com/febse/econ2024.git\nthe one shown in the screenshot is outdated.\n\n\n Click on the Create Project button and wait for git R studio to clone the repository and open the project.\n\n\n\n\n\n\nRenv\n\n\n\nTo install the packages necessary for the course, click on the command line in the R console and type:\nrenv::restore()\nThis will trigger the download and installation of all the dependencies. It can take some time, so be patient. You only need to do it once for the project,\n\n\n\n\n\nStep 5\n\n\n 6. The content of the GitHub repository will be updated continuously throughout the semester. In order to download the new files or updated versions of already existing files, you can use git pull. Open the git window in the upper right pane of R studio and click the pull button. This will download all changes from the GitHub repository to your local copy.\n\n\n\nStep 7\n\n\n\nNote that if you have modified the files tracked by git that have changed in the repository, git pull will fail with an error similar to this one:\n\n\n\n\nPull error\n\n\nTo avoid this, you can roll back the file to its original state. Right-click on the file in the git window and choose “revert.”\n\n\n\nRevert\n\n\n\nIn the exercise classes, we will use many functions from the tidyverse system and several other packages. Before accessing these packages’ functionality, you need to install them first. Find the R console in R studio and paste the following line on the command line. Press enter to run it and wait for the installation to complete. The renv package will take care of the installation of the packages in a separate environment, so you should’t worry about installing packages.\n\nIn case it does not work, you can install the packages manually by running the following command in the R console:\ninstall.packages(c(\"tidyverse\", \"broom\", \"patchwork\", \"GGally\", \"caret\"))\nOptional: more on Quarto: https://quarto.org/docs/guide/\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "00-Recommended-Literature.html",
    "href": "00-Recommended-Literature.html",
    "title": "Recommended reading",
    "section": "",
    "text": "The R for data science (Wickham and Grolemund 2016) book covers data management and exploration using the tools from the tidyverse packages.\nDalgaard (2008): Introductory statistics with R. 2nd edition. New York: Springer. This textbook offers an introductory level to probability and basic linear models with a focus on the software (R) and with only a minimal discussion of the mathematical fundamentals. The code shown is base R and mostly differs from what we are using in the classes (tidyverse).\nSheather (2009): A Modern Approach to Regression with R. New York, NY: Springer New York (Springer Texts in Statistics). Available at: https://doi.org/10.1007/978-0-387-09608-7.\nThe Textbook focuses on linear regression models and develops the mathematical fundametals alongside with examples using R.\nGelman, Hill, and Vehtari (2021): Regression and other stories. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press (Analytical methods for social research). Available at: https://doi.org/10.1017/9781139161879.\nAnother introduction into basic probability theory and linear models. The references to Bayesian inference in the book are not relevant to our course.\nFaraway (2015): Linear models with R. Second edition. Boca Raton: CRC Press, Taylor & Francis Group.\nAnother introduction into linear models with examples in R.\n\n\n\n\n\nDalgaard, Peter. 2008. Introductory Statistics with R. 2nd edition. Statistics and Computing. New York: Springer.\n\n\nFaraway, Julian James. 2015. Linear Models with R. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press, Taylor & Francis Group.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with R. Edited by George Casella, Stephen Fienberg, and Ingram Olkin. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. First edition. Sebastopol, CA: O’Reilly.",
    "crumbs": [
      "Recommended reading"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html",
    "href": "01-Introduction-to-R.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Arithmetic Operations\n1 + 4\n\n[1] 5\n\n3 - 2\n\n[1] 1\n\n2 * 8\n\n[1] 16\n\n2 / 8\n\n[1] 0.25\n\n2^4\n\n[1] 16",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#assignment",
    "href": "01-Introduction-to-R.html#assignment",
    "title": "1  Introduction to R",
    "section": "1.2 Assignment",
    "text": "1.2 Assignment\nVery often we want to store a value in the memory of the computer so that we can reuse it later. In R we store values under names (variables) by using the assignment operator `&lt;-` Shortcut for the assignment operator: Alt - (minus)\n\ny &lt;- 34\ny - 40\n\n[1] -6\n\n\nRun this chunks and look at the global environment (right side of R Studio) to see it appear\nin the list of objects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#numeric-vectors",
    "href": "01-Introduction-to-R.html#numeric-vectors",
    "title": "1  Introduction to R",
    "section": "1.3 Numeric Vectors",
    "text": "1.3 Numeric Vectors\nIt is very common to group values that belong together in a single structure. By default numeric vectors in R are created as double precision floating point numbers. You can create a numeric vector using the c (concatenate) function.\n\nx &lt;- c(1, 4)\nx\n\n[1] 1 4\n\n\nTo see the type of a variable, you can use the typeof function.\n\ntypeof(x)\n\n[1] \"double\"\n\n\n\n## Length, average, sum of a numeric vector\nmean(x)\n\n[1] 2.5\n\nsum(x)\n\n[1] 5\n\nlength(x)\n\n[1] 2\n\n\n\n## Documentation\n# ?mean\n\nVectors can only hold data of the same type: either numeric, character, or logical. If you try to create a vector with different types of data, R will coerce the data to the same type. For example, if you try to create a vector with a number and a string, R will coerce the number to a string.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#character-vectors",
    "href": "01-Introduction-to-R.html#character-vectors",
    "title": "1  Introduction to R",
    "section": "1.4 Character Vectors",
    "text": "1.4 Character Vectors\nYou can assign character vectors to variables as well. String literals are enclosed in quotes. It does not matter if you use single or double quotes, but you have to be consistent.\n\nz &lt;- \"Hello, world!\"\n\nThe c function can be used to create character vectors as well.\n\nz1 &lt;- c(\"Hello\", \"world!\")\nz1\n\n[1] \"Hello\"  \"world!\"\n\n\nStrings can be concatenated using the paste function.\n\npaste(z1, collapse = \",\")\n\n[1] \"Hello,world!\"\n\n\n\npaste(z1, \"some string\", sep = \" \")\n\n[1] \"Hello some string\"  \"world! some string\"\n\n\nR is different from other programming languages in that it is vectorized. This means that most functions are designed to work with vectors. For example, the paste function can take a vector of strings as input and return a vector of strings as output.\nUnlike other languages, using length on a variable holding a string will not return the number of characters in the string, but the number of elements in the vector. To count the number of characters in a string, you can use the nchar function.\n\nlength(z)\n\n[1] 1\n\n\n\nnchar(z)\n\n[1] 13\n\n\nAs a lot of other function, nchar is vectorized, meaning that it can take a vector of strings as input and return a vector of integers as output.\n\nnchar(c(\"Hi\", \"world!\", \"Some longer sentence\"))\n\n[1]  2  6 20",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#recycling",
    "href": "01-Introduction-to-R.html#recycling",
    "title": "1  Introduction to R",
    "section": "1.5 Recycling",
    "text": "1.5 Recycling\nIf you try to perform an operation on two vectors of different lengths, R will recycle the shorter vector to match the length of the longer vector. This is called recycling or broadcasting.\nLet’s run an example to see how recycling works. We want to add a scalar to each element of a vector. Mathematically, this does not make sense, because you can only add/subtract element-wise two vectors of the same length. However, R will recycle the scalar to match the length of the vector. Recycling means that it creates a new vector by repeating the shorter vector until it has the same length as the longer vector.\n\nc(2, 5) + 1\n\n[1] 3 6\n\n\n\nc(2, 3, 5, 7) + c(10, 20)\n\n[1] 12 23 15 27\n\nc(2, 3, 5, 7) / c(10, 20)\n\n[1] 0.20 0.15 0.50 0.35\n\n\nTake care when using recycling, because it can lead to unexpected results. For example, if you try to add two vectors of different lengths, R will still recycle the shorter vector to match the length of the longer vector, but it will also issue a warning.\n\nc(1, 2, 10) + c(2, 5)\n\nWarning in c(1, 2, 10) + c(2, 5): longer object length is not a multiple of\nshorter object length\n\n\n[1]  3  7 12\n\n\nPay attention to the warning message. It is telling you that the shorter vector is being recycled to match the length of the longer vector but it cannot expand the shorter vector to match the longer vector exactly. Although this is not an error that will stop your program, most of the time, this is not what you want and is a result from some error before it. You should not rely on recycling vectors of incompatible lengths. Instead, you should be explicit about what you want to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "href": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "title": "1  Introduction to R",
    "section": "1.6 Logical Operators and Logical Values",
    "text": "1.6 Logical Operators and Logical Values\nThere are two logical values: TRUE and FALSE. These emerge from logical operations and indicate whether some condition is fulfilled (TRUE) or not FALSE. You will find similar constructs in all other languages, where this type of data is commonly known as boolean or binary (i.e., only two values).\nThe basic logical operators in R are\n\n## Less than\n2 &lt; 5\n\n[1] TRUE\n\n## Less than or equal\n2 &lt;= 5\n\n[1] TRUE\n\n## Greater than\n2 &gt; 5\n\n[1] FALSE\n\n## Greater or equal\n2 &gt;= 5\n\n[1] FALSE\n\n## Exactly equal\n2 == 5\n\n[1] FALSE\n\n\"Text 2\" == \"Text 2\"\n\n[1] TRUE\n\n\n\nz == \"Text 2\"\n\n[1] FALSE\n\n\n\n\n\n\n\n\nStrict Equality and Floating Point Numbers\n\n\n\nStrict equality generally makes sense for strings and integers, but not for floating point numbers! This is because real numbers cannot be stored exactly in memory and computers work with finite precision. This can lead to unexpected results when comparing floating point numbers. For example, you may get a result like this:\n\nsqrt(2)^2 == 2\n\n[1] FALSE\n\n\nMathematically, \\sqrt(2)^2 is exactl yequal to 2, but the comparison in R returns FALSE.\nWhen printing the number in the console you may not see the difference because the print function formats the number. To compare floating point numbers, you should use the all.equal function, which takes into accounts for the finite precision of floating point numbers.\n\nall.equal(sqrt(2)^2, 2)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#indexing",
    "href": "01-Introduction-to-R.html#indexing",
    "title": "1  Introduction to R",
    "section": "1.7 Indexing",
    "text": "1.7 Indexing\nYou can access elements of a vector using the square brackets. The index of the first element is 1, not 0 (this is different from many other programming languages). You can also use negative indices to exclude elements from the vector.\n\nexpenses &lt;- c(100, 200, 300, 400, 500)\nexpenses[1]\n\n[1] 100\n\n\n\nexpenses[2:4]\n\n[1] 200 300 400\n\n\n\nexpenses[-1]\n\n[1] 200 300 400 500\n\n\nYou can also use logical vectors to index a vector.\n\nexpenses[c(TRUE, FALSE, TRUE, FALSE, TRUE)]\n\n[1] 100 300 500\n\n\nBe careful when using logical vectors to index a vector. If the logical vector is shorter than the vector you are indexing, R will recycle the logical vector to match the length of the vector you are indexing.\n\n(1:10)[c(TRUE, FALSE)]\n\n[1] 1 3 5 7 9",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#factors",
    "href": "01-Introduction-to-R.html#factors",
    "title": "1  Introduction to R",
    "section": "1.8 Factors",
    "text": "1.8 Factors\nFactors are used to represent categorical data (e.g., sex: male/femal, employment status: employed, unemployed, retired, etc.). They are stored as integers and have labels associated with them. Factors are important in statistical modeling and are commonly used in plotting functions. Factors are not strings, they are integers with labels.\n\nsome_vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\")\nsome_factor &lt;- factor(some_vector)\nsome_factor\n\n[1] A B A C B A\nLevels: A B C\n\nlevels(some_factor)\n\n[1] \"A\" \"B\" \"C\"\n\n\nYou can coerce (convert) a factor to an integer vector using the as.integer function.\n\nas.integer(some_factor)\n\n[1] 1 2 1 3 2 1\n\n\nYou can coerce (convert) as factor to a character vector using the as.character function.\n\nas.character(some_factor)\n\n[1] \"A\" \"B\" \"A\" \"C\" \"B\" \"A\"\n\n\nFactors have also some safeguards. If you try to perform an operation that is not defined for factors, R will issue a warning. For example, you cannot meaningfully add a number to a factor. Note that this will only raise a warning, not an error.\n\nsome_factor + 1\n\nWarning in Ops.factor(some_factor, 1): '+' not meaningful for factors\n\n\n[1] NA NA NA NA NA NA\n\n\n\nsome_factor[1] &lt;- \"Some undefined level\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 1, value = \"Some undefined level\"): invalid\nfactor level, NA generated\n\nsome_factor\n\n[1] &lt;NA&gt; B    A    C    B    A   \nLevels: A B C\n\n\nRead more about factors in the R documentation, as well as the section on factors here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#functions",
    "href": "01-Introduction-to-R.html#functions",
    "title": "1  Introduction to R",
    "section": "1.9 Functions",
    "text": "1.9 Functions\nFunctions are a fundamental building block most languages, including R. They are used to carry out a specific task and encapsulate a sequence of steps. You can avoid repeating the same code over and over again by abstracting the code into a function.\nYou can think of a function as a recipe. It takes some ingredients (arguments) and returns a dish (output).\nLet’s look at some examples of functions in R, without going into too much detail.\nTo define a function, you use the function keyword and assing the function to a variable. The function is then called by using the variable name and passing the arguments in parentheses. The function has a body that is enclosed in curly braces.\nLet’s write a function that takes two numbers and returns TRUE or FALSE depending on whether the sum of the arguments is odd. Functions return the value of the last expression in the body of the function unless you explicitly use the return keyword.\n\nis_even_sum &lt;- function(x, y) {\n  (x + y) %% 2 == 0\n}\n\nis_even_sum(2, 3)\n\n[1] FALSE\n\nis_even_sum(2, 4)\n\n[1] TRUE\n\n\nThe %% operator is the modulo operator. It returns the remainder of the division of the first number by the second number.\n\n2 %% 2\n\n[1] 0\n\n3 %% 2\n\n[1] 1\n\n\nWriting functions is a large topic and we will not cover all the details in this course. You can find more information about functions in R in the R documentation.\nA thing to note is that R is (mostly) a functional programming language and functions generally do not modify their arguments. This means that if you pass a variable to a function and the function modifies that variable, the original value will not be changed. This is different from many other languages, where functions can modify their arguments. The reason for this is that arguments are copied when they are passed to a function. This is done to avoid side effects and make the code easier to reason about.\n\nx &lt;- c(1, 2, 3)\n\nf &lt;- function(y) {\n  y[1] &lt;- 100\n  y\n}\n\nf(x)\n\n[1] 100   2   3\n\nx\n\n[1] 1 2 3\n\n\nThe function above assigns a value to the frst element of the vector y and returns the modified vector. However, the original vector x is not modified, because y is a copy of x.\n\ns1 &lt;- c(4, 1, 2, 3)\ns1\n\n[1] 4 1 2 3\n\ns1[1] &lt;- 100\ns1\n\n[1] 100   1   2   3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#data-frames",
    "href": "01-Introduction-to-R.html#data-frames",
    "title": "1  Introduction to R",
    "section": "1.10 Data Frames",
    "text": "1.10 Data Frames\nThe data for our course will most often be a table of values with columns representing measurements of different characteristics (variables) and rows representing different observations. The base data structure to store this kind of data in R is the data frame. In this course we will use a structure called tibble that is provided by the dplyr package, part of the tidyverse collection of packages.\nYou can create a data frame using the tibble function. The tibble function takes named arguments, where the names are the names of the columns and the values are the vectors that will be the columns of the data frame.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndt &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  employed = c(TRUE, FALSE, TRUE)\n)\n\ndt\n\n# A tibble: 3 × 3\n  name      age employed\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;   \n1 Alice      25 TRUE    \n2 Bob        30 FALSE   \n3 Charlie    35 TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html",
    "href": "02-Descriptive-Statistics.html",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "2.1 Data Frames (Tibbles)\nIn most of our work we will use data tables containing variables (columns) that describe characteristics of observations (rows). Most of the time we will use tibble objects to hold the data. tibble objects are a modern rewrite of the data.frame (an older object type for storing data).\nTo use it we need to load the dplyr package, a part of the tidyverse collection of packages.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nIn a limited number of cases we need to construct tables by hand. You can find out more about tibble here.\ndt &lt;- tibble(\n  ## Shorthand syntax for creating a sequence of integers from one to five\n  id = 1:5,\n  y = c(2, 2.5, 3, 8, 12)\n)\ndt\n\n# A tibble: 5 × 2\n     id     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1   2  \n2     2   2.5\n3     3   3  \n4     4   8  \n5     5  12\nMost of our data will come from external sources such as text files in a csv format. For the purpose of this course you don’t need to worry about reading these files, you will always have a starter code chunk that imports the data.\nThe earnings data set contains data on 1816 customers of a shopping mall. The customers have answered a short interview and gave information about their sex, age, ethnicity, annual income, weight and height.\nWe will use this data set to demonstrate some common operations and basic data summaries.\nearnings &lt;- read_csv(\"https://raw.githubusercontent.com/feb-uni-sofia/econometrics2021/main/data/earnings.csv\")\n\nRows: 1816 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): ethnicity\ndbl (14): height, weight, male, earn, earnk, education, mother_education, fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nFirst we will convert the height and weight measurements from their original scales (inch, pound) to cm and kg. We will create two new columns with informative names using the mutate function.\nearnings &lt;- mutate(\n  earnings,\n  height_cm = 2.54 * height,\n  weight_kg = 0.45 * weight\n)\nearnings1 &lt;- select(earnings, height_cm, weight_kg)\nThe same code can be rewritten in a more convenient way using pipes.\nearnings1 &lt;- earnings %&gt;%\n  mutate(\n    height_cm = 2.54 * height,\n    weight_kg = 0.45 * weight\n  ) %&gt;%\n  select(height_cm, weight_kg)\nNote that the object holding the original data is unaffected by mutate and select. The reason for this is that functions in R generally do not change their arguments. If you want to add the two new columns to the original data set earnings, you need to overwrite it with an assignment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "href": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "height (numeric): Height in inches (1 inch = 2.54 cm)\nweight (numeric): Weight in pounds (1 pound \\approx 0.45 kilograms)\nmale (numeric): 1: Male, 0: Female\nearn (numeric): Annual income in USD\nearnk (numeric): Annual income in 1,000 USD\nethnicity (character): Ethnicity\nage (numeric): Age",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#basic-data-summaries",
    "href": "02-Descriptive-Statistics.html#basic-data-summaries",
    "title": "2  Descriptive Statistics",
    "section": "2.2 Basic data summaries",
    "text": "2.2 Basic data summaries\nThe first step in any data analysis is to gain an initial understanding of the context of the data and the distributions of the variables of interest. In this course our main focus will be on two features of the variables: their location and their variability (how different are the observations between each other).\n\n2.2.1 Location\nThe most important measure of location for us will be the empirical mean of a variable (arithmetic average). Let i index the observation in our data set from the first (i = 1) to the last i = n. In our case n = 1816: the number of all interviewed customers. We can represent the values of some (numeric) characteristic (e.g., the persons’ weight) as a vector of values x = (x_1, \\ldots, x_n). In this notation x_1 is the weight of the first customer in the data set (x_1 = 210 pounds). The arithmetic average is defined as the sum of all values divided by the number of observations:\n\n\\bar{x} = \\frac{1}{n}(x_1 + x_2 + \\ldots + x_n) = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\nLet us now compute the arithmetic average of weight and height. One way to access the columns of the data set earnings is to write the name of the data set and then after a $ sign the name of the column.\n\nmean(earnings$height)\n\n[1] 66.56883\n\nmean(earnings$weight, na.rm = TRUE)\n\n[1] 156.3052\n\n\nAnother measure of location is the (empirical) median. You can compute it using the median function.\n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe result is a median height of 66 inches. This means that about half of the customers were taller than 66 inches.\n\n\n2.2.2 Variability (Spread)\nThe next important feature of the data is its variability. It answers the following question: how different are the customers between each other with respect to body height (for example). There are numerous ways to measure variability.\nOne intuitive measure would be the range of the data, defined as the difference by the maximal observed height and the minimal observed height\n\nmin(earnings$height)\n\n[1] 57\n\nmax(earnings$height)\n\n[1] 82\n\nrange(earnings$height)\n\n[1] 57 82\n\n\n\nmax(earnings$height) - min(earnings$height)\n\n[1] 25\n\n\nAnother measure is the inter-quartile range. The quartiles are defined similar to the median. To see this lets use the example of body height. The first quartile of height (25-th percentile and 0.25 quantile are different names for the same thing) is the height for which about one quarter of the customers are shorter than it. You can compute it with the function quantile.\n\nquantile(earnings$height, 0.25)\n\n25% \n 64 \n\n\nAbout 25 percent of our customers were shorter than 64 inches.\nThe second quartile is the same as the median (two quarters).\n\nquantile(earnings$height, 0.5)\n\n50% \n 66 \n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe third quartile is the height for which three quarter of the customers are shorter than it.\n\nquantile(earnings$height, 0.75)\n\n  75% \n69.25 \n\n\nIn our case about three quarter of the customers were shorter than 69.25 inches.\nThe inter-quartile range is simply the difference between the third quartile and the second quartile.\n\nquantile(earnings$height, 0.75) - quantile(earnings$height, 0.25)\n\n 75% \n5.25 \n\n\nAbout half of our customers had a hight between the first quartile (64 inches) and the third quartile (69.25 inches). The inter-quartile range shows you the difference between the height of the talles person and the shortest person for the central 50 percent of the customers.\nAs the range, the inter-quartile range is a measure of variability.\nThe most important measure of variability and the one that will be central to our analysis is the (empirical) variance.\n\nDefinition 2.1 (Empirical Variance) For a vector of values x = (x_1, \\ldots, x_n) it is defined as the average (apart from a small correction in the denominator) squared deviation of the values from their mean.\n\nS^2_x = \\frac{(x_1 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x}^2)}{n - 1} = \\frac{1}{n - 1} \\sum_{i = 1}^{n}(x_i - \\bar{x})^2: \\quad \\text{variance}\\\\\nS_x = \\sqrt{S^2_x} \\quad \\text{standard deviation}\n\n\n\nExample 2.1 (Computing the empirical variance) Lets apply the formula from Definition 2.1 to a very simple example with just three values.\n\nx_1 = -1, x_2 = 0, x_3 = 4\n\nFirst, the empirical mean of these values is\n\n\\bar{x} = \\frac{-1 + 0 + 4}{3} = 1\n\nNow lets substitute these values in the definition of the empirical variance:\n\n\\begin{aligned}\nS^2_{x} & = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 }{n - 1} \\\\\n        & = \\frac{(-1 - 1)^2 + (0 - 1)^2 + (4 - 1)^2 }{3 - 1} \\\\\n        & = \\frac{(-2)^2 + (- 1)^2 + (3 )^2 }{2} \\\\\n        & = \\frac{4 + 1 + 9 }{2} \\\\\n        & = \\frac{14}{2} \\\\\n        & = 7\n\\end{aligned}\n Using R to compute the same thing:\n\nx &lt;- c(-1, 0, 4)\nx_avg &lt;- mean(x)\n\n((-1 - x_avg)^2 + (0 - x_avg)^2 + (4 - x_avg)^2) / (length(x) - 1)\n\n[1] 7\n\n\nThere is also a special function var that can compute it from a vector\n\nvar(x)\n\n[1] 7\n\n\nThe (empirical) standard deviation is simply the square root of the (empirical) variance.\n\nS_x = \\sqrt{S^2_x} = \\sqrt{7} \\approx 2.64\n\nIn R you have two options: take the square root of the result of var using the sqrt function or use sd (standard deviation) to compute the standard deviation directly.\n\nsqrt(var(x))\n\n[1] 2.645751\n\nsd(x)\n\n[1] 2.645751\n\n\n\n\nExercise 2.1 (Empirical Moments (Mean and Variance))  \n\nCompute the sample mean and variance of the annual earnings of the customers in the earnings data set. First, compute it by accessing the columns of the data using the dollar syntax.\n\n\nThen use the summarize function from the dplyr (already loaded) package to compute the mean and the variance of the earn variable.\n\n\nRun the same calculations as before, but this time group the dataset by the ethnicity variable. This will give you the mean and the variance of the annual earnings for every ethnic group in the data set. Use the group_by function.\n\n\n\n\n2.2.3 Overview of the data\nThe skim function from the skimr package provides a quick overview of the data set. It shows the number of observations, the number of variables, the names of the variables, the type of the variables, the number of missing values, the mean, the standard deviation, the minimum and maximum values, and the quartiles for the numeric variables.\n\n## Basic summaries for the whole tibble\nearnings %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nethnicity\n0\n1\n5\n8\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1.00\n66.57\n3.83\n57.00\n64.00\n66.00\n69.25\n82.00\n▂▇▅▁▁\n\n\nweight\n27\n0.99\n156.31\n34.62\n80.00\n130.00\n150.00\n180.00\n342.00\n▅▇▃▁▁\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nearn\n0\n1.00\n21147.30\n22531.77\n0.00\n6000.00\n16000.00\n27000.00\n400000.00\n▇▁▁▁▁\n\n\nearnk\n0\n1.00\n21.15\n22.53\n0.00\n6.00\n16.00\n27.00\n400.00\n▇▁▁▁▁\n\n\neducation\n2\n1.00\n13.24\n2.56\n2.00\n12.00\n12.00\n15.00\n18.00\n▁▁▁▇▃\n\n\nmother_education\n244\n0.87\n13.61\n3.22\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nfather_education\n295\n0.84\n13.65\n3.25\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nwalk\n0\n1.00\n5.30\n2.60\n1.00\n3.00\n6.00\n8.00\n8.00\n▃▁▃▁▇\n\n\nexercise\n0\n1.00\n3.05\n2.32\n1.00\n1.00\n2.00\n5.00\n7.00\n▇▁▁▁▃\n\n\nsmokenow\n1\n1.00\n1.75\n0.44\n1.00\n1.00\n2.00\n2.00\n2.00\n▃▁▁▁▇\n\n\ntense\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nangry\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nage\n0\n1.00\n42.93\n17.16\n18.00\n29.00\n39.00\n56.00\n91.00\n▇▇▃▃▁\n\n\nheight_cm\n0\n1.00\n169.08\n9.73\n144.78\n162.56\n167.64\n175.89\n208.28\n▂▇▅▁▁\n\n\nweight_kg\n27\n0.99\n70.34\n15.58\n36.00\n58.50\n67.50\n81.00\n153.90\n▅▇▃▁▁\n\n\n\n\n\n\n\n2.2.4 Summarizing a single categorical variable\nWhile the mean, median, and variance are useful for numeric variables, they are not defined for categorical variables. Instead, we can use the table function to count the number of observations in each category.\n\ntable(earnings$ethnicity)\n\n\n   Black Hispanic    Other    White \n     180      104       38     1494",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#visualizations",
    "href": "02-Descriptive-Statistics.html#visualizations",
    "title": "2  Descriptive Statistics",
    "section": "2.3 Visualizations",
    "text": "2.3 Visualizations\nHistogram\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nA smooth density plot is an alternative way to visualize the distribution of a variable.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density()\n\n\n\n\n\n\n\n\nThe boxplot shows the median and the 25-th and 75-th percentiles (the box). The whiskers in the plot stretch to the minimum or the maximum observed value, unless there are extreme observations that are shown as single dots.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nGroup comparisons\n\nearnings %&gt;%\n  ggplot(aes(x = height, y = ethnicity)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe scatterplot will be our primary tool in studying associations between variables. It represents each observation as a point in a coordinate system defined by the variables that we would like to study.\n\nearnings1 %&gt;%\n  ggplot(aes(x = weight_kg, y = height_cm)) +\n  geom_point(position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Weight (kg)\",\n    y = \"Height (cm)\"\n  )\n\nWarning: Removed 27 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create a scatterplot\nfig &lt;- plot_ly(earnings1, x = ~weight_kg, y = ~height_cm, mode = 'markers')\nfig\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nWarning: Ignoring 27 observations\n\n\n\n\n\n\n\nsummary(lm(height_cm ~ weight_kg, data = earnings1))\n\n\nCall:\nlm(formula = height_cm ~ weight_kg, data = earnings1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.639  -5.611  -0.084   5.645  36.248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 145.00916    0.89138  162.68   &lt;2e-16 ***\nweight_kg     0.34314    0.01237   27.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.15 on 1787 degrees of freedom\n  (27 observations deleted due to missingness)\nMultiple R-squared:  0.3009,    Adjusted R-squared:  0.3005 \nF-statistic: 769.1 on 1 and 1787 DF,  p-value: &lt; 2.2e-16\n\n\n\nExercise 2.2 (Data Manipulation and Visualization)  \n\nCreate a new variable bmi (body mass index) in the earnings data set. The BMI is defined as the weight in kilograms divided by the square of the height in meters. The height in meters is the height in centimeters divided by 100.\nCreate a scatterplot of the BMI against the annual earnings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "03-Probability-Review.html",
    "href": "03-Probability-Review.html",
    "title": "3  Statistics Review (Optional)",
    "section": "",
    "text": "3.1 Probability\nImagine a game where you flip a single coin once. The possible outcomes are head (H) and tail (T). The set of all possible outcomes of the game is called the sample space of the experiment. We will denote this set with \\Omega. For a single coin toss game, the sample space is \\Omega = \\{\\text{heads}, \\text{tails}\\}.\nSee Bertsekas and Tsitsiklis (2008) (Chapter 1) for a more thorough treatment of the subject.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "03-Probability-Review.html#sec-probability",
    "href": "03-Probability-Review.html#sec-probability",
    "title": "3  Statistics Review (Optional)",
    "section": "",
    "text": "Definition 3.1 (Probability) Let \\Omega denote the sample space of a random experiment. Let A \\subseteq \\Omega and B \\subset \\Omega be two disjoint events (i.e. A \\cap B = \\varnothing). Disjoint events are events that cannot co-occur. A probability measure on this space has the following properties:\n\n\\begin{align}\n  & P(A) \\geq 0 \\qquad \\text{non-negativity}\\\\\n  & P(\\Omega) = 1 \\qquad  \\text{unit measure} \\\\\n  & P(A \\cup B) = P(A) + P(B) \\qquad \\text{additivity}\n\\end{align}\n\n\n\nTheorem 3.1 (Probability of complementary sets) Let \\Omega be a sample space, let A \\subseteq \\Omega be a subset of the sample space, and let \\bar{A} = \\Omega \\setminus A be the complement of A in \\Omega. Then the probability of the complementary set is given by:\n\nP(A) = 1 - P(\\bar{A})\n\n\n\nProof. For the proof note that A and \\bar{A} are disjoint by definition (A \\cap \\bar{A} = \\varnothing). Using the additivity of probability together with the unit probability of the sample space \\Omega from Definition 3.1, it follows that:\n\\begin{align}\n    P(A \\cup \\bar{A}) = P(A) + P(\\bar{A})\n  \\end{align}\n\\begin{align}\n    P(A \\cup \\bar{A}) & =  P(\\Omega) \\\\\n    P(A) + P(\\bar{A}) & = 1 \\implies \\\\\n    P(A) & = 1 - P(\\bar{A}).\n  \\end{align}\n\n\nExample 3.1 (Weather forecast) The weather forecast for the next day shows that it will be raining (A) with probability P(A) = 0.3. The sample space is \\Omega = \\{A, \\bar{A}\\} and the probability of not raining (\\bar{A}) is then P(\\bar{A}) = 1 - 0.3 = 0.7.\n\n\nExample 3.2 (Dice) In a game where you roll a (6-sided) dice once the sample space is \\Omega = \\{1, 2, 3, 4, 5, 6\\}. Denote the outcome of a roll with X and assume that the probability of each outcome is equal: P(X = i) = 1 / 6, i = 1,2,\\ldots,6. The probability of the event X = 1 is then P(X = 1) = 1/6. The probability of the event “outcome is not one” is P(X \\neq 1) = P(X &gt; 1) = 5 / 6.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "03-Probability-Review.html#sec-discrete-distributions",
    "href": "03-Probability-Review.html#sec-discrete-distributions",
    "title": "3  Statistics Review (Optional)",
    "section": "3.2 Discrete distributions",
    "text": "3.2 Discrete distributions\nWhen we presented the axioms of probability we mentioned the concept of a sample space and probability. For the sake of brevity we will skip almost all of the set theoretic foundation of probability measures and will directly introduce the concept of a random variable.\nLet us go back to the roots of probability theory that date back at least to the 17th century and the study of games of chance (gambling) (Freedman, Pisani, and Purves 2007, 248). Almost all introductory text on probability theory start with an example of some simple game of chance. We will follow this example, because it is easy to understand a simple game and the mathematical concepts involved. Later we will see how we can apply the concepts developed here to an extremely broad range of problems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "03-Probability-Review.html#discrete-random-variables",
    "href": "03-Probability-Review.html#discrete-random-variables",
    "title": "3  Statistics Review (Optional)",
    "section": "3.3 Discrete Random Variables",
    "text": "3.3 Discrete Random Variables\nImagine that you are about to buy a lottery ticket that costs 1 EUR. Until you buy and scratch the ticket you don’t actually know how much you will win, but before you commit to the purchase you may wonder what winnings you should expect. Without knowing the rules of the lottery you would be completely in the dark about your prospects, so let us assume that you actually know how the lottery works.\nFrom our point of view the rules of the game are completely determined by two things. The first one is the set of possible outcomes (the sample space) of the lottery and let’s assume that each ticket can win 0 EUR, 1 EUR or 2 EUR. Notice that the set of possible values is finite as there are only three possible outcomes. Let us write X for the (yet unknown) winning from our lottery ticket. In lottery games the value of X depends on some random mechanism (for example drawing numbered balls, spinning a wheel, etc.), therefore it is a function of the outcome of this random mechanism. We will call functions like X random variables. For the most part we will not refer the underlying random mechanism and will simply focus on the distribution of the possible values of X. The second part of the rules is how often winnings of 0 EUR, 1 EUR and 2 EUR occur when you repeatedly play the game. Obviously, a game where half of the tickets win 2 EUR is quite different from a game where only one out of 100 tickets wins 2 EUR.\n\n\n\nTable 3.1: Distribution of outcomes for two games of chance. Possible outcomes and probabilities for each outcome.\n\n\n\n  Winnings(x) P(x) Winnings(y) P(y)\n1       0 EUR  0.5       0 EUR  0.5\n2       1 EUR  0.3       1 EUR  0.1\n3       2 EUR  0.2       2 EUR  0.3\n\n\n\n\nLet us focus on the first game with probabilities (0.5, 0.3 and 0.2) and develop some intuitive understanding of these quantities. You can think about the probabilities in Table 3.1 as theoretical proportions in a sense that if you play the lottery 100 times you would expect to win nothing (0 EUR) in about 50 games, 1 EUR in about 30 games and 2 EUR in about 20 games. Notice that to expect 20 2-EUR wins out of 100 games is absolutely not the same as the statement that you will win 2 EUR in exactly 20 out of 100 games! To convince yourself look at Table 3.2 which presents the results of five simulated games with 100 tickets each. You can play with this and similar games by changing the number of tickets and the number of games in this simulation. In the first game the player had 49 tickets that won nothing, but in the second game she had only 38 0-win tickets. When we say to expect 50 0-wins out of 100 tickets we mean that the number of observed (actually played) 0-wins will vary around 50. In neither of the five simulated games was the number of 0-wins exactly equal to 50 (this is also possible, though).\n\nset.seed(43243)\n\nEx &lt;- sum(x * probs)\nnGames &lt;- 5\ncnts &lt;- rmultinom(n = nGames, size = 100, prob = probs)\navgWinnings &lt;-apply(cnts, 2, function(cnt) sum(cnt * x) / 100)\ngameResult &lt;- data.frame(t(cnts), avgWinnings)\n\ncolnames(gameResult) &lt;- c(paste('x =', 0:(length(probs) - 1), sep = ' '), 'Average winnings per ticket')\nrownames(gameResult) &lt;- paste('Game', 1:nGames, sep = ' ')\n\ngameResult %&gt;%\n  knitr::kable()\n\n\n\nTable 3.2: Simulation of five games with 100 tickets each. Number of tickets by outcome (0, 1, or 2 EUR) and average ticket win.\n\n\n\n\n\n\n\nx = 0\nx = 1\nx = 2\nAverage winnings per ticket\n\n\n\n\nGame 1\n49\n34\n17\n0.68\n\n\nGame 2\n38\n36\n26\n0.88\n\n\nGame 3\n47\n31\n22\n0.75\n\n\nGame 4\n49\n26\n25\n0.76\n\n\nGame 5\n62\n23\n15\n0.53\n\n\n\n\n\n\n\n\nThe probabilities in Table 3.1 completely describe the two games. The functions that assigns a probability to each possible outcome p(x) and p(y) are called probability mass functions. These functions incorporate everything there is to know about our hypothetical games. While this knowledge will not guarantee you a profit from gambling, it enables you to compute the expected value of each ticket, the probability that none of your tickets will win, etc. An important property of the probability mass function is that it is always non-negative (no negative probabilities) and that the sum of the probabilities over all possible values is exactly 1.\n\nDefinition 3.2 (Probability mass function) For a discrete random variable X with possible values x_1,\\ldots,x_K a function p(x_i) that assigns a probability to the possible values of X is called a probability mass function.\n\n\\begin{align}\n  P(X = x_k) = p_k.\n\\end{align}\n The probabilities are real numbers in the interval [0, 1] and need to sum to 1 over all possible values.\n\\begin{align}\n  p_k \\geq 0\\\\\n  \\sum_{k = 1} ^ {K} p_k = 1.\n\\end{align}\n\nNote that the set of K possible values x_1,\\ldots,x_K is finite. The same definition can be used for infinite sets of possible values as long as these are countably infinite but we will skip this discussion.\n\nExample 3.3 (Probability mass function) The first game in Table 3.1 has three possible outcomes: x_1 = 0, x_2 = 1, x_3 = 2. The probability mass function is then\n\np(x) = \\begin{cases}\n0.5 & \\text{if} \\quad x = 0 \\\\\n0.3 & \\text{if} \\quad x = 1 \\\\\n0.2 & \\text{if} \\quad x = 2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\n\nA common visualization of the probability mass function is a barchart with the heights of the bars corresponding to the probability of each outcome.\n\ntibble(\n  x = c(0:2),\n  p = c(0.5, 0.3, 0.2)\n) %&gt;%\n  ggplot(aes(x = x, y = p)) + \n    geom_bar(stat = \"identity\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "03-Probability-Review.html#expected-value",
    "href": "03-Probability-Review.html#expected-value",
    "title": "3  Statistics Review (Optional)",
    "section": "3.4 Expected value",
    "text": "3.4 Expected value\nJust as the descriptive statistics (average, empirical median, empirical quantiles, etc.) are useful for summarizing a set of numbers we would like to be able to summarize distribution functions.\nImagine that you plan to buy 100 tickets from the first game in Table 3.1. Based on the interpretation of probabilities as theoretical proportions you would expect that 50 of the tickets will win nothing, 30 of the tickets will bring you 1 EUR and 20 of the tickets will win 2 EUR. Thus you can write the expected winnings per ticket by summing the contributions of each ticket type:\n\n\\begin{align}\n  \\text{expected winnings} & =\n    \\underbrace{\\frac{50}{100} \\times 0\\text{ EUR}}_{0\\text{ EUR tickets}} +\n    \\underbrace{\\frac{30}{100} \\times 1\\text{ EUR}}_{1\\text{ EUR tickets}} +\n    \\underbrace{\\frac{20}{100} \\times 2\\text{ EUR}}_{2\\text{ EUR tickets}} \\\\\n    & = 0.5 \\times 0 \\text{EUR} + 0.3 \\times 1 \\text{EUR} + 0.2 \\times 2 \\text{EUR} = 0.7\\text{EUR}.\n\\end{align}\n\\tag{3.1}\nNote that the coefficients before the possible outcomes are simply their probabilities. Therefore in a game with 100 tickets you expect that each ticket will bring you 0.7 EUR (on average). Just as with the probabilities, the expected values does not tell you that your average win per ticket will be 0.7 EUR. If you take a look at the five simulated games in Table 3.2 you will notice that the realized average ticket wins are not equal to 0.7 but they vary around it. You can think about the expected value as the center of the distribution (see Table 3.2 and Freedman, Pisani, and Purves (2007), pp. 288). It is important to see that the expected value only depends on the probabilities and the possible values and it does not depend on the outcome of any particular game. Therefore it is a constant and not a random variable itself.\n\n\n\n\n\n\n\n\nFigure 3.1: Probabilities plot. The black vertical line depicts the expected value.\n\n\n\n\n\n\nLet us write the expected value in a more general way:\n\nDefinition 3.3 (Expected Value) For discrete random variable X with possible values x_1, x_2,\\ldots,x_n the weighted average of the possible outcomes:\n\nE(X) = \\sum_{i = 1} ^ {n} x_i p(x_i)\n\nis called the expected value of X. Sometimes we will refer to the expected value as the mean of the distribution or the mean of the random variable following the distribution.\n\nWe introduced the expected value with an example of a game with 100 tickets in order to illustrate it. You should notice from Definition 3.3 that the expected value is independent of the number of games played as it is a property of the probability mass function of the game.\n\nExercise 3.1 (Expected Value) Let Y be a game with possible winnings of -1, 0, and 2 EUR. The probabilities of these outcomes are P(x = -1) = 0.2, P(x = 0) = 0.7, P(x = 2) = 0.1.\n\nPlot the PMF using a bar chart\nCalculate the expected winnings of this game using Definition 3.3\n\nPlay the game using the function sample (check its documentation either by typing ?sample on the command line in R or by searching for it online). It selects size of the values specified in x according to the probabilities given in prob.\n\nsample(\n  x = c(-1, 0, 2), \n  size = 10, \n  prob = c(0.2, 0.7, 0.1),\n  replace = TRUE\n)\n\n [1]  0  0  0 -1  0  0  0  0 -1  0\n\n\nPlay the game a couple of times and look at the outcomes. Compute the average winnings using mean and compare the result with the expected value that you calculated in the previous step.\n\n\n3.4.1 Properties of the expected value\nIn the following we list a few important properties of the expected value that we will use throughout the course. In the following let X and Y be random variables with expected values E(X) and E(Y).\n\nTheorem 3.2 (Linearity of the Expected Value) Let X and Y be two random variables. Then the expected value of their sum equals the sum of their expected values.\n\\begin{align}\n  E(X + Y) = E(X) + E(Y)\n\\end{align}\n\n\nTheorem 3.3 (The Expected Value of a Constant) The expected value of a constant equals the constant itself. Let a be any fixed (not random) real number. Then its expected value is:\n\\begin{align}\n  E(a) = a.\n\\end{align}\n\n\nTheorem 3.4 (Expected value of a scaled random variable) Let X be a random variable and let a be any fixed (not random) real number. Then the expected value of aX is:\n\\begin{align}\n  E(aX) = a E(X)\n\\end{align}\n\n\nProof. \n\\begin{align}\n  E(aX) = \\sum_{i = 1} ^ {n} a x_i p(x_i) = a \\sum_{i = 1} ^ {n} x_i p(x_i) = aE(X).\n\\end{align}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "03-Probability-Review.html#variance",
    "href": "03-Probability-Review.html#variance",
    "title": "3  Statistics Review (Optional)",
    "section": "3.5 Variance",
    "text": "3.5 Variance\nLet us compare the two games in Table 3.1. Both have the same sample space (set of possible outcomes) and the same expected winnings; see Equation 3.1 and ?eq-expected-value-game-2. However, the games are not identical because their probability distributions are different. If given the choice to play only one game, which would you prefer?\nThe second game offers a higher probability of winning the highest prize (2 EUR) at the cost of a lower probability for the middle prize (1 EUR). In other words, it places a higher probability on extreme outcomes (far from the distribution’s center, i.e., the expected value). A summary of a distribution that measures its spread (i.e., how likely are extreme values) is the variance:\n\nDefinition 3.4 (Variance) For a discrete random variable X with possible outcomes x_1, x_2,\\ldots,x_n and probability function p(x) the variance is the expected quadratic deviation from the expected value of the distribution E(X).\n\n\\begin{align}\n  Var(X) & = E(X - E(X)) ^ 2 \\\\\n         & = \\sum_{i = 1}  ^ n \\left(x_i - E(X)\\right) ^ 2 p(x_i)\n\\end{align}\n\n\n\nExample 3.4 (Variance) The variance of the first game (X) is:\n\n\\begin{align}\n    Var(X) & = \\sum_{i = 1} ^ {3} (x_i - E(X)) ^ 2 p(x_i) \\\\\n           & = (x_1 - E(X)) ^ 2 p(x_i) + (x_2 - E(X)) ^ 2 p(x_2) + (x_3 - E(X)) ^ 2 p(x_3) \\\\\n           & = (0 - 0.7) ^ 2 \\times 0.5 + (1 - 0.7) ^ 2 \\times 0.3 + (2 - 0.7) ^ 2 \\times 0.2 \\\\\n           & = 0.61.\n  \\end{align}\n\n\n\nExercise 3.2 Compute the variance of Y, the second game described in Table 3.1.\n\n\nSolution. Var(Y) = 0.76.\n\n\nTheorem 3.5 (Independence of Random Variables) Two discrete random variables X and Y with possible values x_1,\\ldots,x_K and y_1,\\ldots,y_L are independent if for every k and l:\n\n\\begin{align}\n  P(X = x_k, Y = y_l) = P(X = x_k)P(Y = y_l)\n\\end{align}\n\n\n\nTheorem 3.6 For two independent random variables X and Y.\n\\begin{align}\n  Var(X + Y) = Var(X) + Var(Y)\n\\end{align}\nMore generally, if X_i, i = 1,\\ldots,n are independent random variables, the variance of their sum is equal to the sum of their variances:\n\n\\begin{align}\nVar\\left(\\sum_{i = 1}^{n} X_i\\right) = \\sum_{i = 1}^{n} Var(X_i).\n\\end{align}\n\n\n\nTheorem 3.6 actually holds even for only uncorrelated variables. We will discuss the concept correlation later.\n\nTheorem 3.7 (Variance of a Scaled Random Variable) Let a \\in \\mathbf{R} be a fixed (not random) real number and let X be a random variable with variance Var(X).\nThe variance of aX is given by:\n\\begin{align}\nVar(aX) = a^2 Var(X).\n\\end{align}\n\n\nProof. \n\\begin{align}\n  Var(aX) & = \\sum_{k = 1} ^ K (ax_k - E(aX))^2 p(x_k) \\\\\n          & = \\sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\\\\n          & = a^2 \\sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\\\\n          & = a^2 Var(X).\n\\end{align}\n\nAnother useful formula for working with the variance is:\nOften it will be easier to compute the variance using the following decomposition: \\begin{align}\n  Var(X) & = E(X - E(X))^2 \\\\\n         & = E\\left(X^2 - 2XE(X) + E(X) ^ 2\\right) \\\\\n         & = E(X^2) - E(2XE(X)) + E(E(X) ^ 2) \\\\\n         & = E(X^2) - 2 E(X)E(X) + E(X) ^ 2 \\\\\n         & = E(X^2) - E(X)^2.\n\\end{align}\nThe proof above uses the fact that E(X) is a constant and applies Theorem 3.3 and Theorem 3.4.\n\n\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to Probability. 2nd ed. Optimization and Computation Series. Belmont: Athena scientific.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2007. Statistics. 4th ed. New York: W.W. Norton & Co.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html",
    "href": "04-Least-Squares.html",
    "title": "4  Least Squares",
    "section": "",
    "text": "4.1 Least Squares\nThe previous discussion leads us to a question. Can we find values for the coefficients in the linear equation that minimize the MSE? The answer is yes. The method is called least squares.\nFirst, let us visualize the MSE as a function of the coefficients. For this purpose, we will simply calculate the MSE for a grid of values of the coefficients and plot the results (Figure 4.3)\n# NOTE: the code here is only for illustration purposes, you don't need to understand it\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create a grid of values for beta0_hat and beta1_hat\n\nbeta0_hat &lt;- seq(0.4, 1, length.out = 50)\nbeta1_hat &lt;- seq(0.001, 0.015, length.out = 50)\n\ndt &lt;- expand.grid(beta0_hat = beta0_hat, beta1_hat = beta1_hat) %&gt;%\n  mutate(\n    # Compute the RSS for each combination of beta0_hat and beta1_hat\n    RSS = map2_dbl(beta0_hat, beta1_hat, ~{\n      Time_hat &lt;- .x + .y * invoices$Invoices\n      sum((invoices$Time - Time_hat)^2)\n    })\n  )\n\nfig &lt;- plot_ly(\n  x=beta0_hat, \n  y=beta1_hat, \n  z = matrix(dt$RSS, nrow = length(beta0_hat), ncol = length(beta1_hat)),\n  ) %&gt;% \n    add_surface() %&gt;% \n    layout(\n        scene = list(\n          xaxis = list(title = \"beta1_hat\"),\n          yaxis = list(title = \"beta1_hat\"),\n          zaxis = list(title = \"RSS\")\n        )\n    )\n\nfig\n\n\n\n\n\n\n\nFigure 4.3: MSE as a function of the coefficients\nOur goal is to find the values of \\beta_0 and \\beta_1 that minimize the MSE. The method of least squares provides a formula for the coefficients that minimize the MSE.\nFirst, let us solve a simpler problem. Assume that the predictive equation is\n\\widehat{\\text{Time}}_i = \\hat{beta}_0\nThe MSE in this case is much simpler.\ninvoices_beta_0_dt &lt;- expand_grid(\n    beta0_hat = seq(0, 4, length.out = 100),\n    invoices\n)\n\nrss_dt &lt;- invoices_beta_0_dt %&gt;%\n  group_by(beta0_hat) %&gt;%\n  summarise(\n    RSS = sum((Time - beta0_hat)^2)\n  )\n\nrss_dt %&gt;%\n    ggplot(aes(x = beta0_hat, y = RSS)) +\n    geom_line()\n\n\n\n\n\n\n\nFigure 4.4: MSE as a function of beta0_hat in an intercept-only equation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to\nProbability. 2nd ed. Optimization and Computation Series.\nBelmont: Athena scientific.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with\nR. 2nd edition. Statistics and Computing. New\nYork: Springer.\n\n\nFaraway, Julian James. 2015. Linear Models with R.\nSecond edition. Chapman & Hall/CRC Texts\nin Statistical Science Series. Boca Raton: CRC Press,\nTaylor & Francis Group.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2007.\nStatistics. 4th ed. New York: W.W. Norton\n& Co.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research.\nCambridge New York, NY Port Melbourne, VIC New Delhi\nSingapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to\nRegression with R. Edited by George\nCasella, Stephen Fienberg, and Ingram Olkin. Springer Texts\nin Statistics. New York, NY: Springer\nNew York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, and Garrett Grolemund. 2016. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. First edition.\nSebastopol, CA: O’Reilly.",
    "crumbs": [
      "References"
    ]
  }
]