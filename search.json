[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear regression analysis",
    "section": "",
    "text": "General Information",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to linear regression analysis",
    "section": "Schedule",
    "text": "Schedule\n\nTu 10:15-11:45 in room 308 (will move to room 303 after 27.02.2024)\nTu 12:15-13:45 in room 308\nWe 14:15-15:45 in room 308",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Introduction to linear regression analysis",
    "section": "Grading",
    "text": "Grading\nTBA",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#github-repository",
    "href": "index.html#github-repository",
    "title": "Introduction to linear regression analysis",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nAll course materials for the exercise classes will be available in the GitHub repository:\nhttps://github.com/febse/econ2024",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#software-setup",
    "href": "index.html#software-setup",
    "title": "Introduction to linear regression analysis",
    "section": "Software Setup",
    "text": "Software Setup\nThe exercise classes require a minimal software setup:\n\nInstall Git for you operating system from https://git-scm.com/downloads/.\nOpen a GitHub account at https://github.com/signup. While you don’t need an account to download the course materials, you will need one to receive and submit your assignments (this will be explained in details during the exercise classes). You can apply for the GitHub student benefits at https://education.github.com/benefits. If approved you can receive free access to the GitHub Pro plan and to GitHub Copilot, an AI tool that helps you write code.\nOpen https://cran.r-project.org/, and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system, you will see a link “Install R for the first time .” Click on this link and then download the R installer. Run the installer. Leave the default settings unchanged unless you know what you are doing.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system, you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nShould you encounter difficulties installing R and R Studio, you can watch these video guides:\n\n\nWindows\nMac\nUbuntu 22.04\n\n\nHere are some video guides on how to install git:\n\n\nWindows\nMac\nLinux\n\n\nThe following steps depend on git being installed. Open R Studio and open a new project dialog: File -&gt; New Project. In the dialog, click on the third option: version control. From the next menu, select git.\n\n  \nIn the Repository URL field, paste the address of the course repository:\n\n\n\n\n\n\nRepository URL\n\n\n\nInsert the following address in the Repository URL field:\nhttps://github.com/febse/econ2024.git\nthe one shown in the screenshot is outdated.\n\n\n Click on the Create Project button and wait for git R studio to clone the repository and open the project.\n\n\n\n\n\n\nRenv\n\n\n\nTo install the packages necessary for the course, click on the command line in the R console and type:\nrenv::restore()\nThis will trigger the download and installation of all the dependencies. It can take some time, so be patient. You only need to do it once for the project,\n\n\n\n\n\nStep 5\n\n\n 6. The content of the GitHub repository will be updated continuously throughout the semester. In order to download the new files or updated versions of already existing files, you can use git pull. Open the git window in the upper right pane of R studio and click the pull button. This will download all changes from the GitHub repository to your local copy.\n\n\n\nStep 7\n\n\n\nNote that if you have modified the files tracked by git that have changed in the repository, git pull will fail with an error similar to this one:\n\n\n\n\nPull error\n\n\nTo avoid this, you can roll back the file to its original state. Right-click on the file in the git window and choose “revert.”\n\n\n\nRevert\n\n\n\nIn the exercise classes, we will use many functions from the tidyverse system and several other packages. Before accessing these packages’ functionality, you need to install them first. Find the R console in R studio and paste the following line on the command line. Press enter to run it and wait for the installation to complete. The renv package will take care of the installation of the packages in a separate environment, so you should’t worry about installing packages.\n\nIn case it does not work, you can install the packages manually by running the following command in the R console:\ninstall.packages(c(\"tidyverse\", \"broom\", \"patchwork\", \"GGally\", \"caret\", \"plotly\"))\nOptional: more on Quarto: https://quarto.org/docs/guide/\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "00-Recommended-Literature.html",
    "href": "00-Recommended-Literature.html",
    "title": "Recommended reading",
    "section": "",
    "text": "The R for data science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) book covers data management and exploration using the tools from the tidyverse packages.\nDalgaard (2008): Introductory statistics with R. 2nd edition. New York: Springer. This textbook offers an introductory level to probability and basic linear models with a focus on the software (R) and with only a minimal discussion of the mathematical fundamentals. The code shown is base R and mostly differs from what we are using in the classes (tidyverse).\nSheather (2009): A Modern Approach to Regression with R. New York, NY: Springer New York (Springer Texts in Statistics). Available at: https://doi.org/10.1007/978-0-387-09608-7.\nThe Textbook focuses on linear regression models and develops the mathematical fundamentals alongside with examples using R.\nGelman, Hill, and Vehtari (2021): Regression and other stories. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press (Analytical methods for social research). Available at: https://doi.org/10.1017/9781139161879.\nAnother introduction into basic probability theory and linear models. The references to Bayesian inference in the book are not relevant to our course.\nFaraway (2015): Linear models with R. Second edition. Boca Raton: CRC Press, Taylor & Francis Group.\nAnother introduction into linear models with examples in R.\n\n\n\n\n\nDalgaard, Peter. 2008. Introductory Statistics with R. 2nd edition. Statistics and Computing. New York: Springer.\n\n\nFaraway, Julian James. 2015. Linear Models with R. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press, Taylor & Francis Group.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with R. Edited by George Casella, Stephen Fienberg, and Ingram Olkin. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly.",
    "crumbs": [
      "Recommended reading"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html",
    "href": "01-Introduction-to-R.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Arithmetic Operations\n1 + 4\n\n[1] 5\n\n3 - 2\n\n[1] 1\n\n2 * 8\n\n[1] 16\n\n2 / 8\n\n[1] 0.25\n\n2^4\n\n[1] 16",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#assignment",
    "href": "01-Introduction-to-R.html#assignment",
    "title": "1  Introduction to R",
    "section": "1.2 Assignment",
    "text": "1.2 Assignment\nVery often we want to store a value in the memory of the computer so that we can reuse it later. In R we store values under names (variables) by using the assignment operator `&lt;-` Shortcut for the assignment operator: Alt - (minus)\n\ny &lt;- 34\ny - 40\n\n[1] -6\n\n\nRun this chunks and look at the global environment (right side of R Studio) to see it appear\nin the list of objects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#numeric-vectors",
    "href": "01-Introduction-to-R.html#numeric-vectors",
    "title": "1  Introduction to R",
    "section": "1.3 Numeric Vectors",
    "text": "1.3 Numeric Vectors\nIt is very common to group values that belong together in a single structure. By default numeric vectors in R are created as double precision floating point numbers. You can create a numeric vector using the c (concatenate) function.\n\nx &lt;- c(1, 4)\nx\n\n[1] 1 4\n\n\nTo see the type of a variable, you can use the typeof function.\n\ntypeof(x)\n\n[1] \"double\"\n\n\n\n## Length, average, sum of a numeric vector\nmean(x)\n\n[1] 2.5\n\nsum(x)\n\n[1] 5\n\nlength(x)\n\n[1] 2\n\n\n\n## Documentation\n# ?mean\n\nVectors can only hold data of the same type: either numeric, character, or logical. If you try to create a vector with different types of data, R will coerce the data to the same type. For example, if you try to create a vector with a number and a string, R will coerce the number to a string.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#character-vectors",
    "href": "01-Introduction-to-R.html#character-vectors",
    "title": "1  Introduction to R",
    "section": "1.4 Character Vectors",
    "text": "1.4 Character Vectors\nYou can assign character vectors to variables as well. String literals are enclosed in quotes. It does not matter if you use single or double quotes, but you have to be consistent.\n\nz &lt;- \"Hello, world!\"\n\nThe c function can be used to create character vectors as well.\n\nz1 &lt;- c(\"Hello\", \"world!\")\nz1\n\n[1] \"Hello\"  \"world!\"\n\n\nStrings can be concatenated using the paste function.\n\npaste(z1, collapse = \",\")\n\n[1] \"Hello,world!\"\n\n\n\npaste(z1, \"some string\", sep = \" \")\n\n[1] \"Hello some string\"  \"world! some string\"\n\n\nR is different from other programming languages in that it is vectorized. This means that most functions are designed to work with vectors. For example, the paste function can take a vector of strings as input and return a vector of strings as output.\nUnlike other languages, using length on a variable holding a string will not return the number of characters in the string, but the number of elements in the vector. To count the number of characters in a string, you can use the nchar function.\n\nlength(z)\n\n[1] 1\n\n\n\nnchar(z)\n\n[1] 13\n\n\nAs a lot of other function, nchar is vectorized, meaning that it can take a vector of strings as input and return a vector of integers as output.\n\nnchar(c(\"Hi\", \"world!\", \"Some longer sentence\"))\n\n[1]  2  6 20",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#recycling",
    "href": "01-Introduction-to-R.html#recycling",
    "title": "1  Introduction to R",
    "section": "1.5 Recycling",
    "text": "1.5 Recycling\nIf you try to perform an operation on two vectors of different lengths, R will recycle the shorter vector to match the length of the longer vector. This is called recycling or broadcasting.\nLet’s run an example to see how recycling works. We want to add a scalar to each element of a vector. Mathematically, this does not make sense, because you can only add/subtract element-wise two vectors of the same length. However, R will recycle the scalar to match the length of the vector. Recycling means that it creates a new vector by repeating the shorter vector until it has the same length as the longer vector.\n\nc(2, 5) + 1\n\n[1] 3 6\n\n\n\nc(2, 3, 5, 7) + c(10, 20)\n\n[1] 12 23 15 27\n\nc(2, 3, 5, 7) / c(10, 20)\n\n[1] 0.20 0.15 0.50 0.35\n\n\nTake care when using recycling, because it can lead to unexpected results. For example, if you try to add two vectors of different lengths, R will still recycle the shorter vector to match the length of the longer vector, but it will also issue a warning.\n\nc(1, 2, 10) + c(2, 5)\n\nWarning in c(1, 2, 10) + c(2, 5): longer object length is not a multiple of\nshorter object length\n\n\n[1]  3  7 12\n\n\nPay attention to the warning message. It is telling you that the shorter vector is being recycled to match the length of the longer vector but it cannot expand the shorter vector to match the longer vector exactly. Although this is not an error that will stop your program, most of the time, this is not what you want and is a result from some error before it. You should not rely on recycling vectors of incompatible lengths. Instead, you should be explicit about what you want to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "href": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "title": "1  Introduction to R",
    "section": "1.6 Logical Operators and Logical Values",
    "text": "1.6 Logical Operators and Logical Values\nThere are two logical values: TRUE and FALSE. These emerge from logical operations and indicate whether some condition is fulfilled (TRUE) or not FALSE. You will find similar constructs in all other languages, where this type of data is commonly known as boolean or binary (i.e., only two values).\nThe basic logical operators in R are\n\n## Less than\n2 &lt; 5\n\n[1] TRUE\n\n## Less than or equal\n2 &lt;= 5\n\n[1] TRUE\n\n## Greater than\n2 &gt; 5\n\n[1] FALSE\n\n## Greater or equal\n2 &gt;= 5\n\n[1] FALSE\n\n## Exactly equal\n2 == 5\n\n[1] FALSE\n\n\"Text 2\" == \"Text 2\"\n\n[1] TRUE\n\n\n\nz == \"Text 2\"\n\n[1] FALSE\n\n\n\n\n\n\n\n\nStrict Equality and Floating Point Numbers\n\n\n\nStrict equality generally makes sense for strings and integers, but not for floating point numbers! This is because real numbers cannot be stored exactly in memory and computers work with finite precision. This can lead to unexpected results when comparing floating point numbers. For example, you may get a result like this:\n\nsqrt(2)^2 == 2\n\n[1] FALSE\n\n\nMathematically, \\sqrt(2)^2 is exactl yequal to 2, but the comparison in R returns FALSE.\nWhen printing the number in the console you may not see the difference because the print function formats the number. To compare floating point numbers, you should use the all.equal function, which takes into accounts for the finite precision of floating point numbers.\n\nall.equal(sqrt(2)^2, 2)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#indexing",
    "href": "01-Introduction-to-R.html#indexing",
    "title": "1  Introduction to R",
    "section": "1.7 Indexing",
    "text": "1.7 Indexing\nYou can access elements of a vector using the square brackets. The index of the first element is 1, not 0 (this is different from many other programming languages). You can also use negative indices to exclude elements from the vector.\n\nexpenses &lt;- c(100, 200, 300, 400, 500)\nexpenses[1]\n\n[1] 100\n\n\n\nexpenses[2:4]\n\n[1] 200 300 400\n\n\n\nexpenses[-1]\n\n[1] 200 300 400 500\n\n\nYou can also use logical vectors to index a vector.\n\nexpenses[c(TRUE, FALSE, TRUE, FALSE, TRUE)]\n\n[1] 100 300 500\n\n\nBe careful when using logical vectors to index a vector. If the logical vector is shorter than the vector you are indexing, R will recycle the logical vector to match the length of the vector you are indexing.\n\n(1:10)[c(TRUE, FALSE)]\n\n[1] 1 3 5 7 9",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#factors",
    "href": "01-Introduction-to-R.html#factors",
    "title": "1  Introduction to R",
    "section": "1.8 Factors",
    "text": "1.8 Factors\nFactors are used to represent categorical data (e.g., sex: male/femal, employment status: employed, unemployed, retired, etc.). They are stored as integers and have labels associated with them. Factors are important in statistical modeling and are commonly used in plotting functions. Factors are not strings, they are integers with labels.\n\nsome_vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\")\nsome_factor &lt;- factor(some_vector)\nsome_factor\n\n[1] A B A C B A\nLevels: A B C\n\nlevels(some_factor)\n\n[1] \"A\" \"B\" \"C\"\n\n\nYou can coerce (convert) a factor to an integer vector using the as.integer function.\n\nas.integer(some_factor)\n\n[1] 1 2 1 3 2 1\n\n\nYou can coerce (convert) as factor to a character vector using the as.character function.\n\nas.character(some_factor)\n\n[1] \"A\" \"B\" \"A\" \"C\" \"B\" \"A\"\n\n\nFactors have also some safeguards. If you try to perform an operation that is not defined for factors, R will issue a warning. For example, you cannot meaningfully add a number to a factor. Note that this will only raise a warning, not an error.\n\nsome_factor + 1\n\nWarning in Ops.factor(some_factor, 1): '+' not meaningful for factors\n\n\n[1] NA NA NA NA NA NA\n\n\n\nsome_factor[1] &lt;- \"Some undefined level\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 1, value = \"Some undefined level\"): invalid\nfactor level, NA generated\n\nsome_factor\n\n[1] &lt;NA&gt; B    A    C    B    A   \nLevels: A B C\n\n\nRead more about factors in the R documentation, as well as the section on factors here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#functions",
    "href": "01-Introduction-to-R.html#functions",
    "title": "1  Introduction to R",
    "section": "1.9 Functions",
    "text": "1.9 Functions\nFunctions are a fundamental building block most languages, including R. They are used to carry out a specific task and encapsulate a sequence of steps. You can avoid repeating the same code over and over again by abstracting the code into a function.\nYou can think of a function as a recipe. It takes some ingredients (arguments) and returns a dish (output).\nLet’s look at some examples of functions in R, without going into too much detail.\nTo define a function, you use the function keyword and assing the function to a variable. The function is then called by using the variable name and passing the arguments in parentheses. The function has a body that is enclosed in curly braces.\nLet’s write a function that takes two numbers and returns TRUE or FALSE depending on whether the sum of the arguments is odd. Functions return the value of the last expression in the body of the function unless you explicitly use the return keyword.\n\nis_even_sum &lt;- function(x, y) {\n  (x + y) %% 2 == 0\n}\n\nis_even_sum(2, 3)\n\n[1] FALSE\n\nis_even_sum(2, 4)\n\n[1] TRUE\n\n\nThe %% operator is the modulo operator. It returns the remainder of the division of the first number by the second number.\n\n2 %% 2\n\n[1] 0\n\n3 %% 2\n\n[1] 1\n\n\nWriting functions is a large topic and we will not cover all the details in this course. You can find more information about functions in R in the R documentation.\nA thing to note is that R is (mostly) a functional programming language and functions generally do not modify their arguments. This means that if you pass a variable to a function and the function modifies that variable, the original value will not be changed. This is different from many other languages, where functions can modify their arguments. The reason for this is that arguments are copied when they are passed to a function. This is done to avoid side effects and make the code easier to reason about.\n\nx &lt;- c(1, 2, 3)\n\nf &lt;- function(y) {\n  y[1] &lt;- 100\n  y\n}\n\nf(x)\n\n[1] 100   2   3\n\nx\n\n[1] 1 2 3\n\n\nThe function above assigns a value to the frst element of the vector y and returns the modified vector. However, the original vector x is not modified, because y is a copy of x.\n\ns1 &lt;- c(4, 1, 2, 3)\ns1\n\n[1] 4 1 2 3\n\ns1[1] &lt;- 100\ns1\n\n[1] 100   1   2   3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#data-frames",
    "href": "01-Introduction-to-R.html#data-frames",
    "title": "1  Introduction to R",
    "section": "1.10 Data Frames",
    "text": "1.10 Data Frames\nThe data for our course will most often be a table of values with columns representing measurements of different characteristics (variables) and rows representing different observations. The base data structure to store this kind of data in R is the data frame. In this course we will use a structure called tibble that is provided by the dplyr package, part of the tidyverse collection of packages.\nYou can create a data frame using the tibble function. The tibble function takes named arguments, where the names are the names of the columns and the values are the vectors that will be the columns of the data frame.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndt &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  employed = c(TRUE, FALSE, TRUE)\n)\n\ndt\n\n# A tibble: 3 × 3\n  name      age employed\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;   \n1 Alice      25 TRUE    \n2 Bob        30 FALSE   \n3 Charlie    35 TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html",
    "href": "02-Descriptive-Statistics.html",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "2.1 Data Frames (Tibbles)\nIn most of our work we will use data tables containing variables (columns) that describe characteristics of observations (rows). Most of the time we will use tibble objects to hold the data. tibble objects are a modern rewrite of the data.frame (an older object type for storing data).\nTo use it we need to load the dplyr package, a part of the tidyverse collection of packages.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nIn a limited number of cases we need to construct tables by hand. You can find out more about tibble here.\ndt &lt;- tibble(\n  ## Shorthand syntax for creating a sequence of integers from one to five\n  id = 1:5,\n  y = c(2, 2.5, 3, 8, 12)\n)\ndt\n\n# A tibble: 5 × 2\n     id     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1   2  \n2     2   2.5\n3     3   3  \n4     4   8  \n5     5  12\nMost of our data will come from external sources such as text files in a csv format. For the purpose of this course you don’t need to worry about reading these files, you will always have a starter code chunk that imports the data.\nThe earnings data set contains data on 1816 customers of a shopping mall. The customers have answered a short interview and gave information about their sex, age, ethnicity, annual income, weight and height.\nWe will use this data set to demonstrate some common operations and basic data summaries.\nearnings &lt;- read_csv(\"https://raw.githubusercontent.com/feb-uni-sofia/econometrics2021/main/data/earnings.csv\")\n\nRows: 1816 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): ethnicity\ndbl (14): height, weight, male, earn, earnk, education, mother_education, fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nFirst we will convert the height and weight measurements from their original scales (inch, pound) to cm and kg. We will create two new columns with informative names using the mutate function.\nearnings &lt;- mutate(\n  earnings,\n  height_cm = 2.54 * height,\n  weight_kg = 0.45 * weight\n)\nearnings1 &lt;- select(earnings, height_cm, weight_kg)\nThe same code can be rewritten in a more convenient way using pipes.\nearnings1 &lt;- earnings %&gt;%\n  mutate(\n    height_cm = 2.54 * height,\n    weight_kg = 0.45 * weight\n  ) %&gt;%\n  select(height_cm, weight_kg)\nNote that the object holding the original data is unaffected by mutate and select. The reason for this is that functions in R generally do not change their arguments. If you want to add the two new columns to the original data set earnings, you need to overwrite it with an assignment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "href": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "height (numeric): Height in inches (1 inch = 2.54 cm)\nweight (numeric): Weight in pounds (1 pound \\approx 0.45 kilograms)\nmale (numeric): 1: Male, 0: Female\nearn (numeric): Annual income in USD\nearnk (numeric): Annual income in 1,000 USD\nethnicity (character): Ethnicity\nage (numeric): Age",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#basic-data-summaries",
    "href": "02-Descriptive-Statistics.html#basic-data-summaries",
    "title": "2  Descriptive Statistics",
    "section": "2.2 Basic data summaries",
    "text": "2.2 Basic data summaries\nThe first step in any data analysis is to gain an initial understanding of the context of the data and the distributions of the variables of interest. In this course our main focus will be on two features of the variables: their location and their variability (how different are the observations between each other).\n\n2.2.1 Location\nThe most important measure of location for us will be the empirical mean of a variable (arithmetic average). Let i index the observation in our data set from the first (i = 1) to the last i = n. In our case n = 1816: the number of all interviewed customers. We can represent the values of some (numeric) characteristic (e.g., the persons’ weight) as a vector of values x = (x_1, \\ldots, x_n). In this notation x_1 is the weight of the first customer in the data set (x_1 = 210 pounds). The arithmetic average is defined as the sum of all values divided by the number of observations:\n\n\\bar{x} = \\frac{1}{n}(x_1 + x_2 + \\ldots + x_n) = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\nLet us now compute the arithmetic average of weight and height. One way to access the columns of the data set earnings is to write the name of the data set and then after a $ sign the name of the column.\n\nmean(earnings$height)\n\n[1] 66.56883\n\nmean(earnings$weight, na.rm = TRUE)\n\n[1] 156.3052\n\n\nAnother measure of location is the (empirical) median. You can compute it using the median function.\n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe result is a median height of 66 inches. This means that about half of the customers were taller than 66 inches.\n\n\n2.2.2 Variability (Spread)\nThe next important feature of the data is its variability. It answers the following question: how different are the customers between each other with respect to body height (for example). There are numerous ways to measure variability.\nOne intuitive measure would be the range of the data, defined as the difference by the maximal observed height and the minimal observed height\n\nmin(earnings$height)\n\n[1] 57\n\nmax(earnings$height)\n\n[1] 82\n\nrange(earnings$height)\n\n[1] 57 82\n\n\n\nmax(earnings$height) - min(earnings$height)\n\n[1] 25\n\n\nAnother measure is the inter-quartile range. The quartiles are defined similar to the median. To see this lets use the example of body height. The first quartile of height (25-th percentile and 0.25 quantile are different names for the same thing) is the height for which about one quarter of the customers are shorter than it. You can compute it with the function quantile.\n\nquantile(earnings$height, 0.25)\n\n25% \n 64 \n\n\nAbout 25 percent of our customers were shorter than 64 inches.\nThe second quartile is the same as the median (two quarters).\n\nquantile(earnings$height, 0.5)\n\n50% \n 66 \n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe third quartile is the height for which three quarter of the customers are shorter than it.\n\nquantile(earnings$height, 0.75)\n\n  75% \n69.25 \n\n\nIn our case about three quarter of the customers were shorter than 69.25 inches.\nThe inter-quartile range is simply the difference between the third quartile and the second quartile.\n\nquantile(earnings$height, 0.75) - quantile(earnings$height, 0.25)\n\n 75% \n5.25 \n\n\nAbout half of our customers had a hight between the first quartile (64 inches) and the third quartile (69.25 inches). The inter-quartile range shows you the difference between the height of the talles person and the shortest person for the central 50 percent of the customers.\nAs the range, the inter-quartile range is a measure of variability.\nThe most important measure of variability and the one that will be central to our analysis is the (empirical) variance.\n\nDefinition 2.1 (Empirical Variance) For a vector of values x = (x_1, \\ldots, x_n) it is defined as the average (apart from a small correction in the denominator) squared deviation of the values from their mean.\n\nS^2_x = \\frac{(x_1 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x})^2}{n - 1} = \\frac{1}{n - 1} \\sum_{i = 1}^{n}(x_i - \\bar{x})^2: \\quad \\text{variance}\\\\\nS_x = \\sqrt{S^2_x} \\quad \\text{standard deviation}\n\n\n\nExample 2.1 (Computing the empirical variance) Lets apply the formula from Definition 2.1 to a very simple example with just three values.\n\nx_1 = -1, x_2 = 0, x_3 = 4\n\nFirst, the empirical mean of these values is\n\n\\bar{x} = \\frac{-1 + 0 + 4}{3} = 1\n\nNow lets substitute these values in the definition of the empirical variance:\n\n\\begin{aligned}\nS^2_{x} & = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 }{n - 1} \\\\\n        & = \\frac{(-1 - 1)^2 + (0 - 1)^2 + (4 - 1)^2 }{3 - 1} \\\\\n        & = \\frac{(-2)^2 + (- 1)^2 + (3 )^2 }{2} \\\\\n        & = \\frac{4 + 1 + 9 }{2} \\\\\n        & = \\frac{14}{2} \\\\\n        & = 7\n\\end{aligned}\n\nUsing R to compute the same thing:\n\n# Create a vector called x\nx &lt;- c(-1, 0, 4)\n# Compute the average of the values in x and store it in a variable called x_avg (look it up in the global environment)\n\nx_avg &lt;- mean(x)\n\n# Manually compute the variance of x\n((-1 - x_avg)^2 + (0 - x_avg)^2 + (4 - x_avg)^2) / (length(x) - 1)\n\n[1] 7\n\n\nThere is also a special function var that can compute it from a vector\n\nvar(x)\n\n[1] 7\n\n\nThe (empirical) standard deviation is simply the square root of the (empirical) variance.\n\nS_x = \\sqrt{S^2_x} = \\sqrt{7} \\approx 2.64\n\nIn R you have two options: take the square root of the result of var using the sqrt function or use sd (standard deviation) to compute the standard deviation directly.\n\nsqrt(var(x))\n\n[1] 2.645751\n\nsd(x)\n\n[1] 2.645751\n\n\n\n\nExercise 2.1 (Empirical Moments (Mean and Variance))  \n\nCompute the sample mean and variance of the annual earnings of the customers in the earnings data set. First, compute it by accessing the columns of the data using the dollar syntax.\n\n\nThen use the summarize function from the dplyr (already loaded) package to compute the mean and the variance of the earn variable.\n\n\nRun the same calculations as before, but this time group the dataset by the ethnicity variable. This will give you the mean and the variance of the annual earnings for every ethnic group in the data set. Use the group_by function.\n\n\n\n\n2.2.3 Overview of the data\nThe skim function from the skimr package provides a quick overview of the data set. It shows the number of observations, the number of variables, the names of the variables, the type of the variables, the number of missing values, the mean, the standard deviation, the minimum and maximum values, and the quartiles for the numeric variables.\n\n## Basic summaries for the whole tibble\nearnings %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nethnicity\n0\n1\n5\n8\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1.00\n66.57\n3.83\n57.00\n64.00\n66.00\n69.25\n82.00\n▂▇▅▁▁\n\n\nweight\n27\n0.99\n156.31\n34.62\n80.00\n130.00\n150.00\n180.00\n342.00\n▅▇▃▁▁\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nearn\n0\n1.00\n21147.30\n22531.77\n0.00\n6000.00\n16000.00\n27000.00\n400000.00\n▇▁▁▁▁\n\n\nearnk\n0\n1.00\n21.15\n22.53\n0.00\n6.00\n16.00\n27.00\n400.00\n▇▁▁▁▁\n\n\neducation\n2\n1.00\n13.24\n2.56\n2.00\n12.00\n12.00\n15.00\n18.00\n▁▁▁▇▃\n\n\nmother_education\n244\n0.87\n13.61\n3.22\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nfather_education\n295\n0.84\n13.65\n3.25\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nwalk\n0\n1.00\n5.30\n2.60\n1.00\n3.00\n6.00\n8.00\n8.00\n▃▁▃▁▇\n\n\nexercise\n0\n1.00\n3.05\n2.32\n1.00\n1.00\n2.00\n5.00\n7.00\n▇▁▁▁▃\n\n\nsmokenow\n1\n1.00\n1.75\n0.44\n1.00\n1.00\n2.00\n2.00\n2.00\n▃▁▁▁▇\n\n\ntense\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nangry\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nage\n0\n1.00\n42.93\n17.16\n18.00\n29.00\n39.00\n56.00\n91.00\n▇▇▃▃▁\n\n\nheight_cm\n0\n1.00\n169.08\n9.73\n144.78\n162.56\n167.64\n175.89\n208.28\n▂▇▅▁▁\n\n\nweight_kg\n27\n0.99\n70.34\n15.58\n36.00\n58.50\n67.50\n81.00\n153.90\n▅▇▃▁▁\n\n\n\n\n\n\n\n2.2.4 Summarizing a single categorical variable\nWhile the mean, median, and variance are useful for numeric variables, they are not defined for categorical variables. Instead, we can use the table function to count the number of observations in each category.\n\ntable(earnings$ethnicity)\n\n\n   Black Hispanic    Other    White \n     180      104       38     1494",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#visualizations",
    "href": "02-Descriptive-Statistics.html#visualizations",
    "title": "2  Descriptive Statistics",
    "section": "2.3 Visualizations",
    "text": "2.3 Visualizations\nHistogram\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nA smooth density plot is an alternative way to visualize the distribution of a variable.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density()\n\n\n\n\n\n\n\n\nThe boxplot shows the median and the 25-th and 75-th percentiles (the box). The whiskers in the plot stretch to the minimum or the maximum observed value, unless there are extreme observations that are shown as single dots.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nGroup comparisons\n\nearnings %&gt;%\n  ggplot(aes(x = height, y = ethnicity)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe scatterplot will be our primary tool in studying associations between variables. It represents each observation as a point in a coordinate system defined by the variables that we would like to study.\n\nearnings1 %&gt;%\n  ggplot(aes(x = weight_kg, y = height_cm)) +\n  geom_point(position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Weight (kg)\",\n    y = \"Height (cm)\"\n  )\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create a scatterplot\nfig &lt;- plot_ly(earnings1, x = ~weight_kg, y = ~height_cm, mode = 'markers')\nfig\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nWarning: Ignoring 27 observations\n\n\n\n\n\n\n\nsummary(lm(height_cm ~ weight_kg, data = earnings1))\n\n\nCall:\nlm(formula = height_cm ~ weight_kg, data = earnings1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.639  -5.611  -0.084   5.645  36.248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 145.00916    0.89138  162.68   &lt;2e-16 ***\nweight_kg     0.34314    0.01237   27.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.15 on 1787 degrees of freedom\n  (27 observations deleted due to missingness)\nMultiple R-squared:  0.3009,    Adjusted R-squared:  0.3005 \nF-statistic: 769.1 on 1 and 1787 DF,  p-value: &lt; 2.2e-16\n\n\n\nExercise 2.2  \n\nCreate a new variable bmi (body mass index) in the earnings data set. The BMI is defined as the weight in kilograms divided by the square of the height in meters. The height in meters is the height in centimeters divided by 100.\n\n\nearnings &lt;- earnings %&gt;%\n  mutate(\n    bmi = weight_kg / (height_cm / 100)^2\n  )\n\n\nThe reference ranges for the BMI are as follows:\n\n\nUnder 18,5: Underweight\n18,5 - 24,9: Normal\n25 - 29,9: Overweight\n30 oder mehr: Obese\n\nCreate a two new variables is_underweight, is_normal in the dataset. Hint: Use the mutate function and the &lt;, &lt;=, &gt;, &gt;= operators. You can join multiple conditions using the & (logical and) operator. Don’t forget to uncommend the code first (Ctrl+Shift+C). How many customers are underweight and how many have a normal weight? Use the summarize function to compute the number of customers in each category.\n\nearnings &lt;- earnings %&gt;%\n  mutate(\n    is_underweight = bmi &lt; 18.5,\n    is_normal = bmi &gt;= 18.5 & bmi &lt;= 24.9\n  )\n\nearnings %&gt;%\n  summarize(\n    n_underweight = sum(is_underweight, na.rm = TRUE),\n    n_normal = sum(is_normal, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 2\n  n_underweight n_normal\n          &lt;int&gt;    &lt;int&gt;\n1            87     1032\n\n\n\n\nGroup the data by male and compute the number of customers in each group, as well as the number of underweight and normal weight customers in each group. Additionally, compute the share of underweight and normal weight customers in each group. Hint: Use the group_by function and the summarize function. Don’t forget to uncommend the code first (Ctrl+Shift+C). The n() function can be used to compute the number of observations in each group.\n\n\nearnings %&gt;%\n  group_by(male) %&gt;%\n  summarize(\n    # Count the number of customers in each group\n    n = n(),\n    n_underweight = sum(is_underweight, na.rm = TRUE),\n    n_normal = sum(is_normal, na.rm = TRUE),\n    share_underweight = mean(is_underweight, na.rm = TRUE),\n    share_normal = mean(is_normal, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 6\n   male     n n_underweight n_normal share_underweight share_normal\n  &lt;dbl&gt; &lt;int&gt;         &lt;int&gt;    &lt;int&gt;             &lt;dbl&gt;        &lt;dbl&gt;\n1     0  1141            71      688            0.0637        0.618\n2     1   675            16      344            0.0237        0.510",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html",
    "href": "03-Least-Squares.html",
    "title": "3  Least Squares",
    "section": "",
    "text": "3.1 Least Squares\nThe previous discussion leads us to a question. Can we find values for the coefficients in the linear equation that minimize the MSE? The answer is yes. The method is called least squares.\nFirst, let us visualize the MSE as a function of the coefficients. For this purpose, we will simply calculate the MSE for a grid of values of the coefficients and plot the results (Figure 3.4)\nCode\n# NOTE: this code here is only for illustration purposes, you don't need to study it or understand it for the course\n\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\n# Create a grid of values for beta0_hat and beta1_hat\n\nbeta0_hat &lt;- seq(0.4, 1, length.out = 50)\nbeta1_hat &lt;- seq(0.001, 0.015, length.out = 50)\n\ndt &lt;- expand.grid(beta0_hat = beta0_hat, beta1_hat = beta1_hat) %&gt;%\n  mutate(\n    # Compute the RSS for each combination of beta0_hat and beta1_hat\n    RSS = map2_dbl(beta0_hat, beta1_hat, ~{\n      Time_hat &lt;- .x + .y * invoices$Invoices\n      sum((invoices$Time - Time_hat)^2)\n    })\n  )\n\nfig &lt;- plot_ly(\n  x=beta0_hat, \n  y=beta1_hat, \n  z = matrix(dt$RSS, nrow = length(beta0_hat), ncol = length(beta1_hat)),\n  ) %&gt;% \n    add_surface() %&gt;% \n    layout(\n        scene = list(\n          xaxis = list(title = \"beta1_hat\"),\n          yaxis = list(title = \"beta0_hat\"),\n          zaxis = list(title = \"RSS\")\n        )\n    )\n\nfig\n\n\n\n\n\n\n\n\nFigure 3.4: RSS as a function of the coefficients\nOur goal is to find the values of \\beta_0 and \\beta_1 that minimize the RSS. The method of least squares provides a formula for the coefficients that minimize the RSS.\nFirst, let us solve a simpler problem. Assume that the predictive equation is\n\\widehat{\\text{Time}}_i = \\hat{\\beta}_0\nThe MSE in this case is much simpler.\ninvoices_beta_0_dt &lt;- expand_grid(\n    beta0_hat = seq(0, 4, length.out = 100),\n    invoices\n)\n\nrss_dt &lt;- invoices_beta_0_dt %&gt;%\n  group_by(beta0_hat) %&gt;%\n  summarise(\n    RSS = sum((Time - beta0_hat)^2)\n  )\n\nrss_dt %&gt;%\n    ggplot(aes(x = beta0_hat, y = RSS)) +\n    geom_line()\n\n\n\n\n\n\n\nFigure 3.5: MSE as a function of beta0_hat in an intercept-only equation\nHow do we find the value of \\hat{\\beta}_0 that minimizes the MSE? We take the derivative of the MSE with respect to \\hat{\\beta}_0 and set it to zero.\nIn order to find the minimum of the MSE, we take the derivative of the MSE with respect to \\hat{\\beta}_0 and set it to zero. Instead of \\text{Time} we will use the shorter notation y_i and instead of \\hat{\\text{Time}}_i we will use the shorter notation \\hat{y}_i.\n\\begin{align*}\nRSS(\\hat{\\beta}_0) & = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n& = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0)^2\n\\end{align*}\nTry to find the value of \\hat{\\beta}_0 that minimizes the MSE. Hint: Take the derivative of the RSS with respect to \\hat{\\beta}_0, set it to zero and solve for \\hat{\\beta}_0.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#least-squares",
    "href": "03-Least-Squares.html#least-squares",
    "title": "3  Least Squares",
    "section": "",
    "text": "Solution for the intercept-only equation\n\n\n\n\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} RSS(\\hat{\\beta}_0) & =\n\\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0) \\cdot (-1) \\\\\n& = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0) \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 \\sum_{i=1}^n \\hat{\\beta}_0 \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0\n\\end{align*}\n\nSetting the derivative to zero, we get\n\n\\begin{align*}\n-2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0 & = 0 \\\\\n\\hat{\\beta}_0 & = \\frac{1}{n} \\sum_{i=1}^n y_i \\\\\n& = \\overline{y}\n\\end{align*}\n\nThe value of \\hat{\\beta}_0 that minimizes the MSE is thus just the average of the observed values of y_i.\n\n\n\n\nExercise 3.3 Consider the following equation for the predictions\n\n\\hat{y} = \\hat{\\beta}_1 x\n\nFind the value of \\hat{\\beta}_1 that minimizes the RSS. Hint: Take the derivative of the RSS with respect to \\hat{\\beta}_1, set it to zero and solve for \\hat{\\beta}_1.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#the-one-variable-case-with-an-intercept",
    "href": "03-Least-Squares.html#the-one-variable-case-with-an-intercept",
    "title": "3  Least Squares",
    "section": "3.2 The one variable case with an intercept",
    "text": "3.2 The one variable case with an intercept\nNow let us consider the case where we have two variables, \\text{Invoices}_i and \\text{Time}_i. We want to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the RSS.\nThe prediction equation is\n\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\nThe RSS is (as above)\n\n\\begin{align*}\n\\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n  & = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n\\end{align*}\n\nThe first order conditions for the minimum are\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n\\frac{\\partial}{\\partial \\hat{\\beta}_1} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) x_i = 0\n\\end{align*}\n\nThe first equation gives\n\n\\begin{align*}\n\\sum_{i=1}^n y_i - \\hat{\\beta}_0 n - \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = 0 \\\\\n\\hat{\\beta}_0 n + \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = \\sum_{i=1}^n y_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nThe second equation gives\n\n\\begin{align*}\n\\sum_{i=1}^n y_i x_i - \\hat{\\beta}_0 \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = 0 \\\\\n\\hat{\\beta}_0 \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nSubstituting the expression for \\hat{\\beta}_0 in the second equation, we get\n\n\\begin{align*}\n(\\overline{y} - \\hat{\\beta}_1 \\overline{x}) \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\overline{y} \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i}{\\sum_{i=1}^n x_i^2 - \\overline{x} \\sum_{i=1}^n x_i}\n\\end{align*}\n\nSimplifying the expression, we get\n\n\\begin{align*}\n\\hat{\\beta}_1 & = \\frac{\\overline{x y} - \\overline{x} \\cdot \\overline{y}}{\\overline{x^2} - \\overline{x}^2} \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nThe last expression may seem a bit complicated, but it is actually quite simple. It is just the ratio between the empirical covariance between x_i and y_i divided by the variance of x_i.\nThe empirical covariance between x_i and y_i is defined as the sum of the products of the deviations of x_i and y_i from their respective means, divided by the number of observations.\n\nDefinition 3.1 (Covariance) The covariance between two variables x and y with n values is defined as\n\nS_{xy} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n\n\n\nDefinition 3.2 (Variance decomposition) We have already defined the variance of a variable x as\n\nS_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})^2\n\nIt can be shown that the variance of x can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.\n\nS_x^2 = \\frac{1}{n  - 1}(\\overline{x_i^2} - \\overline{x}^2)\n\n\n\nProof. The proof is straightforward. We have\n\n\\begin{align*}\n(n - 1) S_x^2 & =  \\sum_{i=1}^n (x_i - \\overline{x})^2 \\\\\n& =  \\sum_{i=1}^n (x_i^2 - 2x_i \\overline{x} + \\overline{x}^2) \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 \\sum_{i=1}^n 1 \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x}^2 n + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - n \\overline{x}^2 \\\\\n& = n (\\overline{x_i^2} - \\overline{x}^2)\n\\end{align*}\n\n\n\nDefinition 3.3 (Covariance and Covariance decomposition) The covariance between two variables x and y with n values is defined as\n\nS_{xy} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n\nMuch in the same way as the variance, the covariance can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.\n\n(n - 1) S_{xy} = n(\\overline{x y} - \\overline{x} \\overline{y})\n\nThe proof is similar to the proof for the variance decomposition.\n\n\nProof. The proof is straightforward. We have\n\n\\begin{align*}\n(n - 1) S_{xy} & = \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) \\\\\n& = \\sum_{i=1}^n x_i y_i - \\overline{x} \\sum_{i=1}^n y_i - \\overline{y} \\sum_{i=1}^n x_i + n \\overline{x} \\overline{y} \\\\\n& = n(\\overline{x y} - \\overline{x} \\overline{y})\n\\end{align*}\n\n\nTo understand what the covariance measures, consider the following scatterplot:\n\nset.seed(123)\ndt &lt;- tibble(\n    x = rnorm(100),\n    y = 2 * x + rnorm(100)\n)\n\ndt %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_vline(xintercept = mean(dt$x), colour = \"firebrick4\") +\n    geom_hline(yintercept = mean(dt$y), colour = \"steelblue4\")\n\n\n\n\n\n\n\nFigure 3.6: Scatterplot of two variables with a positive linear assocition\n\n\n\n\n\nThe reddish line is drawn at the average of the x values and the bluish line is drawn at the average of the y values. The covariance measures the average product of the deviations of the x and y values from their respective means. If the product is positive, it means that when x is above its average, y is also above its average. If the product is negative, it means that when x is above its average, y is below its average.\nYou can compute the empirical covariance between two variables using the cov function in R.\n\ncov(dt$x, dt$y)\n\n[1] 1.622745\n\n\nOnly the sign of the covariance is important. The magnitude of the covariance depends on the units of the variables. To make the covariance unit-free, we can divide it by the product of the standard deviations of the two variables. This gives us the correlation coefficient.\n\ncov(dt$x * 1000, dt$y)\n\n[1] 1622.745\n\ncov(dt$x , dt$y * 50)\n\n[1] 81.13723\n\n\n\nset.seed(123)\ndt1 &lt;- tibble(\n    x = rnorm(100),\n    y = -3 * x + rnorm(100)\n)\n\ndt1 %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_vline(xintercept = mean(dt1$x), colour = \"firebrick4\") +\n    geom_hline(yintercept = mean(dt1$y), colour = \"steelblue4\")\n\n\n\n\n\n\n\nFigure 3.7: Scatterplot of two variables with a positive linear assocition\n\n\n\n\n\n\ncov(dt1$x, dt1$y)\n\n[1] -2.54342\n\n\n\nDefinition 3.4 (Correlation) The correlation between two variables x and y with n values is defined as\n\nr_{xy} = \\frac{S_{xy}}{S_x S_y}\n\nwhere S_{xy} is the covariance between x and y, and S_x and S_y are the standard deviations of x and y respectively.\n\nBecause the covariance is divided by the product of the standard deviations, the correlation is unit-free. Furthermore, the correlation is always between -1 and 1. A correlation of 1 means that the two variables lie on a straight line with a positive slope. A correlation of -1 means that the two variables lie on a straight line with a negative slope. A correlation of 0 means that there is no linear association between the two variables.\n\ncor(dt$x, dt$y)\n\n[1] 0.8786993\n\ncor(dt1$x, dt1$y)\n\n[1] -0.9448502",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#exercise",
    "href": "03-Least-Squares.html#exercise",
    "title": "3  Least Squares",
    "section": "3.3 Exercise",
    "text": "3.3 Exercise\nWrite a function that takes two vectors x and y as arguments and returns the OLS coefficients \\hat{\\beta}_0 and \\hat{\\beta}_1 using the formulas above.\n\nols_two_variables &lt;- function(x, y){\n    # Compute the coefficients\n    beta_1_hat &lt;- \n    beta_0_hat &lt;- \n\n    list(beta_0_hat = beta_0_hat, beta_1_hat = beta_1_hat)\n}\n\nTest your function using the following data set.\n\nset.seed(123)\n\nx_test &lt;- rnorm(100)\ny_test &lt;- 1 + 2 * x_test + rnorm(100)\n\nCompare your results with the lm function in R.\n\n# Uncomment the following line to compare your results with the lm function\n\n# ols_two_variables(x, y)\nlm(y_test ~ x_test)\n\n\nCall:\nlm(formula = y_test ~ x_test)\n\nCoefficients:\n(Intercept)       x_test  \n     0.8972       1.9475",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#ols-using-the-lm-function-in-r",
    "href": "03-Least-Squares.html#ols-using-the-lm-function-in-r",
    "title": "3  Least Squares",
    "section": "3.4 OLS using the lm function in R",
    "text": "3.4 OLS using the lm function in R\nOur primary tool for calculating the least squares coefficients will be the lm function in R. The lm function takes a formula as its first argument and a data set as its second argument. The formula is of the form y ~ x where y is the dependent variable and x is the independent variable. The data set is a data frame/tibble with the variables x and y.\nUse the lm function to compute the OLS coefficients for the invoices data set. and the model:\n\n\\widehat{\\text{Time}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{Invoices}\n\n\n# Uncomment the following two lines to fit the model\n\n# fit_OLS &lt;- lm(WRITE THE FORMULA HERE, data = invoices)\n# fit_OLS\n\n\n# Compute the predicted values for the days (observations) in the sample\n\n# time_predicted_OLS &lt;- predict(fit_OLS)\n\n\n# Compute the RSS for the OLS model\n\n# invoices &lt;- invoices %&gt;%\n#     mutate(\n#       Time_hat_OLS = time_predicted_OLS,\n#       residuals_OLS = # WRITE YOUR CODE HERE\n#     )\n\n\n# Compute the RSS and MSE for the OLS model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#plot-the-ols-predictions",
    "href": "03-Least-Squares.html#plot-the-ols-predictions",
    "title": "3  Least Squares",
    "section": "3.5 Plot the OLS predictions",
    "text": "3.5 Plot the OLS predictions\n\n# Plot the predictions\n\n# invoices %&gt;%\n#     ggplot(aes(x = Invoices, y = Time)) +\n#     geom_point() +\n#     geom_line(aes(y = Time_hat_OLS), colour = \"steelblue4\")\n\n\n# Plot the predictions using geom_smooth\n\n# invoices %&gt;%\n#     ggplot(aes(x = Invoices, y = Time)) +\n#     geom_point() +\n#     geom_smooth(method = \"lm\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#interpretation-of-the-ols-coefficients",
    "href": "03-Least-Squares.html#interpretation-of-the-ols-coefficients",
    "title": "3  Least Squares",
    "section": "3.6 Interpretation of the OLS coefficients",
    "text": "3.6 Interpretation of the OLS coefficients\n\nScale of the coefficients\nInterpretation of the intercept\nInterpretation of the slope",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#least-squares-in-matrix-notation",
    "href": "03-Least-Squares.html#least-squares-in-matrix-notation",
    "title": "3  Least Squares",
    "section": "3.7 Least Squares in Matrix Notation",
    "text": "3.7 Least Squares in Matrix Notation\nThe previous discussion can be summarized in matrix notation and you will commonly find it in this form in textbooks and articles.\nFor the one variable case, the prediction equation, written for all observations is\n\n\\begin{align*}\n\\hat{y}_1 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 \\\\\n\\hat{y}_2 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_2 \\\\\n\\vdots & \\\\\n\\hat{y}_n & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_n \\\\\n\\end{align*}\n\nThis can be written in matrix notation as\n\n\\underbrace{\\begin{bmatrix}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\vdots \\\\\n\\hat{y}_n\n\\end{bmatrix}\n}_{\\hat{y}_{n \\times 1}}\n=\n\\underbrace{\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}}_{X_{n \\times 2}}\n\\underbrace{\\begin{bmatrix}\n\\hat{\\beta}_0 \\\\\n\\hat{\\beta}_1\n\\end{bmatrix}}_{\\hat{\\beta}_{2 \\times 1}}\n\nThe matrix X is called the design matrix and contains the predictor terms. The vector \\hat{\\beta} contains the coefficients that we want to estimate. The vector \\hat{y} contains the predicted values.\nYou can extract the design matrix from the fit_OLS object using the model.matrix function.\n\nfit_OLS &lt;- lm(Time ~ Invoices, data = invoices)\nmodel.matrix(fit_OLS)\n\n   (Intercept) Invoices\n1            1      149\n2            1       60\n3            1      188\n4            1       23\n5            1      201\n6            1       58\n7            1       77\n8            1      222\n9            1      181\n10           1       30\n11           1      110\n12           1       83\n13           1       60\n14           1       25\n15           1      173\n16           1      169\n17           1      190\n18           1      233\n19           1      289\n20           1       45\n21           1      193\n22           1       70\n23           1      241\n24           1      103\n25           1      163\n26           1      120\n27           1      201\n28           1      135\n29           1       80\n30           1       29\nattr(,\"assign\")\n[1] 0 1\n\n\nThe colum of ones in the design matrix is the intercept term. The other column is the predictor term (the number of invoices in our example).\n\n\\hat{y} = X \\hat{\\beta}\n\nYou can think about the least squares as finding a solution to a system of linear equations where the number of equations is greater than the number of unknowns. Let us focus on a case with one variable and two observations (the first two from the invoices data set).\n\ninvoices %&gt;%\n head(n = 2)\n\n  Day Invoices Time Time_hat_eq1 residuals_eq1 Time_hat_eq2 residuals_eq2\n1   1      149  2.1         2.11         -0.01         2.09          0.01\n2   2       60  1.8         2.11         -0.31         1.20          0.60\n\n\n\n2.1 = \\hat{\\beta}_0 + \\hat{\\beta}_1 149 \\\\\n1.8 = \\hat{\\beta}_0 + \\hat{\\beta}_1 60\n\nIn this system of equations you have two unknowns, \\hat{\\beta}_0 and \\hat{\\beta}_1, and two equations. Therefore, you can actually solve for the unknowns (left as an exercise).\n\nM &lt;- matrix(c(1, 149, 1, 60), ncol = 2, byrow = TRUE)\ny &lt;- c(2.1, 1.8)\nsolve(M, y)\n\n[1] 1.597752809 0.003370787\n\n\nYou will find that the solution is \n\\begin{align*}\n\\hat{\\beta}_0 \\approx 1.598 \\\\\n\\hat{\\beta}_1 \\approx 0.0034\n\\end{align*}\n\nLet’s look at the first three observations in the invoices data set.\n\ninvoices %&gt;%\n head(n = 3)\n\n  Day Invoices Time Time_hat_eq1 residuals_eq1 Time_hat_eq2 residuals_eq2\n1   1      149  2.1         2.11         -0.01         2.09          0.01\n2   2       60  1.8         2.11         -0.31         1.20          0.60\n3   3      188  2.3         2.11          0.19         2.48         -0.18\n\n\nThe equations for the first three observations are\n\n\\begin{align*}\n2.1 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 149 \\\\\n1.8 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 60 \\\\\n2.2 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 201\n\\end{align*}\n\nSubstituting the solution for \\hat{\\beta}_0 and \\hat{\\beta}_1 that we obtained from the first two equations into the third equation, we get\n\n1.597752809 + 0.003370787 * 201\n\n[1] 2.275281\n\n\nwhich is not exactly equal to 2.2, therefore the third equation is not satisfied.\nIn the general case we cannot hope to solve the system of equations exactly. What we could hope for is to find an approximate solution which is close to the actual left hand side values. The least squares method provides such a solution by defining a sense of closeness between the approximate solution and the actual left hand side values (data) and then finding the approximate solution that minimizes that closeness measure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "03-Least-Squares.html#the-geometry-of-least-squares",
    "href": "03-Least-Squares.html#the-geometry-of-least-squares",
    "title": "3  Least Squares",
    "section": "3.8 The Geometry of Least Squares",
    "text": "3.8 The Geometry of Least Squares\nIn the previous section we derived the formula using calculus and an objective function (the RSS) that we wanted to minimize. In this section we will derive the formula using a geometric approach.\nFirst, let us think about the data in terms of vectors in a \\mathbb{R}^n dimensional space (where n is the number of observations). For an easier visualization, let us consider the case n = 2 (only two observations) so that we can easily plot the data.\n\n\nCode\nif (!require(\"ggforce\")) {\n  install.packages(\"ggforce\")\n}\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading ggforce from CRAN ...             OK [1.8 Mb in 0.87s]\n- Downloading tweenr from CRAN ...              OK [449 Kb in 0.59s]\n- Downloading polyclip from CRAN ...            OK [115 Kb in 0.57s]\n- Downloading RcppEigen from CRAN ...           OK [1.8 Mb in 0.63s]\nSuccessfully downloaded 4 packages in 3.7 seconds.\n\nThe following package(s) will be installed:\n- ggforce   [0.4.2]\n- polyclip  [1.10-6]\n- RcppEigen [0.3.4.0.0]\n- tweenr    [2.0.3]\nThese packages will be installed into \"~/work/econ2024/econ2024/renv/library/R-4.3/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing tweenr ...                         OK [installed binary and cached in 0.58s]\n- Installing polyclip ...                       OK [installed binary and cached in 0.42s]\n- Installing RcppEigen ...                      OK [installed binary and cached in 0.72s]\n- Installing ggforce ...                        OK [installed binary and cached in 1.0s]\nSuccessfully installed 4 packages in 2.9 seconds.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggforce)\n\nX &lt;- c(2, 3) / 4\nY &lt;- c(1, 0.2)\n\nY_proj &lt;- X %*% Y / X %*% X * X\nY_min_Y_proj &lt;- Y_proj - Y\n\ndf &lt;- tibble(\n  x = c(0, 0, Y[1], 0, Y[1], Y[2], Y_proj[1], Y_proj[1]),\n  y = c(0, 0, Y[2], 0, X[2], Y[2], Y_proj[2], Y_proj[2]),\n  xend = c(X[1], Y[1], Y_proj[1], Y_proj[1], NA, NA, NA, NA),\n  yend = c(X[2], Y[2], Y_proj[2], Y_proj[2], NA, NA, NA, NA),\n  color = c('A', 'B', 'C', 'D', NA, NA, NA, NA)\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_segment(\n    aes(\n      xend = xend, yend = yend, \n      color = color\n      ),\n      arrow = arrow(length = unit(0.3,\"cm\")\n    )\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"x\",\n    x = 0.51, y = 0.77\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"y_hat = bx\",\n    x = 0.31, y = 0.6\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"y\",\n    x = 1.02, y = 0.19\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"r = y - bx\",\n    x = 0.76, y = 0.42\n  ) + \n  labs(\n    x = \"\",\n    y = \"\"\n  ) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 3.8: Vector projection\n\n\n\n\n\nWhat is the projection of the vector y onto the vector x? It is the vector that is closest to y and lies on the line spanned by x.\nTwo vectors are said to be orthogonal if their dot product is zero. The dot product of two vectors x and y is defined as\n\nx \\cdot y = \\sum_{i=1}^n x_i y_i\n\nAnother way to write the dot product is as a matrix product:\n\nx \\cdot y = x^T y\n\nwhere x^T is the transpose of x (meaning that x^T is a row vector).\nThe least squares method can be thought of as finding the vector y_{proj} that is closest to y and lies on the line spanned by x. The vector y - y_{proj} is the vector that is orthogonal to x and has the smallest length.\nThe vector y - y_{proj} is called the residual vector. The length of the residual vector is the square root of the sum of the squares of its elements. This is the same as the square root of the sum of the squares of the residuals.\nThe residual vector will be smallest in length when it is orthogonal to x. Therefore we can find a scalar \\hat{\\beta}_1 such that the residual vector is orthogonal to x.\nThe condition that the residual vector is orthogonal to x is that the dot product between the two vectors is equal to zero.\n\n\\begin{align*}\nx^T(y - \\hat{\\beta}_1 x) = 0 \\\\\nx^T y - \\hat{\\beta}_1 x^T x = 0 \\\\\n\\hat{\\beta}_1 x^T x = x^T y \\\\\n\\hat{\\beta}_1 = \\frac{x^T y}{x^T x}\n\\end{align*}\n\nYou can rewrite the dot products as sum to see the similarity to the formulas we derived using calculus.\n\n\\begin{align*}\nx^T y = \\sum_{i=1}^n x_i y_i \\\\\nx^T x = \\sum_{i=1}^n x_i^2\n\\end{align*}\n\n\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} = \\frac{n \\overline{x y}}{n \\overline{x^2}} = \\frac{\\overline{x y}}{\\overline{x^2}}\n\nIn the general case of multiple variables, the projection of the outcome vector y onto the space spanned by all linear combinations of the predictor variables X is\n\n\\begin{align*}\nX^T(y - X \\hat{\\beta}) = 0 \\\\\nX^T y - X^T X \\hat{\\beta} = 0 \\\\\nX^T X \\hat{\\beta} = X^T y \\\\\n\\end{align*}\n\nIn the one-variable case without an intercept term, we could devide the equation by x^T x to get the formula for \\hat{\\beta}_1. This worked because the scalar product of a vector is a scalar. However, here we need to deal with a whole matrix (X^TX). We can solve the equation for \\hat{\\beta} by pre-multiplying both sides by the inverse of X^T X.\n\n\\begin{align*}\n\\hat{\\beta} & = (X^T X)^{-1} X^T y\n\\end{align*}\n\nThe last operation requires that inverse of X^T X exists. This is the case if the columns of X are linearly independent. If some of the columns of X are linearly dependent, the matrix X^T X will be singular and its inverse will not exist.\n\n# Create t full column rank matrix\nx_fcr &lt;- matrix(\n  c(1, 149, 1, 60, 1, 201),\n  ncol = 2,\n  byrow = TRUE\n)\nx_fcr\n\n     [,1] [,2]\n[1,]    1  149\n[2,]    1   60\n[3,]    1  201\n\n\n\n# Compute the inverse of the matrix. The %*% operator is the matrix multiplication operator in R\n\nsolve(t(x_fcr) %*% x_fcr)\n\n            [,1]          [,2]\n[1,]  2.17013047 -1.343998e-02\n[2,] -0.01343998  9.834131e-05\n\n\n\n# To veritfy that the inverse is correct, multiply the matrix with its inverse\n\nt(x_fcr) %*% x_fcr %*% solve(t(x_fcr) %*% x_fcr)\n\n              [,1]          [,2]\n[1,]  1.000000e+00 -4.282599e-18\n[2,] -1.344411e-13  1.000000e+00\n\n\nWhat happens if we have a matrix that is not full column rank?\n\nx_rcr &lt;- matrix(\n  c(1, 149, 2, 1, 60, 2, 1, 149, 2),\n  ncol = 3,\n  byrow = TRUE\n)\nx_rcr\n\n     [,1] [,2] [,3]\n[1,]    1  149    2\n[2,]    1   60    2\n[3,]    1  149    2\n\n\n\n# solve(t(x_rcr) %*% x_rcr)\n\nIn the case of a matrix that is not full column rank, the inverse of the matrix does not exist, and solve will throw an error. We say that there is (perfect) multicollinearity in predictors.\nThe same will happen if some column is a linear combination (e.g. the sum) of some other columns.\n\nx_rcr1 &lt;- cbind(x_fcr, 0.2 * x_fcr[, 1] + 5 * x_fcr[, 2])\nx_rcr1\n\n     [,1] [,2]   [,3]\n[1,]    1  149  745.2\n[2,]    1   60  300.2\n[3,]    1  201 1005.2\n\n\n\n# solve(t(x_rcr1) %*% x_rcr1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html",
    "href": "04a-Probability-Review.html",
    "title": "4  Statistics Review (Optional)",
    "section": "",
    "text": "4.1 Probability\nImagine a game where you flip a single coin once. The possible outcomes are head (H) and tail (T). The set of all possible outcomes of the game is called the sample space of the experiment. We will denote this set with \\Omega. For a single coin toss game, the sample space is \\Omega = \\{\\text{heads}, \\text{tails}\\}.\nSee Bertsekas and Tsitsiklis (2008) (Chapter 1) for a more thorough treatment of the subject.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#sec-probability",
    "href": "04a-Probability-Review.html#sec-probability",
    "title": "4  Statistics Review (Optional)",
    "section": "",
    "text": "Definition 4.1 (Probability) Let \\Omega denote the sample space of a random experiment. Let A \\subseteq \\Omega and B \\subset \\Omega be two disjoint events (i.e. A \\cap B = \\varnothing). Disjoint events are events that cannot co-occur. A probability measure on this space has the following properties:\n\n\\begin{align}\n  & P(A) \\geq 0 \\qquad \\text{non-negativity}\\\\\n  & P(\\Omega) = 1 \\qquad  \\text{unit measure} \\\\\n  & P(A \\cup B) = P(A) + P(B) \\qquad \\text{additivity}\n\\end{align}\n\n\n\nTheorem 4.1 (Probability of complementary sets) Let \\Omega be a sample space, let A \\subseteq \\Omega be a subset of the sample space, and let \\bar{A} = \\Omega \\setminus A be the complement of A in \\Omega. Then the probability of the complementary set is given by:\n\nP(A) = 1 - P(\\bar{A})\n\n\n\nProof. For the proof note that A and \\bar{A} are disjoint by definition (A \\cap \\bar{A} = \\varnothing). Using the additivity of probability together with the unit probability of the sample space \\Omega from Definition 4.1, it follows that:\n\\begin{align}\n    P(A \\cup \\bar{A}) = P(A) + P(\\bar{A})\n  \\end{align}\n\\begin{align}\n    P(A \\cup \\bar{A}) & =  P(\\Omega) \\\\\n    P(A) + P(\\bar{A}) & = 1 \\implies \\\\\n    P(A) & = 1 - P(\\bar{A}).\n  \\end{align}\n\n\nExample 4.1 (Weather forecast) The weather forecast for the next day shows that it will be raining (A) with probability P(A) = 0.3. The sample space is \\Omega = \\{A, \\bar{A}\\} and the probability of not raining (\\bar{A}) is then P(\\bar{A}) = 1 - 0.3 = 0.7.\n\n\nExample 4.2 (Dice) In a game where you roll a (6-sided) dice once the sample space is \\Omega = \\{1, 2, 3, 4, 5, 6\\}. Denote the outcome of a roll with X and assume that the probability of each outcome is equal: P(X = i) = 1 / 6, i = 1,2,\\ldots,6. The probability of the event X = 1 is then P(X = 1) = 1/6. The probability of the event “outcome is not one” is P(X \\neq 1) = P(X &gt; 1) = 5 / 6.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#sec-discrete-distributions",
    "href": "04a-Probability-Review.html#sec-discrete-distributions",
    "title": "4  Statistics Review (Optional)",
    "section": "4.2 Discrete distributions",
    "text": "4.2 Discrete distributions\nWhen we presented the axioms of probability we mentioned the concept of a sample space and probability. For the sake of brevity we will skip almost all of the set theoretic foundation of probability measures and will directly introduce the concept of a random variable.\nLet us go back to the roots of probability theory that date back at least to the 17th century and the study of games of chance (gambling) (Freedman, Pisani, and Purves 2007, 248). Almost all introductory text on probability theory start with an example of some simple game of chance. We will follow this example, because it is easy to understand a simple game and the mathematical concepts involved. Later we will see how we can apply the concepts developed here to an extremely broad range of problems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#discrete-random-variables",
    "href": "04a-Probability-Review.html#discrete-random-variables",
    "title": "4  Statistics Review (Optional)",
    "section": "4.3 Discrete Random Variables",
    "text": "4.3 Discrete Random Variables\nImagine that you are about to buy a lottery ticket that costs 1 EUR. Until you buy and scratch the ticket you don’t actually know how much you will win, but before you commit to the purchase you may wonder what winnings you should expect. Without knowing the rules of the lottery you would be completely in the dark about your prospects, so let us assume that you actually know how the lottery works.\nFrom our point of view the rules of the game are completely determined by two things. The first one is the set of possible outcomes (the sample space) of the lottery and let’s assume that each ticket can win 0 EUR, 1 EUR or 2 EUR. Notice that the set of possible values is finite as there are only three possible outcomes. Let us write X for the (yet unknown) winning from our lottery ticket. In lottery games the value of X depends on some random mechanism (for example drawing numbered balls, spinning a wheel, etc.), therefore it is a function of the outcome of this random mechanism. We will call functions like X random variables. For the most part we will not refer the underlying random mechanism and will simply focus on the distribution of the possible values of X. The second part of the rules is how often winnings of 0 EUR, 1 EUR and 2 EUR occur when you repeatedly play the game. Obviously, a game where half of the tickets win 2 EUR is quite different from a game where only one out of 100 tickets wins 2 EUR.\n\n\n\nTable 4.1: Distribution of outcomes for two games of chance. Possible outcomes and probabilities for each outcome.\n\n\n\n  Winnings(x) P(x) Winnings(y) P(y)\n1       0 EUR  0.5       0 EUR  0.5\n2       1 EUR  0.3       1 EUR  0.1\n3       2 EUR  0.2       2 EUR  0.3\n\n\n\n\nLet us focus on the first game with probabilities (0.5, 0.3 and 0.2) and develop some intuitive understanding of these quantities. You can think about the probabilities in Table 4.1 as theoretical proportions in a sense that if you play the lottery 100 times you would expect to win nothing (0 EUR) in about 50 games, 1 EUR in about 30 games and 2 EUR in about 20 games. Notice that to expect 20 2-EUR wins out of 100 games is absolutely not the same as the statement that you will win 2 EUR in exactly 20 out of 100 games! To convince yourself look at Table 4.2 which presents the results of five simulated games with 100 tickets each. You can play with this and similar games by changing the number of tickets and the number of games in this simulation. In the first game the player had 49 tickets that won nothing, but in the second game she had only 38 0-win tickets. When we say to expect 50 0-wins out of 100 tickets we mean that the number of observed (actually played) 0-wins will vary around 50. In neither of the five simulated games was the number of 0-wins exactly equal to 50 (this is also possible, though).\n\nset.seed(43243)\n\nEx &lt;- sum(x * probs)\nnGames &lt;- 5\ncnts &lt;- rmultinom(n = nGames, size = 100, prob = probs)\navgWinnings &lt;-apply(cnts, 2, function(cnt) sum(cnt * x) / 100)\ngameResult &lt;- data.frame(t(cnts), avgWinnings)\n\ncolnames(gameResult) &lt;- c(paste('x =', 0:(length(probs) - 1), sep = ' '), 'Average winnings per ticket')\nrownames(gameResult) &lt;- paste('Game', 1:nGames, sep = ' ')\n\ngameResult %&gt;%\n  knitr::kable()\n\n\n\nTable 4.2: Simulation of five games with 100 tickets each. Number of tickets by outcome (0, 1, or 2 EUR) and average ticket win.\n\n\n\n\n\n\n\nx = 0\nx = 1\nx = 2\nAverage winnings per ticket\n\n\n\n\nGame 1\n49\n34\n17\n0.68\n\n\nGame 2\n38\n36\n26\n0.88\n\n\nGame 3\n47\n31\n22\n0.75\n\n\nGame 4\n49\n26\n25\n0.76\n\n\nGame 5\n62\n23\n15\n0.53\n\n\n\n\n\n\n\n\nThe probabilities in Table 4.1 completely describe the two games. The functions that assigns a probability to each possible outcome p(x) and p(y) are called probability mass functions. These functions incorporate everything there is to know about our hypothetical games. While this knowledge will not guarantee you a profit from gambling, it enables you to compute the expected value of each ticket, the probability that none of your tickets will win, etc. An important property of the probability mass function is that it is always non-negative (no negative probabilities) and that the sum of the probabilities over all possible values is exactly 1.\n\nDefinition 4.2 (Probability mass function) For a discrete random variable X with possible values x_1,\\ldots,x_K a function p(x_i) that assigns a probability to the possible values of X is called a probability mass function.\n\n\\begin{align}\n  P(X = x_k) = p_k.\n\\end{align}\n The probabilities are real numbers in the interval [0, 1] and need to sum to 1 over all possible values.\n\\begin{align}\n  p_k \\geq 0\\\\\n  \\sum_{k = 1} ^ {K} p_k = 1.\n\\end{align}\n\nNote that the set of K possible values x_1,\\ldots,x_K is finite. The same definition can be used for infinite sets of possible values as long as these are countably infinite but we will skip this discussion.\n\nExample 4.3 (Probability mass function) The first game in Table 4.1 has three possible outcomes: x_1 = 0, x_2 = 1, x_3 = 2. The probability mass function is then\n\np(x) = \\begin{cases}\n0.5 & \\text{if} \\quad x = 0 \\\\\n0.3 & \\text{if} \\quad x = 1 \\\\\n0.2 & \\text{if} \\quad x = 2 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\n\nA common visualization of the probability mass function is a barchart with the heights of the bars corresponding to the probability of each outcome.\n\ntibble(\n  x = c(0:2),\n  p = c(0.5, 0.3, 0.2)\n) %&gt;%\n  ggplot(aes(x = x, y = p)) + \n    geom_bar(stat = \"identity\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#expected-value",
    "href": "04a-Probability-Review.html#expected-value",
    "title": "4  Statistics Review (Optional)",
    "section": "4.4 Expected value",
    "text": "4.4 Expected value\nJust as the descriptive statistics (average, empirical median, empirical quantiles, etc.) are useful for summarizing a set of numbers we would like to be able to summarize distribution functions.\nImagine that you plan to buy 100 tickets from the first game in Table 4.1. Based on the interpretation of probabilities as theoretical proportions you would expect that 50 of the tickets will win nothing, 30 of the tickets will bring you 1 EUR and 20 of the tickets will win 2 EUR. Thus you can write the expected winnings per ticket by summing the contributions of each ticket type:\n\n\\begin{align}\n  \\text{expected winnings} & =\n    \\underbrace{\\frac{50}{100} \\times 0\\text{ EUR}}_{0\\text{ EUR tickets}} +\n    \\underbrace{\\frac{30}{100} \\times 1\\text{ EUR}}_{1\\text{ EUR tickets}} +\n    \\underbrace{\\frac{20}{100} \\times 2\\text{ EUR}}_{2\\text{ EUR tickets}} \\\\\n    & = 0.5 \\times 0 \\text{EUR} + 0.3 \\times 1 \\text{EUR} + 0.2 \\times 2 \\text{EUR} = 0.7\\text{EUR}.\n\\end{align}\n\\tag{4.1}\nNote that the coefficients before the possible outcomes are simply their probabilities. Therefore in a game with 100 tickets you expect that each ticket will bring you 0.7 EUR (on average). Just as with the probabilities, the expected values does not tell you that your average win per ticket will be 0.7 EUR. If you take a look at the five simulated games in Table 4.2 you will notice that the realized average ticket wins are not equal to 0.7 but they vary around it. You can think about the expected value as the center of the distribution (see Table 4.2 and Freedman, Pisani, and Purves (2007), pp. 288). It is important to see that the expected value only depends on the probabilities and the possible values and it does not depend on the outcome of any particular game. Therefore it is a constant and not a random variable itself.\n\n\n\n\n\n\n\n\nFigure 4.1: Probabilities plot. The black vertical line depicts the expected value.\n\n\n\n\n\n\nLet us write the expected value in a more general way:\n\nDefinition 4.3 (Expected Value) For discrete random variable X with possible values x_1, x_2,\\ldots,x_n the weighted average of the possible outcomes:\n\nE(X) = \\sum_{i = 1} ^ {n} x_i p(x_i)\n\nis called the expected value of X. Sometimes we will refer to the expected value as the mean of the distribution or the mean of the random variable following the distribution.\n\nWe introduced the expected value with an example of a game with 100 tickets in order to illustrate it. You should notice from Definition 4.3 that the expected value is independent of the number of games played as it is a property of the probability mass function of the game.\n\nExercise 4.1 (Expected Value) Let Y be a game with possible winnings of -1, 0, and 2 EUR. The probabilities of these outcomes are P(x = -1) = 0.2, P(x = 0) = 0.7, P(x = 2) = 0.1.\n\nPlot the PMF using a bar chart\nCalculate the expected winnings of this game using Definition 4.3\n\nPlay the game using the function sample (check its documentation either by typing ?sample on the command line in R or by searching for it online). It selects size of the values specified in x according to the probabilities given in prob.\n\nsample(\n  x = c(-1, 0, 2), \n  size = 10, \n  prob = c(0.2, 0.7, 0.1),\n  replace = TRUE\n)\n\n [1]  0  0  0 -1  0  0  0  0 -1  0\n\n\nPlay the game a couple of times and look at the outcomes. Compute the average winnings using mean and compare the result with the expected value that you calculated in the previous step.\n\n\n4.4.1 Properties of the expected value\nIn the following we list a few important properties of the expected value that we will use throughout the course. In the following let X and Y be random variables with expected values E(X) and E(Y).\n\nTheorem 4.2 (Linearity of the Expected Value) Let X and Y be two random variables. Then the expected value of their sum equals the sum of their expected values.\n\\begin{align}\n  E(X + Y) = E(X) + E(Y)\n\\end{align}\n\n\nTheorem 4.3 (The Expected Value of a Constant) The expected value of a constant equals the constant itself. Let a be any fixed (not random) real number. Then its expected value is:\n\\begin{align}\n  E(a) = a.\n\\end{align}\n\n\nTheorem 4.4 (Expected value of a scaled random variable) Let X be a random variable and let a be any fixed (not random) real number. Then the expected value of aX is:\n\\begin{align}\n  E(aX) = a E(X)\n\\end{align}\n\n\nProof. \n\\begin{align}\n  E(aX) = \\sum_{i = 1} ^ {n} a x_i p(x_i) = a \\sum_{i = 1} ^ {n} x_i p(x_i) = aE(X).\n\\end{align}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "04a-Probability-Review.html#variance",
    "href": "04a-Probability-Review.html#variance",
    "title": "4  Statistics Review (Optional)",
    "section": "4.5 Variance",
    "text": "4.5 Variance\nLet us compare the two games in Table 4.1. Both have the same sample space (set of possible outcomes) and the same expected winnings; see Equation 4.1 and ?eq-expected-value-game-2. However, the games are not identical because their probability distributions are different. If given the choice to play only one game, which would you prefer?\nThe second game offers a higher probability of winning the highest prize (2 EUR) at the cost of a lower probability for the middle prize (1 EUR). In other words, it places a higher probability on extreme outcomes (far from the distribution’s center, i.e., the expected value). A summary of a distribution that measures its spread (i.e., how likely are extreme values) is the variance:\n\nDefinition 4.4 (Variance) For a discrete random variable X with possible outcomes x_1, x_2,\\ldots,x_n and probability function p(x) the variance is the expected quadratic deviation from the expected value of the distribution E(X).\n\n\\begin{align}\n  Var(X) & = E(X - E(X)) ^ 2 \\\\\n         & = \\sum_{i = 1}  ^ n \\left(x_i - E(X)\\right) ^ 2 p(x_i)\n\\end{align}\n\n\n\nExample 4.4 (Variance) The variance of the first game (X) is:\n\n\\begin{align}\n    Var(X) & = \\sum_{i = 1} ^ {3} (x_i - E(X)) ^ 2 p(x_i) \\\\\n           & = (x_1 - E(X)) ^ 2 p(x_i) + (x_2 - E(X)) ^ 2 p(x_2) + (x_3 - E(X)) ^ 2 p(x_3) \\\\\n           & = (0 - 0.7) ^ 2 \\times 0.5 + (1 - 0.7) ^ 2 \\times 0.3 + (2 - 0.7) ^ 2 \\times 0.2 \\\\\n           & = 0.61.\n  \\end{align}\n\n\n\nExercise 4.2 Compute the variance of Y, the second game described in Table 4.1.\n\n\nSolution. Var(Y) = 0.76.\n\n\nTheorem 4.5 (Independence of Random Variables) Two discrete random variables X and Y with possible values x_1,\\ldots,x_K and y_1,\\ldots,y_L are independent if for every k and l:\n\n\\begin{align}\n  P(X = x_k, Y = y_l) = P(X = x_k)P(Y = y_l)\n\\end{align}\n\n\n\nTheorem 4.6 For two independent random variables X and Y.\n\\begin{align}\n  Var(X + Y) = Var(X) + Var(Y)\n\\end{align}\nMore generally, if X_i, i = 1,\\ldots,n are independent random variables, the variance of their sum is equal to the sum of their variances:\n\n\\begin{align}\nVar\\left(\\sum_{i = 1}^{n} X_i\\right) = \\sum_{i = 1}^{n} Var(X_i).\n\\end{align}\n\n\n\nTheorem 4.6 actually holds even for only uncorrelated variables. We will discuss the concept correlation later.\n\nTheorem 4.7 (Variance of a Scaled Random Variable) Let a \\in \\mathbf{R} be a fixed (not random) real number and let X be a random variable with variance Var(X).\nThe variance of aX is given by:\n\\begin{align}\nVar(aX) = a^2 Var(X).\n\\end{align}\n\n\nProof. \n\\begin{align}\n  Var(aX) & = \\sum_{k = 1} ^ K (ax_k - E(aX))^2 p(x_k) \\\\\n          & = \\sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\\\\n          & = a^2 \\sum_{k = 1} ^ K a^2(x_k - E(X))^2 p(x_k) \\\\\n          & = a^2 Var(X).\n\\end{align}\n\nAnother useful formula for working with the variance is:\nOften it will be easier to compute the variance using the following decomposition: \\begin{align}\n  Var(X) & = E(X - E(X))^2 \\\\\n         & = E\\left(X^2 - 2XE(X) + E(X) ^ 2\\right) \\\\\n         & = E(X^2) - E(2XE(X)) + E(E(X) ^ 2) \\\\\n         & = E(X^2) - 2 E(X)E(X) + E(X) ^ 2 \\\\\n         & = E(X^2) - E(X)^2.\n\\end{align}\nThe proof above uses the fact that E(X) is a constant and applies Theorem 4.3 and Theorem 4.4.\n\n\n\n\nBertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to Probability. 2nd ed. Optimization and Computation Series. Belmont: Athena scientific.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2007. Statistics. 4th ed. New York: W.W. Norton & Co.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistics Review (Optional)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html",
    "href": "04b-Continuous-Distributions.html",
    "title": "5  Continuous Distributions (Review)",
    "section": "",
    "text": "5.1 The Uniform Distribution on [-1, 1]\nX \\sim \\text{Uniform}(-1, 1)\nf(x) = \\begin{cases}\n\\frac{1}{2} & -1 \\leq x \\leq 1\\\\\n0 & \\text{otherwise}\n\\end{cases} \\\\\nThe density of the distribution is constant on the interval [-1, 1] and zero elsewhere. The mean of the distribution is zero and the variance is 1/3. The distribution function is linear on the interval [-1, 1].\nunif_dens_plt &lt;- ggplot() +\n  xlim(c(-2, 2)) +\n  stat_function(fun = dunif, args = list(min = -1, max = 1), n = 1000) +\n  labs(\n    x = \"x\",\n    y = \"Density\"\n  )\n\nunif_dens_plt\n\n\n\n\n\n\n\nFigure 5.1: Density of the uniform distribution on [-1, 1]\n\\begin{align*}\nE(X) = \\int_{-1}^{1} x f(x) dx = 0 \\\\\nVar(X) = \\int_{-1}^{1} \\left(x - E(X)\\right)^2 f(x) dx = \\frac{1}{3} \\\\\nSD(X) = \\sqrt{Var(X)} = 1/\\sqrt{3} \\\\\n\\end{align*}\nF(x) = \\int_{-\\infty}^{x} f(x) dx = \\begin{cases}\n0 & x &lt; -1 \\\\\n\\frac{x + 1}{2} & -1 \\leq x \\leq 1\\\\\n1 & x &gt; 1\n\\end{cases}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#the-uniform-distribution-on--1-1",
    "href": "04b-Continuous-Distributions.html#the-uniform-distribution-on--1-1",
    "title": "5  Continuous Distributions (Review)",
    "section": "",
    "text": "5.1.1 Sampling from the Uniform Distribution\n\n## Generate 10 random numbers from the uniform distribution on [-1, 1]\nx_unif &lt;- runif(10, min = -1, max = 1)\nx_unif\n\n [1] -0.96697650 -0.32786304  0.71190600  0.07912313 -0.01115017  0.33789003\n [7]  0.99819779  0.41622379 -0.23501996 -0.14051634\n\n\n\n## Count the number of values less than zero\n\nsum(x_unif &lt; 0)\n\n[1] 5\n\n## Count the number of values greater than 0.5\n\n\n# Compute the probability of the event X &lt; 0 using the punif function\npunif(0, min = -1, max = 1)\n\n[1] 0.5\n\n\n\n# Compute the probability of the event X &gt; 0.5 using the punif function\n1 - punif(0.5, min = -1, max = 1)\n\n[1] 0.25\n\n\nCompare the result above with the probability of the event X &lt; 0.\n\n## Compute the average of the values (mean function)\n\n## Compute the variance of the values (var function)\n\nCompare the results above with the expected value and variance of the uniform distribution on [-1, 1].\nRerun the simulation with 10,000 values and compare the share of outcomes less than -1 with the probability of the event X &lt; -1 and the average and variance of the values with the expected value and variance of the uniform distribution on [-1, 1].",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#the-normal-distribution",
    "href": "04b-Continuous-Distributions.html#the-normal-distribution",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.2 The Normal Distribution",
    "text": "5.2 The Normal Distribution\nThe family of normal distributions is defined by two parameters: the mean \\mu and the standard deviation \\sigma. The density of the normal distribution is given by the formula:\n\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\nBecause we will use this distribution very often, we will introduce a special notation for the normal distribution:\n\nX \\sim N(\\mu, \\sigma^2)\n\nFor this course you don’t need to remember the density function of the normal distribution.\nThere are few properties of the normal distribution that you shold remember, though. The first two properties relate the expected value and the variance of the normal distribution to its parameters \\mu and \\sigma:\n\n\\begin{align*}\nE(X) & = \\mu \\\\\nVar(X) & = \\sigma^2\n\\end{align*}\n\n\nmeans &lt;- c(0, 0, 0, 2, 2, 2)\nsds &lt;- c(0.2, 0.5, 1, 0.2, 0.5, 1)\n\ndf &lt;- expand_grid(\n    mean = c(0, 2),\n    sd = c(0.2, 0.5, 1),\n    x = seq(-3, 5, length.out = 200)\n) %&gt;%\nmutate(\n    y = dnorm(x, mean = mean, sd = sd),\n    mean = paste0(\"mu = \", mean),\n    sd = paste0(\"sigma = \", sd)\n)\n\n# Plot\nggplot(df, aes(x = x, y = y, color = mean, lty = sd)) +\n  geom_line() +\n  labs(x = \"x\", y = \"Density\", color = \"Parameters\") +\n  ggtitle(\"Normal distributions with different means and variances\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#how-does-the-normal-distribution-arise",
    "href": "04b-Continuous-Distributions.html#how-does-the-normal-distribution-arise",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.3 How Does the Normal Distribution Arise?",
    "text": "5.3 How Does the Normal Distribution Arise?\n\nplayers_n &lt;- 300\ngames_n &lt;- 16\n\nunif_games &lt;- expand_grid(\n  game = 1:games_n,\n  player = 1:players_n\n) %&gt;%\n  mutate(\n    ## When used in mutate, n() returns the number of rows in a group of obs\n    ## When the data is not grouped as here, it retuns the number of obs in the whole table\n    result = runif(n(), min = -1, max = 1)\n  ) %&gt;%\n  bind_rows(\n    ## Add a initial values so that all players start with 0\n    tibble(\n      player = 1:players_n,\n      game = as.integer(0),\n      result = 0,\n    )\n  )\n\nunif_games &lt;- unif_games %&gt;%\n  ## Sort the data by player id and game id\n  arrange(player, game) %&gt;%\n  ## Groups the data by player, because we want the running totals to be calculated for each\n  ## player separately\n  group_by(player) %&gt;%\n  mutate(\n    running_total = cumsum(result)\n  )\n\n## Illustration only\nunif_games %&gt;%\n  ggplot(aes(x = game, y = running_total, group = player)) +\n  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +\n  geom_hline(yintercept = 0) +\n  geom_line(aes(color = player &lt; 2, alpha = player &lt; 2)) +\n  scale_color_manual(values = c(\"skyblue4\", \"firebrick4\")) +\n  scale_alpha_manual(values = c(1 / 5, 1)) +\n  scale_x_continuous(\"Game number\", breaks = c(0, 4, 8, 12, 16)) +\n  theme(legend.position = \"none\") +\n  labs(y = \"Running Total\")\n\n\n\n\n\n\n\n\n\nunif_games %&gt;%\n  filter(game == 4) %&gt;%\n  ggplot(aes(x = running_total)) +\n  geom_density() +\n  labs(title = \"Running total distribution at the 4-th game\") +\n  labs(\n    x = \"Running total\"\n  )\n\n\n\n\n\n\n\n\n\nunif_games %&gt;%\n  filter(game == 16) %&gt;%\n  ggplot(aes(x = running_total)) +\n  geom_density() +\n  labs(title = \"Running total distribution at the 16-th game\") +\n  labs(\n    x = \"Running total\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#probabilities-and-quantiles-of-the-normal-distribution",
    "href": "04b-Continuous-Distributions.html#probabilities-and-quantiles-of-the-normal-distribution",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.4 Probabilities and Quantiles of the Normal Distribution",
    "text": "5.4 Probabilities and Quantiles of the Normal Distribution\nAs with the other continuous distributions, we can compute probabilities and quantiles of the normal distribution using the functions pnorm and qnorm, respectively.\n\n\n\n\n\n\n\n\nFigure 5.2: Density of the standard normal distribution\n\n\n\n\n\nCompute the probability of the event X &lt; 0.5 for the standard normal distribution.\n\npnorm(0.5, mean=0, sd=1)\n\n[1] 0.6914625\n\n\nCompute the probability of the event X &gt; 1.96 for the standard normal distribution.\nCompute the probability of the event -1.3, 1 for the standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "04b-Continuous-Distributions.html#sampling-from-the-normal-distribution",
    "href": "04b-Continuous-Distributions.html#sampling-from-the-normal-distribution",
    "title": "5  Continuous Distributions (Review)",
    "section": "5.5 Sampling from the Normal Distribution",
    "text": "5.5 Sampling from the Normal Distribution\nTake a sample of 10 values from the standard normal distribution and store them in a variable x_norm.\n\nx_norm &lt;- rnorm(10, mean = 0, sd = 1)\n\n\n# Count the number of values less than zero\n\nsum(x_norm &lt; 0)\n\n[1] 7\n\n\nCompare the result with the theoretical probability of the event X &lt; 0.\n\n# Compute the average of the values\n\n# Compute the standard deviation of the values\n\nCompare your results with the expected value and variance of the standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Continuous Distributions (Review)</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bertsekas, Dimitri P., and John N. Tsitsiklis. 2008. Introduction to\nProbability. 2nd ed. Optimization and Computation Series. Belmont:\nAthena scientific.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with\nR. 2nd edition. Statistics and Computing. New York:\nSpringer.\n\n\nFaraway, Julian James. 2015. Linear Models with R.\nSecond edition. Chapman & Hall/CRC Texts\nin Statistical Science Series. Boca Raton: CRC Press, Taylor &\nFrancis Group.\n\n\nFreedman, David, Robert Pisani, and Roger Purves. 2007.\nStatistics. 4th ed. New York: W.W. Norton & Co.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge\nNew York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge\nUniversity Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to\nRegression with R. Edited by George\nCasella, Stephen Fienberg, and Ingram Olkin. Springer Texts\nin Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo:\nO’Reilly.",
    "crumbs": [
      "References"
    ]
  }
]