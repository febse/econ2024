[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to linear regression analysis",
    "section": "",
    "text": "General Information",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Introduction to linear regression analysis",
    "section": "Schedule",
    "text": "Schedule\n\nTu 10:15-11:45 in room 308 (will move to room 303 after 27.02.2024)\nTu 12:15-13:45 in room 308\nWe 14:15-15:45 in room 308",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "Introduction to linear regression analysis",
    "section": "Grading",
    "text": "Grading\nTBA",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#github-repository",
    "href": "index.html#github-repository",
    "title": "Introduction to linear regression analysis",
    "section": "GitHub Repository",
    "text": "GitHub Repository\nAll course materials for the exercise classes will be available in the GitHub repository:\nhttps://github.com/febse/econ2024",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "index.html#software-setup",
    "href": "index.html#software-setup",
    "title": "Introduction to linear regression analysis",
    "section": "Software Setup",
    "text": "Software Setup\nThe exercise classes require a minimal software setup:\n\nInstall Git for you operating system from https://git-scm.com/downloads/.\nOpen a GitHub account at https://github.com/signup. While you don’t need an account to download the course materials, you will need one to receive and submit your assignments (this will be explained in details during the exercise classes). You can apply for the GitHub student benefits at https://education.github.com/benefits. If approved you can receive free access to the GitHub Pro plan and to GitHub Copilot, an AI tool that helps you write code.\nOpen https://cran.r-project.org/, and you will find links to R builds for different operating systems. Click on the link matching your operating system and choose the latest version of R. When using the Windows operating system, you will see a link “Install R for the first time .” Click on this link and then download the R installer. Run the installer. Leave the default settings unchanged unless you know what you are doing.\nAfter installing R, open https://posit.co/download/rstudio-desktop/. If the web page recognizes your operating system, you will see a download button (right side of the page) for R studio. If the button does not appear, scroll down the page and find the installer appropriate for your operating system.\nShould you encounter difficulties installing R and R Studio, you can watch these video guides:\n\n\nWindows\nMac\nUbuntu 22.04\n\n\nHere are some video guides on how to install git:\n\n\nWindows\nMac\nLinux\n\n\nThe following steps depend on git being installed. Open R Studio and open a new project dialog: File -&gt; New Project. In the dialog, click on the third option: version control. From the next menu, select git.\n\n  \nIn the Repository URL field, paste the address of the course repository:\n\n\n\n\n\n\nRepository URL\n\n\n\nInsert the following address in the Repository URL field:\nhttps://github.com/febse/econ2024.git\nthe one shown in the screenshot is outdated.\n\n\n Click on the Create Project button and wait for git R studio to clone the repository and open the project.\n\n\n\n\n\n\nRenv\n\n\n\nTo install the packages necessary for the course, click on the command line in the R console and type:\nrenv::restore()\nThis will trigger the download and installation of all the dependencies. It can take some time, so be patient. You only need to do it once for the project,\n\n\n\n\n\nStep 5\n\n\n 6. The content of the GitHub repository will be updated continuously throughout the semester. In order to download the new files or updated versions of already existing files, you can use git pull. Open the git window in the upper right pane of R studio and click the pull button. This will download all changes from the GitHub repository to your local copy.\n\n\n\nStep 7\n\n\n\nNote that if you have modified the files tracked by git that have changed in the repository, git pull will fail with an error similar to this one:\n\n\n\n\nPull error\n\n\nTo avoid this, you can roll back the file to its original state. Right-click on the file in the git window and choose “revert.”\n\n\n\nRevert\n\n\n\nIn the exercise classes, we will use many functions from the tidyverse system and several other packages. Before accessing these packages’ functionality, you need to install them first. Find the R console in R studio and paste the following line on the command line. Press enter to run it and wait for the installation to complete. The renv package will take care of the installation of the packages in a separate environment, so you should’t worry about installing packages.\n\nIn case it does not work, you can install the packages manually by running the following command in the R console:\ninstall.packages(c(\"tidyverse\", \"broom\", \"patchwork\", \"GGally\", \"caret\", \"plotly\"))\nOptional: more on Quarto: https://quarto.org/docs/guide/\nOptional: a base R cheatsheet: https://www.datacamp.com/cheat-sheet/getting-started-r",
    "crumbs": [
      "General Information"
    ]
  },
  {
    "objectID": "00-Recommended-Literature.html",
    "href": "00-Recommended-Literature.html",
    "title": "Recommended reading",
    "section": "",
    "text": "The R for data science (Wickham, Çetinkaya-Rundel, and Grolemund 2023) book covers data management and exploration using the tools from the tidyverse packages.\nDalgaard (2008): Introductory statistics with R. 2nd edition. New York: Springer. This textbook offers an introductory level to probability and basic linear models with a focus on the software (R) and with only a minimal discussion of the mathematical fundamentals. The code shown is base R and mostly differs from what we are using in the classes (tidyverse).\nSheather (2009): A Modern Approach to Regression with R. New York, NY: Springer New York (Springer Texts in Statistics). Available at: https://doi.org/10.1007/978-0-387-09608-7.\nThe Textbook focuses on linear regression models and develops the mathematical fundamentals alongside with examples using R.\nGelman, Hill, and Vehtari (2021): Regression and other stories. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press (Analytical methods for social research). Available at: https://doi.org/10.1017/9781139161879.\nAnother introduction into basic probability theory and linear models. The references to Bayesian inference in the book are not relevant to our course.\nFaraway (2015): Linear models with R. Second edition. Boca Raton: CRC Press, Taylor & Francis Group.\nAnother introduction into linear models with examples in R.\n\n\n\n\n\nDalgaard, Peter. 2008. Introductory Statistics with R. 2nd edition. Statistics and Computing. New York: Springer.\n\n\nFaraway, Julian James. 2015. Linear Models with R. Second edition. Chapman & Hall/CRC Texts in Statistical Science Series. Boca Raton: CRC Press, Taylor & Francis Group.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and Other Stories. Analytical Methods for Social Research. Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge University Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to Regression with R. Edited by George Casella, Stephen Fienberg, and Ingram Olkin. Springer Texts in Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly.",
    "crumbs": [
      "Recommended reading"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html",
    "href": "01-Introduction-to-R.html",
    "title": "1  Introduction to R",
    "section": "",
    "text": "1.1 Arithmetic Operations\n1 + 4\n\n[1] 5\n\n3 - 2\n\n[1] 1\n\n2 * 8\n\n[1] 16\n\n2 / 8\n\n[1] 0.25\n\n2^4\n\n[1] 16",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#assignment",
    "href": "01-Introduction-to-R.html#assignment",
    "title": "1  Introduction to R",
    "section": "1.2 Assignment",
    "text": "1.2 Assignment\nVery often we want to store a value in the memory of the computer so that we can reuse it later. In R we store values under names (variables) by using the assignment operator `&lt;-` Shortcut for the assignment operator: Alt - (minus)\n\ny &lt;- 34\ny - 40\n\n[1] -6\n\n\nRun this chunks and look at the global environment (right side of R Studio) to see it appear\nin the list of objects.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#numeric-vectors",
    "href": "01-Introduction-to-R.html#numeric-vectors",
    "title": "1  Introduction to R",
    "section": "1.3 Numeric Vectors",
    "text": "1.3 Numeric Vectors\nIt is very common to group values that belong together in a single structure. By default numeric vectors in R are created as double precision floating point numbers. You can create a numeric vector using the c (concatenate) function.\n\nx &lt;- c(1, 4)\nx\n\n[1] 1 4\n\n\nTo see the type of a variable, you can use the typeof function.\n\ntypeof(x)\n\n[1] \"double\"\n\n\n\n## Length, average, sum of a numeric vector\nmean(x)\n\n[1] 2.5\n\nsum(x)\n\n[1] 5\n\nlength(x)\n\n[1] 2\n\n\n\n## Documentation\n# ?mean\n\nVectors can only hold data of the same type: either numeric, character, or logical. If you try to create a vector with different types of data, R will coerce the data to the same type. For example, if you try to create a vector with a number and a string, R will coerce the number to a string.\n\nc(1, \"Hello\")\n\n[1] \"1\"     \"Hello\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#character-vectors",
    "href": "01-Introduction-to-R.html#character-vectors",
    "title": "1  Introduction to R",
    "section": "1.4 Character Vectors",
    "text": "1.4 Character Vectors\nYou can assign character vectors to variables as well. String literals are enclosed in quotes. It does not matter if you use single or double quotes, but you have to be consistent.\n\nz &lt;- \"Hello, world!\"\n\nThe c function can be used to create character vectors as well.\n\nz1 &lt;- c(\"Hello\", \"world!\")\nz1\n\n[1] \"Hello\"  \"world!\"\n\n\nStrings can be concatenated using the paste function.\n\npaste(z1, collapse = \",\")\n\n[1] \"Hello,world!\"\n\n\n\npaste(z1, \"some string\", sep = \" \")\n\n[1] \"Hello some string\"  \"world! some string\"\n\n\nR is different from other programming languages in that it is vectorized. This means that most functions are designed to work with vectors. For example, the paste function can take a vector of strings as input and return a vector of strings as output.\nUnlike other languages, using length on a variable holding a string will not return the number of characters in the string, but the number of elements in the vector. To count the number of characters in a string, you can use the nchar function.\n\nlength(z)\n\n[1] 1\n\n\n\nnchar(z)\n\n[1] 13\n\n\nAs a lot of other function, nchar is vectorized, meaning that it can take a vector of strings as input and return a vector of integers as output.\n\nnchar(c(\"Hi\", \"world!\", \"Some longer sentence\"))\n\n[1]  2  6 20",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#recycling",
    "href": "01-Introduction-to-R.html#recycling",
    "title": "1  Introduction to R",
    "section": "1.5 Recycling",
    "text": "1.5 Recycling\nIf you try to perform an operation on two vectors of different lengths, R will recycle the shorter vector to match the length of the longer vector. This is called recycling or broadcasting.\nLet’s run an example to see how recycling works. We want to add a scalar to each element of a vector. Mathematically, this does not make sense, because you can only add/subtract element-wise two vectors of the same length. However, R will recycle the scalar to match the length of the vector. Recycling means that it creates a new vector by repeating the shorter vector until it has the same length as the longer vector.\n\nc(2, 5) + 1\n\n[1] 3 6\n\n\n\nc(2, 3, 5, 7) + c(10, 20)\n\n[1] 12 23 15 27\n\nc(2, 3, 5, 7) / c(10, 20)\n\n[1] 0.20 0.15 0.50 0.35\n\n\nTake care when using recycling, because it can lead to unexpected results. For example, if you try to add two vectors of different lengths, R will still recycle the shorter vector to match the length of the longer vector, but it will also issue a warning.\n\nc(1, 2, 10) + c(2, 5)\n\nWarning in c(1, 2, 10) + c(2, 5): longer object length is not a multiple of\nshorter object length\n\n\n[1]  3  7 12\n\n\nPay attention to the warning message. It is telling you that the shorter vector is being recycled to match the length of the longer vector but it cannot expand the shorter vector to match the longer vector exactly. Although this is not an error that will stop your program, most of the time, this is not what you want and is a result from some error before it. You should not rely on recycling vectors of incompatible lengths. Instead, you should be explicit about what you want to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "href": "01-Introduction-to-R.html#logical-operators-and-logical-values",
    "title": "1  Introduction to R",
    "section": "1.6 Logical Operators and Logical Values",
    "text": "1.6 Logical Operators and Logical Values\nThere are two logical values: TRUE and FALSE. These emerge from logical operations and indicate whether some condition is fulfilled (TRUE) or not FALSE. You will find similar constructs in all other languages, where this type of data is commonly known as boolean or binary (i.e., only two values).\nThe basic logical operators in R are\n\n## Less than\n2 &lt; 5\n\n[1] TRUE\n\n## Less than or equal\n2 &lt;= 5\n\n[1] TRUE\n\n## Greater than\n2 &gt; 5\n\n[1] FALSE\n\n## Greater or equal\n2 &gt;= 5\n\n[1] FALSE\n\n## Exactly equal\n2 == 5\n\n[1] FALSE\n\n\"Text 2\" == \"Text 2\"\n\n[1] TRUE\n\n\n\nz == \"Text 2\"\n\n[1] FALSE\n\n\n\n\n\n\n\n\nStrict Equality and Floating Point Numbers\n\n\n\nStrict equality generally makes sense for strings and integers, but not for floating point numbers! This is because real numbers cannot be stored exactly in memory and computers work with finite precision. This can lead to unexpected results when comparing floating point numbers. For example, you may get a result like this:\n\nsqrt(2)^2 == 2\n\n[1] FALSE\n\n\nMathematically, \\sqrt(2)^2 is exactl yequal to 2, but the comparison in R returns FALSE.\nWhen printing the number in the console you may not see the difference because the print function formats the number. To compare floating point numbers, you should use the all.equal function, which takes into accounts for the finite precision of floating point numbers.\n\nall.equal(sqrt(2)^2, 2)\n\n[1] TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#indexing",
    "href": "01-Introduction-to-R.html#indexing",
    "title": "1  Introduction to R",
    "section": "1.7 Indexing",
    "text": "1.7 Indexing\nYou can access elements of a vector using the square brackets. The index of the first element is 1, not 0 (this is different from many other programming languages). You can also use negative indices to exclude elements from the vector.\n\nexpenses &lt;- c(100, 200, 300, 400, 500)\nexpenses[1]\n\n[1] 100\n\n\n\nexpenses[2:4]\n\n[1] 200 300 400\n\n\n\nexpenses[-1]\n\n[1] 200 300 400 500\n\n\nYou can also use logical vectors to index a vector.\n\nexpenses[c(TRUE, FALSE, TRUE, FALSE, TRUE)]\n\n[1] 100 300 500\n\n\nBe careful when using logical vectors to index a vector. If the logical vector is shorter than the vector you are indexing, R will recycle the logical vector to match the length of the vector you are indexing.\n\n(1:10)[c(TRUE, FALSE)]\n\n[1] 1 3 5 7 9",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#factors",
    "href": "01-Introduction-to-R.html#factors",
    "title": "1  Introduction to R",
    "section": "1.8 Factors",
    "text": "1.8 Factors\nFactors are used to represent categorical data (e.g., sex: male/femal, employment status: employed, unemployed, retired, etc.). They are stored as integers and have labels associated with them. Factors are important in statistical modeling and are commonly used in plotting functions. Factors are not strings, they are integers with labels.\n\nsome_vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"B\", \"A\")\nsome_factor &lt;- factor(some_vector)\nsome_factor\n\n[1] A B A C B A\nLevels: A B C\n\nlevels(some_factor)\n\n[1] \"A\" \"B\" \"C\"\n\n\nYou can coerce (convert) a factor to an integer vector using the as.integer function.\n\nas.integer(some_factor)\n\n[1] 1 2 1 3 2 1\n\n\nYou can coerce (convert) as factor to a character vector using the as.character function.\n\nas.character(some_factor)\n\n[1] \"A\" \"B\" \"A\" \"C\" \"B\" \"A\"\n\n\nFactors have also some safeguards. If you try to perform an operation that is not defined for factors, R will issue a warning. For example, you cannot meaningfully add a number to a factor. Note that this will only raise a warning, not an error.\n\nsome_factor + 1\n\nWarning in Ops.factor(some_factor, 1): '+' not meaningful for factors\n\n\n[1] NA NA NA NA NA NA\n\n\n\nsome_factor[1] &lt;- \"Some undefined level\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 1, value = \"Some undefined level\"): invalid\nfactor level, NA generated\n\nsome_factor\n\n[1] &lt;NA&gt; B    A    C    B    A   \nLevels: A B C\n\n\nRead more about factors in the R documentation, as well as the section on factors here",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#functions",
    "href": "01-Introduction-to-R.html#functions",
    "title": "1  Introduction to R",
    "section": "1.9 Functions",
    "text": "1.9 Functions\nFunctions are a fundamental building block most languages, including R. They are used to carry out a specific task and encapsulate a sequence of steps. You can avoid repeating the same code over and over again by abstracting the code into a function.\nYou can think of a function as a recipe. It takes some ingredients (arguments) and returns a dish (output).\nLet’s look at some examples of functions in R, without going into too much detail.\nTo define a function, you use the function keyword and assing the function to a variable. The function is then called by using the variable name and passing the arguments in parentheses. The function has a body that is enclosed in curly braces.\nLet’s write a function that takes two numbers and returns TRUE or FALSE depending on whether the sum of the arguments is odd. Functions return the value of the last expression in the body of the function unless you explicitly use the return keyword.\n\nis_even_sum &lt;- function(x, y) {\n  (x + y) %% 2 == 0\n}\n\nis_even_sum(2, 3)\n\n[1] FALSE\n\nis_even_sum(2, 4)\n\n[1] TRUE\n\n\nThe %% operator is the modulo operator. It returns the remainder of the division of the first number by the second number.\n\n2 %% 2\n\n[1] 0\n\n3 %% 2\n\n[1] 1\n\n\nWriting functions is a large topic and we will not cover all the details in this course. You can find more information about functions in R in the R documentation.\nA thing to note is that R is (mostly) a functional programming language and functions generally do not modify their arguments. This means that if you pass a variable to a function and the function modifies that variable, the original value will not be changed. This is different from many other languages, where functions can modify their arguments. The reason for this is that arguments are copied when they are passed to a function. This is done to avoid side effects and make the code easier to reason about.\n\nx &lt;- c(1, 2, 3)\n\nf &lt;- function(y) {\n  y[1] &lt;- 100\n  y\n}\n\nf(x)\n\n[1] 100   2   3\n\nx\n\n[1] 1 2 3\n\n\nThe function above assigns a value to the frst element of the vector y and returns the modified vector. However, the original vector x is not modified, because y is a copy of x.\n\ns1 &lt;- c(4, 1, 2, 3)\ns1\n\n[1] 4 1 2 3\n\ns1[1] &lt;- 100\ns1\n\n[1] 100   1   2   3",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "01-Introduction-to-R.html#data-frames",
    "href": "01-Introduction-to-R.html#data-frames",
    "title": "1  Introduction to R",
    "section": "1.10 Data Frames",
    "text": "1.10 Data Frames\nThe data for our course will most often be a table of values with columns representing measurements of different characteristics (variables) and rows representing different observations. The base data structure to store this kind of data in R is the data frame. In this course we will use a structure called tibble that is provided by the dplyr package, part of the tidyverse collection of packages.\nYou can create a data frame using the tibble function. The tibble function takes named arguments, where the names are the names of the columns and the values are the vectors that will be the columns of the data frame.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndt &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  employed = c(TRUE, FALSE, TRUE)\n)\n\ndt\n\n# A tibble: 3 × 3\n  name      age employed\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;   \n1 Alice      25 TRUE    \n2 Bob        30 FALSE   \n3 Charlie    35 TRUE",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html",
    "href": "02-Descriptive-Statistics.html",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "2.1 Data Frames (Tibbles)\nIn most of our work we will use data tables containing variables (columns) that describe characteristics of observations (rows). Most of the time we will use tibble objects to hold the data. tibble objects are a modern rewrite of the data.frame (an older object type for storing data).\nTo use it we need to load the dplyr package, a part of the tidyverse collection of packages.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nIn a limited number of cases we need to construct tables by hand. You can find out more about tibble here.\ndt &lt;- tibble(\n  ## Shorthand syntax for creating a sequence of integers from one to five\n  id = 1:5,\n  y = c(2, 2.5, 3, 8, 12)\n)\ndt\n\n# A tibble: 5 × 2\n     id     y\n  &lt;int&gt; &lt;dbl&gt;\n1     1   2  \n2     2   2.5\n3     3   3  \n4     4   8  \n5     5  12\nMost of our data will come from external sources such as text files in a csv format. For the purpose of this course you don’t need to worry about reading these files, you will always have a starter code chunk that imports the data.\nThe earnings data set contains data on 1816 customers of a shopping mall. The customers have answered a short interview and gave information about their sex, age, ethnicity, annual income, weight and height.\nWe will use this data set to demonstrate some common operations and basic data summaries.\nearnings &lt;- read_csv(\"https://raw.githubusercontent.com/feb-uni-sofia/econometrics2021/main/data/earnings.csv\")\n\nRows: 1816 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): ethnicity\ndbl (14): height, weight, male, earn, earnk, education, mother_education, fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nFirst we will convert the height and weight measurements from their original scales (inch, pound) to cm and kg. We will create two new columns with informative names using the mutate function.\nearnings &lt;- mutate(\n  earnings,\n  height_cm = 2.54 * height,\n  weight_kg = 0.45 * weight\n)\nearnings1 &lt;- select(earnings, height_cm, weight_kg)\nThe same code can be rewritten in a more convenient way using pipes.\nearnings1 &lt;- earnings %&gt;%\n  mutate(\n    height_cm = 2.54 * height,\n    weight_kg = 0.45 * weight\n  ) %&gt;%\n  select(height_cm, weight_kg)\nNote that the object holding the original data is unaffected by mutate and select. The reason for this is that functions in R generally do not change their arguments. If you want to add the two new columns to the original data set earnings, you need to overwrite it with an assignment.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "href": "02-Descriptive-Statistics.html#data-frames-tibbles",
    "title": "2  Descriptive Statistics",
    "section": "",
    "text": "height (numeric): Height in inches (1 inch = 2.54 cm)\nweight (numeric): Weight in pounds (1 pound \\approx 0.45 kilograms)\nmale (numeric): 1: Male, 0: Female\nearn (numeric): Annual income in USD\nearnk (numeric): Annual income in 1,000 USD\nethnicity (character): Ethnicity\nage (numeric): Age",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#basic-data-summaries",
    "href": "02-Descriptive-Statistics.html#basic-data-summaries",
    "title": "2  Descriptive Statistics",
    "section": "2.2 Basic data summaries",
    "text": "2.2 Basic data summaries\nThe first step in any data analysis is to gain an initial understanding of the context of the data and the distributions of the variables of interest. In this course our main focus will be on two features of the variables: their location and their variability (how different are the observations between each other).\n\n2.2.1 Location\nThe most important measure of location for us will be the empirical mean of a variable (arithmetic average). Let i index the observation in our data set from the first (i = 1) to the last i = n. In our case n = 1816: the number of all interviewed customers. We can represent the values of some (numeric) characteristic (e.g., the persons’ weight) as a vector of values x = (x_1, \\ldots, x_n). In this notation x_1 is the weight of the first customer in the data set (x_1 = 210 pounds). The arithmetic average is defined as the sum of all values divided by the number of observations:\n\n\\bar{x} = \\frac{1}{n}(x_1 + x_2 + \\ldots + x_n) = \\frac{1}{n}\\sum_{i = 1}^{n} x_i\n\nLet us now compute the arithmetic average of weight and height. One way to access the columns of the data set earnings is to write the name of the data set and then after a $ sign the name of the column.\n\nmean(earnings$height)\n\n[1] 66.56883\n\nmean(earnings$weight, na.rm = TRUE)\n\n[1] 156.3052\n\n\nAnother measure of location is the (empirical) median. You can compute it using the median function.\n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe result is a median height of 66 inches. This means that about half of the customers were taller than 66 inches.\n\n\n2.2.2 Variability (Spread)\nThe next important feature of the data is its variability. It answers the following question: how different are the customers between each other with respect to body height (for example). There are numerous ways to measure variability.\nOne intuitive measure would be the range of the data, defined as the difference by the maximal observed height and the minimal observed height\n\nmin(earnings$height)\n\n[1] 57\n\nmax(earnings$height)\n\n[1] 82\n\nrange(earnings$height)\n\n[1] 57 82\n\n\n\nmax(earnings$height) - min(earnings$height)\n\n[1] 25\n\n\nAnother measure is the inter-quartile range. The quartiles are defined similar to the median. To see this lets use the example of body height. The first quartile of height (25-th percentile and 0.25 quantile are different names for the same thing) is the height for which about one quarter of the customers are shorter than it. You can compute it with the function quantile.\n\nquantile(earnings$height, 0.25)\n\n25% \n 64 \n\n\nAbout 25 percent of our customers were shorter than 64 inches.\nThe second quartile is the same as the median (two quarters).\n\nquantile(earnings$height, 0.5)\n\n50% \n 66 \n\nmedian(earnings$height)\n\n[1] 66\n\n\nThe third quartile is the height for which three quarter of the customers are shorter than it.\n\nquantile(earnings$height, 0.75)\n\n  75% \n69.25 \n\n\nIn our case about three quarter of the customers were shorter than 69.25 inches.\nThe inter-quartile range is simply the difference between the third quartile and the second quartile.\n\nquantile(earnings$height, 0.75) - quantile(earnings$height, 0.25)\n\n 75% \n5.25 \n\n\nAbout half of our customers had a hight between the first quartile (64 inches) and the third quartile (69.25 inches). The inter-quartile range shows you the difference between the height of the talles person and the shortest person for the central 50 percent of the customers.\nAs the range, the inter-quartile range is a measure of variability.\nThe most important measure of variability and the one that will be central to our analysis is the (empirical) variance.\n\nDefinition 2.1 (Empirical Variance) For a vector of values x = (x_1, \\ldots, x_n) it is defined as the average (apart from a small correction in the denominator) squared deviation of the values from their mean.\n\nS^2_x = \\frac{(x_1 - \\bar{x})^2 + \\ldots + (x_n - \\bar{x})^2}{n - 1} = \\frac{1}{n - 1} \\sum_{i = 1}^{n}(x_i - \\bar{x})^2: \\quad \\text{variance}\\\\\nS_x = \\sqrt{S^2_x} \\quad \\text{standard deviation}\n\n\n\nExample 2.1 (Computing the empirical variance) Lets apply the formula from Definition 2.1 to a very simple example with just three values.\n\nx_1 = -1, x_2 = 0, x_3 = 4\n\nFirst, the empirical mean of these values is\n\n\\bar{x} = \\frac{-1 + 0 + 4}{3} = 1\n\nNow lets substitute these values in the definition of the empirical variance:\n\n\\begin{aligned}\nS^2_{x} & = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_3 - \\bar{x})^2 }{n - 1} \\\\\n        & = \\frac{(-1 - 1)^2 + (0 - 1)^2 + (4 - 1)^2 }{3 - 1} \\\\\n        & = \\frac{(-2)^2 + (- 1)^2 + (3 )^2 }{2} \\\\\n        & = \\frac{4 + 1 + 9 }{2} \\\\\n        & = \\frac{14}{2} \\\\\n        & = 7\n\\end{aligned}\n\nUsing R to compute the same thing:\n\n# Create a vector called x\nx &lt;- c(-1, 0, 4)\n# Compute the average of the values in x and store it in a variable called x_avg (look it up in the global environment)\n\nx_avg &lt;- mean(x)\n\n# Manually compute the variance of x\n((-1 - x_avg)^2 + (0 - x_avg)^2 + (4 - x_avg)^2) / (length(x) - 1)\n\n[1] 7\n\n\nThere is also a special function var that can compute it from a vector\n\nvar(x)\n\n[1] 7\n\n\nThe (empirical) standard deviation is simply the square root of the (empirical) variance.\n\nS_x = \\sqrt{S^2_x} = \\sqrt{7} \\approx 2.64\n\nIn R you have two options: take the square root of the result of var using the sqrt function or use sd (standard deviation) to compute the standard deviation directly.\n\nsqrt(var(x))\n\n[1] 2.645751\n\nsd(x)\n\n[1] 2.645751\n\n\n\n\nExercise 2.1 (Empirical Moments (Mean and Variance))  \n\nCompute the sample mean and variance of the annual earnings of the customers in the earnings data set. First, compute it by accessing the columns of the data using the dollar syntax.\n\n\nThen use the summarize function from the dplyr (already loaded) package to compute the mean and the variance of the earn variable.\n\n\nRun the same calculations as before, but this time group the dataset by the ethnicity variable. This will give you the mean and the variance of the annual earnings for every ethnic group in the data set. Use the group_by function.\n\n\n\n\n2.2.3 Overview of the data\nThe skim function from the skimr package provides a quick overview of the data set. It shows the number of observations, the number of variables, the names of the variables, the type of the variables, the number of missing values, the mean, the standard deviation, the minimum and maximum values, and the quartiles for the numeric variables.\n\n## Basic summaries for the whole tibble\nearnings %&gt;% skimr::skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n1816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n16\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nethnicity\n0\n1\n5\n8\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nheight\n0\n1.00\n66.57\n3.83\n57.00\n64.00\n66.00\n69.25\n82.00\n▂▇▅▁▁\n\n\nweight\n27\n0.99\n156.31\n34.62\n80.00\n130.00\n150.00\n180.00\n342.00\n▅▇▃▁▁\n\n\nmale\n0\n1.00\n0.37\n0.48\n0.00\n0.00\n0.00\n1.00\n1.00\n▇▁▁▁▅\n\n\nearn\n0\n1.00\n21147.30\n22531.77\n0.00\n6000.00\n16000.00\n27000.00\n400000.00\n▇▁▁▁▁\n\n\nearnk\n0\n1.00\n21.15\n22.53\n0.00\n6.00\n16.00\n27.00\n400.00\n▇▁▁▁▁\n\n\neducation\n2\n1.00\n13.24\n2.56\n2.00\n12.00\n12.00\n15.00\n18.00\n▁▁▁▇▃\n\n\nmother_education\n244\n0.87\n13.61\n3.22\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nfather_education\n295\n0.84\n13.65\n3.25\n3.00\n12.00\n13.00\n16.00\n99.00\n▇▁▁▁▁\n\n\nwalk\n0\n1.00\n5.30\n2.60\n1.00\n3.00\n6.00\n8.00\n8.00\n▃▁▃▁▇\n\n\nexercise\n0\n1.00\n3.05\n2.32\n1.00\n1.00\n2.00\n5.00\n7.00\n▇▁▁▁▃\n\n\nsmokenow\n1\n1.00\n1.75\n0.44\n1.00\n1.00\n2.00\n2.00\n2.00\n▃▁▁▁▇\n\n\ntense\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nangry\n1\n1.00\n1.42\n2.16\n0.00\n0.00\n0.00\n2.00\n7.00\n▇▁▁▁▁\n\n\nage\n0\n1.00\n42.93\n17.16\n18.00\n29.00\n39.00\n56.00\n91.00\n▇▇▃▃▁\n\n\nheight_cm\n0\n1.00\n169.08\n9.73\n144.78\n162.56\n167.64\n175.89\n208.28\n▂▇▅▁▁\n\n\nweight_kg\n27\n0.99\n70.34\n15.58\n36.00\n58.50\n67.50\n81.00\n153.90\n▅▇▃▁▁\n\n\n\n\n\n\n\n2.2.4 Summarizing a single categorical variable\nWhile the mean, median, and variance are useful for numeric variables, they are not defined for categorical variables. Instead, we can use the table function to count the number of observations in each category.\n\ntable(earnings$ethnicity)\n\n\n   Black Hispanic    Other    White \n     180      104       38     1494",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "02-Descriptive-Statistics.html#visualizations",
    "href": "02-Descriptive-Statistics.html#visualizations",
    "title": "2  Descriptive Statistics",
    "section": "2.3 Visualizations",
    "text": "2.3 Visualizations\nHistogram\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nA smooth density plot is an alternative way to visualize the distribution of a variable.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_density()\n\n\n\n\n\n\n\n\nThe boxplot shows the median and the 25-th and 75-th percentiles (the box). The whiskers in the plot stretch to the minimum or the maximum observed value, unless there are extreme observations that are shown as single dots.\n\nearnings %&gt;%\n  ggplot(aes(x = height)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nGroup comparisons\n\nearnings %&gt;%\n  ggplot(aes(x = height, y = ethnicity)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe scatterplot will be our primary tool in studying associations between variables. It represents each observation as a point in a coordinate system defined by the variables that we would like to study.\n\nearnings1 %&gt;%\n  ggplot(aes(x = weight_kg, y = height_cm)) +\n  geom_point(position = \"jitter\", alpha = 0.5) +\n  labs(\n    x = \"Weight (kg)\",\n    y = \"Height (cm)\"\n  )\n\nWarning: Removed 27 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create a scatterplot\nfig &lt;- plot_ly(earnings1, x = ~weight_kg, y = ~height_cm, mode = 'markers')\nfig\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nWarning: Ignoring 27 observations\n\n\n\n\n\n\n\nsummary(lm(height_cm ~ weight_kg, data = earnings1))\n\n\nCall:\nlm(formula = height_cm ~ weight_kg, data = earnings1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.639  -5.611  -0.084   5.645  36.248 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 145.00916    0.89138  162.68   &lt;2e-16 ***\nweight_kg     0.34314    0.01237   27.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.15 on 1787 degrees of freedom\n  (27 observations deleted due to missingness)\nMultiple R-squared:  0.3009,    Adjusted R-squared:  0.3005 \nF-statistic: 769.1 on 1 and 1787 DF,  p-value: &lt; 2.2e-16\n\n\n\nExercise 2.2  \n\nCreate a new variable bmi (body mass index) in the earnings data set. The BMI is defined as the weight in kilograms divided by the square of the height in meters. The height in meters is the height in centimeters divided by 100.\n\n\n# Select the following code and uncomment it by pressing Ctrl+Shift+C\n\n# earnings &lt;- earnings # YOUR CODE HERE\n#   mutate(\n#     bmi = # YOUR CODE HERE\n#   )\n\n\nThe reference ranges for the BMI are as follows:\n\n\nUnder 18,5: Underweight\n18,5 - 24,9: Normal\n25 - 29,9: Overweight\n30 oder mehr: Obese\n\nCreate a two new variables is_underweight, is_normal in the dataset. Hint: Use the mutate function and the &lt;, &lt;=, &gt;, &gt;= operators. You can join multiple conditions using the & (logical and) operator. Don’t forget to uncommend the code first (Ctrl+Shift+C). How many customers are underweight and how many have a normal weight? Use the summarize function to compute the number of customers in each category.\n\n# earnings &lt;- earnings # YOUR CODE HERE\n#   mutate(\n#     is_underweight = # YOUR CODE HERE\n#     is_normal = # YOUR CODE HERE\n#   )\n\n# earnings %&gt;%\n#   summarize(\n#     n_underweight = # YOUR CODE HERE\n#     n_normal = # YOUR CODE HERE\n#   )\n\n\n\nGroup the data by male and compute the number of customers in each group, as well as the number of underweight and normal weight customers in each group. Additionally, compute the share of underweight and normal weight customers in each group. Hint: Use the group_by function and the summarize function. Don’t forget to uncommend the code first (Ctrl+Shift+C). The n() function can be used to compute the number of observations in each group.\n\n\n# earnings %&gt;%\n#   group_by(# YOUR CODE HERE) %&gt;%\n#   summarize(\n#     n = n(),\n#     n_underweight = # YOUR CODE HERE\n#     n_normal = # YOUR CODE HERE\n#     share_underweight = # YOUR CODE HERE\n#     share_normal = # YOUR CODE HERE\n#   )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html",
    "href": "04-Least-Squares.html",
    "title": "3  Least Squares",
    "section": "",
    "text": "3.1 Least Squares\nThe previous discussion leads us to a question. Can we find values for the coefficients in the linear equation that minimize the MSE? The answer is yes. The method is called least squares.\nFirst, let us visualize the MSE as a function of the coefficients. For this purpose, we will simply calculate the MSE for a grid of values of the coefficients and plot the results (Figure 3.3)\n# NOTE: this code here is only for illustration purposes, you don't need to study it or understand it for the course\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n# Create a grid of values for beta0_hat and beta1_hat\n\nbeta0_hat &lt;- seq(0.4, 1, length.out = 50)\nbeta1_hat &lt;- seq(0.001, 0.015, length.out = 50)\n\ndt &lt;- expand.grid(beta0_hat = beta0_hat, beta1_hat = beta1_hat) %&gt;%\n  mutate(\n    # Compute the RSS for each combination of beta0_hat and beta1_hat\n    RSS = map2_dbl(beta0_hat, beta1_hat, ~{\n      Time_hat &lt;- .x + .y * invoices$Invoices\n      sum((invoices$Time - Time_hat)^2)\n    })\n  )\n\nfig &lt;- plot_ly(\n  x=beta0_hat, \n  y=beta1_hat, \n  z = matrix(dt$RSS, nrow = length(beta0_hat), ncol = length(beta1_hat)),\n  ) %&gt;% \n    add_surface() %&gt;% \n    layout(\n        scene = list(\n          xaxis = list(title = \"beta1_hat\"),\n          yaxis = list(title = \"beta0_hat\"),\n          zaxis = list(title = \"RSS\")\n        )\n    )\n\nfig\n\n\n\n\n\n\n\nFigure 3.3: MSE as a function of the coefficients\nOur goal is to find the values of \\beta_0 and \\beta_1 that minimize the RSS. The method of least squares provides a formula for the coefficients that minimize the RSS.\nFirst, let us solve a simpler problem. Assume that the predictive equation is\n\\widehat{\\text{Time}}_i = \\hat{\\beta}_0\nThe MSE in this case is much simpler.\ninvoices_beta_0_dt &lt;- expand_grid(\n    beta0_hat = seq(0, 4, length.out = 100),\n    invoices\n)\n\nrss_dt &lt;- invoices_beta_0_dt %&gt;%\n  group_by(beta0_hat) %&gt;%\n  summarise(\n    RSS = sum((Time - beta0_hat)^2)\n  )\n\nrss_dt %&gt;%\n    ggplot(aes(x = beta0_hat, y = RSS)) +\n    geom_line()\n\n\n\n\n\n\n\nFigure 3.4: MSE as a function of beta0_hat in an intercept-only equation\nHow do we find the value of \\hat{\\beta}_0 that minimizes the MSE? We take the derivative of the MSE with respect to \\hat{\\beta}_0 and set it to zero.\nIn order to find the minimum of the MSE, we take the derivative of the MSE with respect to \\hat{\\beta}_0 and set it to zero. Instead of \\text{Time} we will use the shorter notation y_i and instead of \\hat{\\text{Time}}_i we will use the shorter notation \\hat{y}_i.\n\\begin{align*}\nRSS(\\hat{\\beta}_0) & = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n& = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0)^2\n\\end{align*}\nTry to find the value of \\hat{\\beta}_0 that minimizes the MSE. Hint: Take the derivative of the RSS with respect to \\hat{\\beta}_0, set it to zero and solve for \\hat{\\beta}_0.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html#least-squares",
    "href": "04-Least-Squares.html#least-squares",
    "title": "3  Least Squares",
    "section": "",
    "text": "Solution for the intercept-only equation\n\n\n\n\n\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} RSS(\\hat{\\beta}_0) & =\n\\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0) \\cdot (-1) \\\\\n& = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0) \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 \\sum_{i=1}^n \\hat{\\beta}_0 \\\\\n& = -2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0\n\\end{align*}\n\nSetting the derivative to zero, we get\n\n\\begin{align*}\n-2 \\sum_{i=1}^n y_i + 2 n \\hat{\\beta}_0 & = 0 \\\\\n\\hat{\\beta}_0 & = \\frac{1}{n} \\sum_{i=1}^n y_i \\\\\n& = \\overline{y}\n\\end{align*}\n\nThe value of \\hat{\\beta}_0 that minimizes the MSE is thus just the average of the observed values of y_i.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html#the-one-variable-case",
    "href": "04-Least-Squares.html#the-one-variable-case",
    "title": "3  Least Squares",
    "section": "3.2 The one variable case",
    "text": "3.2 The one variable case\nNow let us consider the case where we have two variables, \\text{Invoices}_i and \\text{Time}_i. We want to find the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that minimize the RSS.\nThe prediction equation is\n\n\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\nThe RSS is (as above)\n\n\\begin{align*}\n\\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n  & = \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n\\end{align*}\n\nThe first order conditions for the minimum are\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\hat{\\beta}_0} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) = 0 \\\\\n\\frac{\\partial}{\\partial \\hat{\\beta}_1} \\text{RSS}(\\hat{\\beta}_0, \\hat{\\beta}_1) & = -2 \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i) x_i = 0\n\\end{align*}\n\nThe first equation gives\n\n\\begin{align*}\n\\sum_{i=1}^n y_i - \\hat{\\beta}_0 n - \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = 0 \\\\\n\\hat{\\beta}_0 n + \\hat{\\beta}_1 \\sum_{i=1}^n x_i & = \\sum_{i=1}^n y_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nThe second equation gives\n\n\\begin{align*}\n\\sum_{i=1}^n y_i x_i - \\hat{\\beta}_0 \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = 0 \\\\\n\\hat{\\beta}_0 \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nSubstituting the expression for \\hat{\\beta}_0 in the second equation, we get\n\n\\begin{align*}\n(\\overline{y} - \\hat{\\beta}_1 \\overline{x}) \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\overline{y} \\sum_{i=1}^n x_i - \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i \\\\\n\\hat{\\beta}_1 \\sum_{i=1}^n x_i^2 & = \\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i + \\hat{\\beta}_1 \\overline{x} \\sum_{i=1}^n x_i \\\\\n\\hat{\\beta}_1 & = \\frac{\\sum_{i=1}^n y_i x_i - \\overline{y} \\sum_{i=1}^n x_i}{\\sum_{i=1}^n x_i^2 - \\overline{x} \\sum_{i=1}^n x_i}\n\\end{align*}\n\nSimplifying the expression, we get\n\n\\begin{align*}\n\\hat{\\beta}_1 & = \\frac{\\overline{x y} - \\overline{x} \\cdot \\overline{y}}{\\overline{x^2} - \\overline{x}^2} \\\\\n\\hat{\\beta}_0 & = \\overline{y} - \\hat{\\beta}_1 \\overline{x}\n\\end{align*}\n\nThe last expression may seem a bit complicated, but it is actually quite simple. It is just the ratio between the empirical covariance between x_i and y_i divided by the variance of x_i.\nThe empirical covariance between x_i and y_i is defined as the sum of the products of the deviations of x_i and y_i from their respective means, divided by the number of observations.\n\nDefinition 3.1 (Covariance) The covariance between two variables x and y with n values is defined as\n\nS_{xy} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n\n\n\nDefinition 3.2 (Variance decomposition) We have already defined the variance of a variable x as\n\nS_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})^2\n\nIt can be shown that the variance of x can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.\n\nS_x^2 = \\frac{1}{n  - 1}(\\overline{x_i^2} - \\overline{x}^2)\n\n\n\nProof. The proof is straightforward. We have\n\n\\begin{align*}\n(n - 1) S_x^2 & =  \\sum_{i=1}^n (x_i - \\overline{x})^2 \\\\\n& =  \\sum_{i=1}^n (x_i^2 - 2x_i \\overline{x} + \\overline{x}^2) \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 \\sum_{i=1}^n 1 \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x} \\sum_{i=1}^n x_i + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - 2\\overline{x}^2 n + \\overline{x}^2 n \\\\\n& =  \\sum_{i=1}^n x_i^2 - n \\overline{x}^2 \\\\\n& = n (\\overline{x_i^2} - \\overline{x}^2)\n\\end{align*}\n\n\n\nDefinition 3.3 (Covariance and Covariance decomposition) The covariance between two variables x and y with n values is defined as\n\nS_{xy} = \\frac{1}{n - 1} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\n\nMuch in the same way as the variance, the covariance can be decomposed into the sum of the squared mean and the variance of the deviations from the mean.\n\n(n - 1) S_{xy} = n(\\overline{x y} - \\overline{x} \\overline{y})\n\nThe proof is similar to the proof for the variance decomposition.\n\n\nProof. The proof is straightforward. We have\n\n\\begin{align*}\n(n - 1) S_{xy} & = \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) \\\\\n& = \\sum_{i=1}^n x_i y_i - \\overline{x} \\sum_{i=1}^n y_i - \\overline{y} \\sum_{i=1}^n x_i + n \\overline{x} \\overline{y} \\\\\n& = n(\\overline{x y} - \\overline{x} \\overline{y})\n\\end{align*}\n\n\nTo understand what the covariance measures, consider the following scatterplot:\n\nset.seed(123)\ndt &lt;- tibble(\n    x = rnorm(100),\n    y = 2 * x + rnorm(100)\n)\n\ndt %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_vline(xintercept = mean(dt$x), colour = \"firebrick4\") +\n    geom_hline(yintercept = mean(dt$y), colour = \"steelblue4\")\n\n\n\n\n\n\n\nFigure 3.5: Scatterplot of two variables with a positive linear assocition\n\n\n\n\n\nThe reddish line is drawn at the average of the x values and the bluish line is drawn at the average of the y values. The covariance measures the average product of the deviations of the x and y values from their respective means. If the product is positive, it means that when x is above its average, y is also above its average. If the product is negative, it means that when x is above its average, y is below its average.\nYou can compute the empirical covariance between two variables using the cov function in R.\n\ncov(dt$x, dt$y)\n\n[1] 1.622745\n\n\nOnly the sign of the covariance is important. The magnitude of the covariance depends on the units of the variables. To make the covariance unit-free, we can divide it by the product of the standard deviations of the two variables. This gives us the correlation coefficient.\n\ncov(dt$x * 1000, dt$y)\n\n[1] 1622.745\n\ncov(dt$x , dt$y * 50)\n\n[1] 81.13723\n\n\n\nset.seed(123)\ndt1 &lt;- tibble(\n    x = rnorm(100),\n    y = -3 * x + rnorm(100)\n)\n\ndt1 %&gt;%\n    ggplot(aes(x = x, y = y)) +\n    geom_point() +\n    geom_vline(xintercept = mean(dt1$x), colour = \"firebrick4\") +\n    geom_hline(yintercept = mean(dt1$y), colour = \"steelblue4\")\n\n\n\n\n\n\n\nFigure 3.6: Scatterplot of two variables with a positive linear assocition\n\n\n\n\n\n\ncov(dt1$x, dt1$y)\n\n[1] -2.54342\n\n\n\nDefinition 3.4 (Correlation) The correlation between two variables x and y with n values is defined as\n\nr_{xy} = \\frac{S_{xy}}{S_x S_y}\n\nwhere S_{xy} is the covariance between x and y, and S_x and S_y are the standard deviations of x and y respectively.\n\nBecause the covariance is divided by the product of the standard deviations, the correlation is unit-free. Furthermore, the correlation is always between -1 and 1. A correlation of 1 means that the two variables lie on a straight line with a positive slope. A correlation of -1 means that the two variables lie on a straight line with a negative slope. A correlation of 0 means that there is no linear association between the two variables.\n\ncor(dt$x, dt$y)\n\n[1] 0.8786993\n\ncor(dt1$x, dt1$y)\n\n[1] -0.9448502",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html#exercise",
    "href": "04-Least-Squares.html#exercise",
    "title": "3  Least Squares",
    "section": "3.3 Exercise",
    "text": "3.3 Exercise\nWrite a function that takes two vectors x and y as arguments and returns the OLS coefficients \\hat{\\beta}_0 and \\hat{\\beta}_1 using the formulas above.\n\nols_two_variables &lt;- function(x, y){\n    # Compute the coefficients\n    beta_1_hat &lt;- \n    beta_0_hat &lt;- \n\n    list(beta_0_hat = beta_0_hat, beta_1_hat = beta_1_hat)\n}\n\nTest your function using the following data set.\n\nset.seed(123)\n\nx_test &lt;- rnorm(100)\ny_test &lt;- 1 + 2 * x_test + rnorm(100)\n\nCompare your results with the lm function in R.\n\n# Uncomment the following line to compare your results with the lm function\n\n# ols_two_variables(x, y)\nlm(y_test ~ x_test)\n\n\nCall:\nlm(formula = y_test ~ x_test)\n\nCoefficients:\n(Intercept)       x_test  \n     0.8972       1.9475",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html#ols-using-the-lm-function-in-r",
    "href": "04-Least-Squares.html#ols-using-the-lm-function-in-r",
    "title": "3  Least Squares",
    "section": "3.4 OLS using the lm function in R",
    "text": "3.4 OLS using the lm function in R\nOur primary tool for calculating the least squares coefficients will be the lm function in R. The lm function takes a formula as its first argument and a data set as its second argument. The formula is of the form y ~ x where y is the dependent variable and x is the independent variable. The data set is a data frame/tibble with the variables x and y.\nUse the lm function to compute the OLS coefficients for the invoices data set. and the model:\n\n\\widehat{\\text{Time}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\text{Invoices}\n\n\n# Uncomment the following two lines to fit the model\n\n# fit_OLS &lt;- lm(WRITE THE FORMULA HERE, data = invoices)\n# fit_OLS\n\n\n# invoices &lt;- invoices %&gt;%\n#     mutate(\n#       Time_hat_OLS = predict(fit_OLS),\n#       residuals_OLS = # WRITE YOUR CODE HERE\n#     )\n\n# Compute the RSS for the OLS model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html#plot-the-ols-predictions",
    "href": "04-Least-Squares.html#plot-the-ols-predictions",
    "title": "3  Least Squares",
    "section": "3.5 Plot the OLS predictions",
    "text": "3.5 Plot the OLS predictions\n\n# Plot the predictions\n\n# invoices %&gt;%\n#     ggplot(aes(x = Invoices, y = Time)) +\n#     geom_point() +\n#     geom_line(aes(y = Time_hat_OLS), colour = \"steelblue4\")\n\n\n# Plot the predictions using geom_smooth\n\n# invoices %&gt;%\n#     ggplot(aes(x = Invoices, y = Time)) +\n#     geom_point() +\n#     geom_smooth(method = \"lm\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html#least-squares-in-matrix-notation",
    "href": "04-Least-Squares.html#least-squares-in-matrix-notation",
    "title": "3  Least Squares",
    "section": "3.6 Least Squares in Matrix Notation",
    "text": "3.6 Least Squares in Matrix Notation\nThe previous discussion can be summarized in matrix notation and you will commonly find it in this form in textbooks and articles.\nFor the one variable case, the prediction equation, written for all observations is\n\n\\begin{align*}\n\\hat{y}_1 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 \\\\\n\\hat{y}_2 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_2 \\\\\n\\vdots & \\\\\n\\hat{y}_n & = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_n \\\\\n\\end{align*}\n\nThis can be written in matrix notation as\n\n\\underbrace{\\begin{bmatrix}\n\\hat{y}_1 \\\\\n\\hat{y}_2 \\\\\n\\vdots \\\\\n\\hat{y}_n\n\\end{bmatrix}\n}_{\\hat{y}_{n \\times 1}}\n=\n\\underbrace{\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix}}_{X_{n \\times 2}}\n\\underbrace{\\begin{bmatrix}\n\\hat{\\beta}_0 \\\\\n\\hat{\\beta}_1\n\\end{bmatrix}}_{\\hat{\\beta}_{2 \\times 1}}\n\n\n\\hat{y} = X \\hat{\\beta}\n\nYou can think about the least squares as finding a solution to a system of linear equations where the number of equations is greater than the number of unknowns. Let us focus on a case with one variable and two observations (the first two from the invoices data set).\n\ninvoices %&gt;%\n head(n = 2)\n\n  Day Invoices Time\n1   1      149  2.1\n2   2       60  1.8\n\n\n\n2.1 = \\hat{\\beta}_0 + \\hat{\\beta}_1 149 \\\\\n1.8 = \\hat{\\beta}_0 + \\hat{\\beta}_1 60\n\nIn this system of equations you have two unknowns, \\hat{\\beta}_0 and \\hat{\\beta}_1, and two equations. Therefore, you can actually solve for the unknowns (left as an exercise).\n\nM &lt;- matrix(c(1, 149, 1, 60), ncol = 2, byrow = TRUE)\ny &lt;- c(2.1, 1.8)\nsolve(M, y)\n\n[1] 1.597752809 0.003370787\n\n\nYou will find that the solution is \n\\begin{align*}\n\\hat{\\beta}_0 \\approx 1.598 \\\\\n\\hat{\\beta}_1 \\approx 0.0034\n\\end{align*}\n\nLet’s look at the first three observations in the invoices data set.\n\ninvoices %&gt;%\n head(n = 3)\n\n  Day Invoices Time\n1   1      149  2.1\n2   2       60  1.8\n3   3      188  2.3\n\n\nThe equations for the first three observations are\n\n\\begin{align*}\n2.1 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 149 \\\\\n1.8 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 60 \\\\\n2.2 & = \\hat{\\beta}_0 + \\hat{\\beta}_1 201\n\\end{align*}\n\nSubstituting the solution for \\hat{\\beta}_0 and \\hat{\\beta}_1 that we obtained from the first two equations into the third equation, we get\n\n1.597752809 + 0.003370787 * 201\n\n[1] 2.275281\n\n\nwhich is not exactly equal to 2.2, therefore the third equation is not satisfied.\nIn the general case we cannot hope to solve the system of equations exactly. What we could hope for is to find an approximate solution which is close to the actual left hand side values. The least squares method provides such a solution by defining a sense of closeness between the approximate solution and the actual left hand side values (data) and then finding the approximate solution that minimizes that closeness measure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "04-Least-Squares.html#the-geometry-of-least-squares",
    "href": "04-Least-Squares.html#the-geometry-of-least-squares",
    "title": "3  Least Squares",
    "section": "3.7 The Geometry of Least Squares",
    "text": "3.7 The Geometry of Least Squares\nIn the previous section we derived the formula using calculus and an objective function (the RSS) that we wanted to minimize. In this section we will derive the formula using a geometric approach.\nFirst, let us think about the data in terms of vectors in a \\mathbb{R}^n dimensional space (where n is the number of observations). For an easier visualization, let us consider the case n = 2 (only two observations) so that we can easily plot the data.\n\n\nCode\nif (!require(\"ggforce\")) {\n  install.packages(\"ggforce\")\n}\n\n\n# Downloading packages -------------------------------------------------------\n- Downloading ggforce from CRAN ...             OK [1.8 Mb in 0.37s]\n- Downloading tweenr from CRAN ...              OK [449 Kb in 0.31s]\n- Downloading polyclip from CRAN ...            OK [115 Kb in 0.3s]\n- Downloading RcppEigen from CRAN ...           OK [1.8 Mb in 0.33s]\nSuccessfully downloaded 4 packages in 2.7 seconds.\n\nThe following package(s) will be installed:\n- ggforce   [0.4.2]\n- polyclip  [1.10-6]\n- RcppEigen [0.3.4.0.0]\n- tweenr    [2.0.3]\nThese packages will be installed into \"~/work/econ2024/econ2024/renv/library/R-4.3/x86_64-pc-linux-gnu\".\n\n# Installing packages --------------------------------------------------------\n- Installing tweenr ...                         OK [installed binary and cached in 0.58s]\n- Installing polyclip ...                       OK [installed binary and cached in 0.41s]\n- Installing RcppEigen ...                      OK [installed binary and cached in 0.72s]\n- Installing ggforce ...                        OK [installed binary and cached in 1.0s]\nSuccessfully installed 4 packages in 2.9 seconds.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggforce)\n\nX &lt;- c(2, 3) / 4\nY &lt;- c(1, 0.2)\n\nY_proj &lt;- X %*% Y / X %*% X * X\nY_min_Y_proj &lt;- Y_proj - Y\n\ndf &lt;- tibble(\n  x = c(0, 0, Y[1], 0, Y[1], Y[2], Y_proj[1], Y_proj[1]),\n  y = c(0, 0, Y[2], 0, X[2], Y[2], Y_proj[2], Y_proj[2]),\n  xend = c(X[1], Y[1], Y_proj[1], Y_proj[1], NA, NA, NA, NA),\n  yend = c(X[2], Y[2], Y_proj[2], Y_proj[2], NA, NA, NA, NA),\n  color = c('A', 'B', 'C', 'D', NA, NA, NA, NA)\n)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_segment(\n    aes(\n      xend = xend, yend = yend, \n      color = color\n      ),\n      arrow = arrow(length = unit(0.3,\"cm\")\n    )\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"x\",\n    x = 0.51, y = 0.77\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"y_hat = bx\",\n    x = 0.31, y = 0.6\n  ) +\n  annotate(\n    geom = \"text\",\n    label = \"y\",\n    x = 1.02, y = 0.19\n  ) + \n  annotate(\n    geom = \"text\",\n    label = \"y - bx\",\n    x = 0.74, y = 0.42\n  ) + \n  labs(\n    x = \"\",\n    y = \"\"\n  ) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 3.7: Vector projection\n\n\n\n\n\nWhat is the projection of the vector y onto the vector x? It is the vector that is closest to y and lies on the line spanned by x.\nTwo vectors are said to be orthogonal if their dot product is zero. The dot product of two vectors x and y is defined as\n\nx \\cdot y = \\sum_{i=1}^n x_i y_i\n\nAnother way to write the dot product is as a matrix product:\n\nx \\cdot y = x^T y\n\nwhere x^T is the transpose of x (meaning that x^T is a row vector).\nThe least squares method can be thought of as finding the vector y_{proj} that is closest to y and lies on the line spanned by x. The vector y - y_{proj} is the vector that is orthogonal to x and has the smallest length.\nThe vector y - y_{proj} is called the residual vector. The length of the residual vector is the square root of the sum of the squares of its elements. This is the same as the square root of the sum of the squares of the residuals.\nThe residual vector will be smallest in length when it is orthogonal to x. Therefore we can find a scalar \\hat{\\beta}_1 such that the residual vector is orthogonal to x.\nThe condition that the residual vector is orthogonal to x is that the dot product between the two vectors is equal to zero.\n\n\\begin{align*}\nx^T(y - \\hat{\\beta}_1 x) = 0 \\\\\nx^T y - \\hat{\\beta}_1 x^T x = 0 \\\\\n\\hat{\\beta}_1 x^T x = x^T y \\\\\n\\hat{\\beta}_1 = \\frac{x^T y}{x^T x}\n\\end{align*}\n\nYou can rewrite the dot products as sum to see the similarity to the formulas we derived using calculus.\n\n\\begin{align*}\nx^T y = \\sum_{i=1}^n x_i y_i \\\\\nx^T x = \\sum_{i=1}^n x_i^2\n\\end{align*}\n\n\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} = \\frac{n \\overline{x y}}{n \\overline{x^2}} = \\frac{\\overline{x y}}{\\overline{x^2}}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Least Squares</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Dalgaard, Peter. 2008. Introductory Statistics with\nR. 2nd edition. Statistics and Computing. New York:\nSpringer.\n\n\nFaraway, Julian James. 2015. Linear Models with R.\nSecond edition. Chapman & Hall/CRC Texts\nin Statistical Science Series. Boca Raton: CRC Press, Taylor &\nFrancis Group.\n\n\nGelman, Andrew, Jennifer Hill, and Aki Vehtari. 2021. Regression and\nOther Stories. Analytical Methods for Social Research. Cambridge\nNew York, NY Port Melbourne, VIC New Delhi Singapore: Cambridge\nUniversity Press. https://doi.org/10.1017/9781139161879.\n\n\nSheather, Simon. 2009. A Modern Approach to\nRegression with R. Edited by George\nCasella, Stephen Fienberg, and Ingram Olkin. Springer Texts\nin Statistics. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-09608-7.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo:\nO’Reilly.",
    "crumbs": [
      "References"
    ]
  }
]