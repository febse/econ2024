# Confidence Intervals and P-values

```{r setup}
## Warning: to compile the notes you need the "bookdown" and the "broom" packages. Install them by
## running install.packages, see the commented lines below

if (!require("tidyverse")) {
  install.packages("tidyverse")
}

if (!require("broom")) {
  install.packages("broom")
}

if (!require("patchwork")) {
  install.packages("patchwork")
}

if (!require("latex2exp")) {
  install.packages("latex2exp")
}

library(tidyverse)
library(broom)
library(patchwork)

kids <- read_csv(
  "https://raw.githubusercontent.com/feb-uni-sofia/econometrics2020-solutions/master/data/childiq.csv") %>%
  select(kid_score, mom_hs)
```

$$
\begin{align*}
& \text{kid\_score}_i \sim N(\mu_i, \sigma^2 = 20^2), \quad i = 1,\ldots, N = 1e6 \\
& \mu_i = 80 + 15 \text{mom\_hs}_i, \quad \text{mom\_hs} \in \{0, 1\}
\end{align*}
$$ {#eq-simulation-anova-model}

This model takes the sample of children from @sec-simple-anova as an *inspiration* but our goal is to focus on the statistical properties of the OLS estimators and not on the data itself. Basically, we will study an artificially created *population* that looks somewhat like the sample of children in the previous example. However, the insights gained in the next sections are more general and are not tied to that specific sample.

```{r}
## Population parameters

beta0 <- 80 # Average IQ of children with mom_hs = 0
beta1 <- 15 # Difference in average IQ between children with mom_hs = 1 and mom_hs = 0
sigma <- 20 # Standard deviation of the IQ scores
prop_ones <- 0.8 # Proportion of children with mom_hs = 1

## Population
set.seed(123)

N <- 5e6 # Number of children in the population

pop <- tibble(
  mom_hs = rbinom(N, 1, prop_ones),
  mu = beta0 + beta1 * mom_hs,
  kid_score = rnorm(N, mu, sigma)
) %>%
  select(-mu)
```

## Selecting the sample

We will select $R = 1000$ samples from the population. Each sample will have $n = 434$ children. We will store the samples in a data frame called `sim`.

```{r}
## Fix the random numbers generator so that you can reproduce your results
set.seed(123)

n <- 434 # Sample size
R <- 1000 # Number of samples

sim <- pop %>%
  slice_sample(n = n * R, replace = TRUE) %>%
  mutate(
    # We will add a column to identify the sample number
    sample_id = rep(1:R, each = n)
  )
```

The resulting data frame `sim` has $434 \times 1000 = 434000$ rows and contains the samples that we will use to study the statistical properties of the OLS estimators.

## OLS estimates in each sample

Now that we have the samples in the data set `sim_samples` we can compute the OLS estimates for $\beta_0$ and $\beta_1$ in each sample.

```{r}
sim_coeffs <- sim %>%
  group_by(sample_id) %>%
  ## The tidy function reformats the output of lm so that it can fit in a data frame
  do(tidy(lm(kid_score ~ 1 + mom_hs, data = .))) %>%
  select(sample_id, term, estimate, std.error, statistic)

```

```{r}
## Creates a separate table with the coefficients for mom_hs
sim_slopes <- sim_coeffs %>%
  filter(term == "mom_hs")
```

The last code chunk may seem a little bit complex, but is simply groups the `sim_samples` data by the sample number and then runs the `lm` function with the data in each sample. You can verify the results in `sim_coeffs` by running the `lm` function manually with the data from the first sample (of course you can choose another sample). The coefficient estimates in `sim_coeffs` are stored in a column called `estimate`. As there are two coefficients in our model, the column `term` tells you whether a row holds the estimate for the intercept $\hat{\beta}_0$ (`term == "(Intercept)"`) or the slope $\hat{\beta}_1$ (`term == "mom_hs"`).

You can use the `filter` function to select only the observations in the first sample

```{r}
sample_1 <- sim %>% filter(sample_id == 1)
```


```{r}
# Compute the p-value for the null hypothesis that beta1 >= 0 vs beta1 < 0

# Compute the p-value for the null hypothesis that beta1 >= 0 vs beta1 < 0

# Compute the p-value for the null hypothesis that beta1 >= 0 vs beta1 < 0

# Compute the upper and lower bounds of the 95% confidence interval for beta1

# Create a variable that indicates whether beta1 is in the confidence interval in each sample

# Count the number of samples where beta1 is in the confidence interval

# Compute the share of samples where beta1 is not in the confidence interval

```

```{r}
#| label: fig-sim-ci
#| fig-cap: "Confidence intervals for the first 50 samples. The vertical line is drawn at 11.77 (the real value of $\\beta_1$)."

# sim_slopes %>%
#   ungroup() %>%
#   slice_head(n = 50) %>%
#   ggplot(
#     aes(
#       x = estimate, 
#       y = factor(R),
#       xmin = CI_lower,
#       xmax = CI_upper,
#       color = beta1_in_CI
#     )
#   ) + 
#   geom_point() + 
#   geom_errorbarh() + 
#   labs(
#     x = "",
#     y = "Sample",
#     color = "Beta1 in CI"
#   ) + 
#   theme(
#     axis.text.y = element_blank(),
#     axis.ticks.y = element_blank(),
#   ) + 
#   geom_vline(
#     xintercept = 15
#   )
```